{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is CG DevX? Welcome to CG DevX, an all-in-one platform designed to simplify and enhance the development, deployment, and management of cloud-native applications. Whether you are an experienced platform engineer or just a beginner starting your DevOps and cloud-native journey, CG DevX provides the tools and capabilities to empower your team and streamline your workflows. Getting Started Ready to dive into CG DevX? Our comprehensive documentation and guides are your go-to resource. Get started with installation, explore configuration options, discover best practices, and unlock advanced features. Join the CG DevX community to connect with other users, share knowledge, and collaborate on cloud-native development. CG DevX empowers developers and teams to embrace cloud-native practices, supercharge their software delivery, and enhance operational efficiency.","title":"Home"},{"location":"#what-is-cg-devx","text":"Welcome to CG DevX, an all-in-one platform designed to simplify and enhance the development, deployment, and management of cloud-native applications. Whether you are an experienced platform engineer or just a beginner starting your DevOps and cloud-native journey, CG DevX provides the tools and capabilities to empower your team and streamline your workflows.","title":"What is CG DevX?"},{"location":"#getting-started","text":"Ready to dive into CG DevX? Our comprehensive documentation and guides are your go-to resource. Get started with installation, explore configuration options, discover best practices, and unlock advanced features. Join the CG DevX community to connect with other users, share knowledge, and collaborate on cloud-native development. CG DevX empowers developers and teams to embrace cloud-native practices, supercharge their software delivery, and enhance operational efficiency.","title":"Getting Started"},{"location":"approach/","text":"Core Initiative CG DEVX leverages a selection of the best open-source tools, reflecting industry best practices and community feedback, aiming to elevate the overall user experience and streamline the collaborative environment for cloud-native operations by deploying a standard yet adaptable framework. We structure our platform around distinct phases of service delivery, each with defined interfaces that support easy customization and scalability. This enables users to easily tailor their environment by integrating, modifying, or replacing components without compromising the platform's integrity. For example, users can select from various CI tools, such as GitHub Actions, Argo Workflow, or GitLab Pipelines, according to their specific requirements. This design principle helps simplify initial setups and optimize user interaction by pre-selecting the most effective tools for specific tasks. CG DEVX predominantly relies on Kubernetes due to its widespread adoption as a scalable solution for cloud-native services. It forms the backbone of our architecture, ensuring compatibility and enhancing performance with numerous CNCF-endorsed tools. Note that while Kubernetes acts as a substrate for our core services, those services can manage workloads outside the cluster, on various platforms. This gives developers greater flexibility to choose different cloud ecosystems. CG DEVX provides an implementation blueprint that addresses common platform engineering challenges. This blueprint includes detailed specifications and templates that serve as demonstrations of the best possible configurations and technological capabilities. By promoting the exchange of configurations and best practices, we aim to set a high standard for operational excellence. We recognize the need for ongoing enhancements and adaptations, and we look to our community to help us extend and refine our platform to suit a diverse array of use cases. Stakeholder Personas Development Teams Developers: Specialize in developing application-specific services intended for end users, focusing predominantly on software development rather than infrastructure complexities. Operators: Handle the operational management of platform and infrastructure services. They are responsible for cloud-based technologies, Infrastructure as Code (IaC), GitOps, and automated systems. Security Engineers: Focus on integrating and upholding security protocols within the platform, collaborating closely with operators to ensure robust and compliant infrastructure. Solution Architects Integrators: Skilled in combining various components into integrated solutions. These professionals improve the platform's value by constructing additional services that capitalize on the pre-established infrastructure.","title":"Concept"},{"location":"approach/#core-initiative","text":"CG DEVX leverages a selection of the best open-source tools, reflecting industry best practices and community feedback, aiming to elevate the overall user experience and streamline the collaborative environment for cloud-native operations by deploying a standard yet adaptable framework. We structure our platform around distinct phases of service delivery, each with defined interfaces that support easy customization and scalability. This enables users to easily tailor their environment by integrating, modifying, or replacing components without compromising the platform's integrity. For example, users can select from various CI tools, such as GitHub Actions, Argo Workflow, or GitLab Pipelines, according to their specific requirements. This design principle helps simplify initial setups and optimize user interaction by pre-selecting the most effective tools for specific tasks. CG DEVX predominantly relies on Kubernetes due to its widespread adoption as a scalable solution for cloud-native services. It forms the backbone of our architecture, ensuring compatibility and enhancing performance with numerous CNCF-endorsed tools. Note that while Kubernetes acts as a substrate for our core services, those services can manage workloads outside the cluster, on various platforms. This gives developers greater flexibility to choose different cloud ecosystems. CG DEVX provides an implementation blueprint that addresses common platform engineering challenges. This blueprint includes detailed specifications and templates that serve as demonstrations of the best possible configurations and technological capabilities. By promoting the exchange of configurations and best practices, we aim to set a high standard for operational excellence. We recognize the need for ongoing enhancements and adaptations, and we look to our community to help us extend and refine our platform to suit a diverse array of use cases.","title":"Core Initiative"},{"location":"approach/#stakeholder-personas","text":"","title":"Stakeholder Personas"},{"location":"approach/#development-teams","text":"Developers: Specialize in developing application-specific services intended for end users, focusing predominantly on software development rather than infrastructure complexities. Operators: Handle the operational management of platform and infrastructure services. They are responsible for cloud-based technologies, Infrastructure as Code (IaC), GitOps, and automated systems. Security Engineers: Focus on integrating and upholding security protocols within the platform, collaborating closely with operators to ensure robust and compliant infrastructure.","title":"Development Teams"},{"location":"approach/#solution-architects","text":"Integrators: Skilled in combining various components into integrated solutions. These professionals improve the platform's value by constructing additional services that capitalize on the pre-established infrastructure.","title":"Solution Architects"},{"location":"cgdevx/","text":"CloudGeometry Development Excellence When choosing the technology that runs your architecture, it's crucial to understand not only whether it integrates well with your existing components, but also whether it will support your business in the long term. If you get it wrong, you'll need to make changes to fix it, including modernizing outdated technologies. Additionally, the cost of re-tooling or re-platforming grows with the size of the organization, so the larger your organization is (or is expected to become), the more important these choices become. CG DevX's goal is to be a framework and test playground, simplifying the adoption of open-source technology and providing it in a pre-configured and pre-integrated form. At the same time, it comes in a production-ready configuration that can be easily extended to not only make necessary remediations but also to meet compliance or security regulations, thus reducing the risks of compatibility issues when building an Internal Developer Platform. Scenarios where CG DevX can be used include, but are not limited to: Bootstrapping Platform Engineering efforts to achieve level 2\u20133 in the CNCF platform engineering maturity model Modernizing existing infrastructure and runtime Fostering cloud adoption Serving as a set of templates when implementing the functionality of any CG DevX modules or their parts CG DevX is: Open Source-centric : We prioritize open source over proprietary components when building functionality modules. Self-contained : All components of the platform are inside Kubernetes to enable platform portability, so you can run it on any K8s flavor, virtualized or bare metal. Feedback and community-driven : The initial version of CG DevX was built based on feedback from engineering teams. Standardized but customizable : We provide a set of guidelines that can be further extended and enforced based on specific requirements. Developers and operators can decide whether to follow these guidelines. Technology enabler : We offer a pre-configured set of tools that enable technical capabilities. Practices and processes on top of them are out of CG DevX's scope.","title":"Why CG DevX"},{"location":"cgdevx/#cloudgeometry-development-excellence","text":"When choosing the technology that runs your architecture, it's crucial to understand not only whether it integrates well with your existing components, but also whether it will support your business in the long term. If you get it wrong, you'll need to make changes to fix it, including modernizing outdated technologies. Additionally, the cost of re-tooling or re-platforming grows with the size of the organization, so the larger your organization is (or is expected to become), the more important these choices become. CG DevX's goal is to be a framework and test playground, simplifying the adoption of open-source technology and providing it in a pre-configured and pre-integrated form. At the same time, it comes in a production-ready configuration that can be easily extended to not only make necessary remediations but also to meet compliance or security regulations, thus reducing the risks of compatibility issues when building an Internal Developer Platform. Scenarios where CG DevX can be used include, but are not limited to: Bootstrapping Platform Engineering efforts to achieve level 2\u20133 in the CNCF platform engineering maturity model Modernizing existing infrastructure and runtime Fostering cloud adoption Serving as a set of templates when implementing the functionality of any CG DevX modules or their parts CG DevX is: Open Source-centric : We prioritize open source over proprietary components when building functionality modules. Self-contained : All components of the platform are inside Kubernetes to enable platform portability, so you can run it on any K8s flavor, virtualized or bare metal. Feedback and community-driven : The initial version of CG DevX was built based on feedback from engineering teams. Standardized but customizable : We provide a set of guidelines that can be further extended and enforced based on specific requirements. Developers and operators can decide whether to follow these guidelines. Technology enabler : We offer a pre-configured set of tools that enable technical capabilities. Practices and processes on top of them are out of CG DevX's scope.","title":"CloudGeometry Development Excellence"},{"location":"platform_engineering/","text":"Platform Engineering Platform engineering is a new discipline that emerged in response to the growing complexity of modern cloud-native architectures. It describes the practice of building and maintaining an integrated product, called an \"Internal Developer Platform,\" which acts as a flexible and supported abstraction layer between developers and the underlying technologies of their applications. The goal of platform engineering is to improve developer productivity through improving the developer experience ( DevEx). Internal developer platforms improve the velocity and happiness of teams by enabling developer self-service and reducing cognitive load on developers. Core benefits of Platform Engineering and Internal Developer Platforms: Faster innovation : Faster time to launch, frequent updates, focus on business problems Higher quality : Fewer environment issues, more deterministic test results, faster rollback Increased reliability : Observable services, graceful degradation, improved business continuity, optimisation for a fluctuating load Improved security : Reduced security engineering overhead, Improved security operations and governance Improved ways of working : Define policy via experimentation and simpler processes, apply enabling constraints, zero downtime updates Reduced costs : Economies of scale, easier cost management Better productivity : Lower cognitive load, easier to identify talent needs Platform Engineering and the Internal Developer Platform is: Not a commodity : It cannot be bought off the shelf, as it must satisfy the specific needs of your organisation. It\u2019s built by weaving together open-source and bespoke commodity tools to create a technology accelerator. Not a project : It isn\u2019t a one-off development with a fixed end date. It will keep changing, as the needs of your teams will change based on their customers\u2019 demands. Not a universal infrastructure platform : It cannot run all cloud services for all possible consumers. It needs to focus on a subset of cloud services to support specific workloads. It\u2019s important to remember that an Internal Developer Platform isn\u2019t a silver bullet. It\u2019s a long-term commitment to providing Services at scale. It\u2019s not appropriate for all workloads, teams, or organizations.","title":"Platform Engineering"},{"location":"platform_engineering/#platform-engineering","text":"Platform engineering is a new discipline that emerged in response to the growing complexity of modern cloud-native architectures. It describes the practice of building and maintaining an integrated product, called an \"Internal Developer Platform,\" which acts as a flexible and supported abstraction layer between developers and the underlying technologies of their applications. The goal of platform engineering is to improve developer productivity through improving the developer experience ( DevEx). Internal developer platforms improve the velocity and happiness of teams by enabling developer self-service and reducing cognitive load on developers. Core benefits of Platform Engineering and Internal Developer Platforms: Faster innovation : Faster time to launch, frequent updates, focus on business problems Higher quality : Fewer environment issues, more deterministic test results, faster rollback Increased reliability : Observable services, graceful degradation, improved business continuity, optimisation for a fluctuating load Improved security : Reduced security engineering overhead, Improved security operations and governance Improved ways of working : Define policy via experimentation and simpler processes, apply enabling constraints, zero downtime updates Reduced costs : Economies of scale, easier cost management Better productivity : Lower cognitive load, easier to identify talent needs Platform Engineering and the Internal Developer Platform is: Not a commodity : It cannot be bought off the shelf, as it must satisfy the specific needs of your organisation. It\u2019s built by weaving together open-source and bespoke commodity tools to create a technology accelerator. Not a project : It isn\u2019t a one-off development with a fixed end date. It will keep changing, as the needs of your teams will change based on their customers\u2019 demands. Not a universal infrastructure platform : It cannot run all cloud services for all possible consumers. It needs to focus on a subset of cloud services to support specific workloads. It\u2019s important to remember that an Internal Developer Platform isn\u2019t a silver bullet. It\u2019s a long-term commitment to providing Services at scale. It\u2019s not appropriate for all workloads, teams, or organizations.","title":"Platform Engineering"},{"location":"tools/","text":"Credits The CG DevX could not be possible without open source. We use some of the most popular open source projects in the world. This page is dedicated to giving those projects credit for their incredible offerings. Actions Runner Controller ( Apache 2.0 ) Argo CD ( Apache 2.0 ) Argo Workflows ( Apache 2.0 ) Atlantis ( Apache 2.0 ) Aqua Security Trivy ( Apache 2.0 ) Aqua Security Tracee ( Apache 2.0 ) Backstage ( Apache 2.0 ) cert-manager ( Apache 2.0 ) External Secrets Operator ( Apache 2.0 ) Grafana ( AGPL 3.0 ) Harbor ( Apache 2.0 ) HashiCorp Terraform ( MPL 2.0 ) HashiCorp Vault ( MPL 2.0 ) Ingress NGINX Controller ( Apache 2.0 ) Kubernetes ( Apache 2.0 ) Kubernetes ExternalDNS ( Apache 2.0 ) Kyverno ( Apache 2.0 ) Loki ( AGPL 3.0 ) Prometheus ( Apache 2.0 ) Reloader ( Apache 2.0 ) SonarQube ( GNU 3.0 )","title":"Acknowledgements"},{"location":"tools/#credits","text":"The CG DevX could not be possible without open source. We use some of the most popular open source projects in the world. This page is dedicated to giving those projects credit for their incredible offerings. Actions Runner Controller ( Apache 2.0 ) Argo CD ( Apache 2.0 ) Argo Workflows ( Apache 2.0 ) Atlantis ( Apache 2.0 ) Aqua Security Trivy ( Apache 2.0 ) Aqua Security Tracee ( Apache 2.0 ) Backstage ( Apache 2.0 ) cert-manager ( Apache 2.0 ) External Secrets Operator ( Apache 2.0 ) Grafana ( AGPL 3.0 ) Harbor ( Apache 2.0 ) HashiCorp Terraform ( MPL 2.0 ) HashiCorp Vault ( MPL 2.0 ) Ingress NGINX Controller ( Apache 2.0 ) Kubernetes ( Apache 2.0 ) Kubernetes ExternalDNS ( Apache 2.0 ) Kyverno ( Apache 2.0 ) Loki ( AGPL 3.0 ) Prometheus ( Apache 2.0 ) Reloader ( Apache 2.0 ) SonarQube ( GNU 3.0 )","title":"Credits"},{"location":"capabilities/capabilities/","text":"Capabilities Core Services In CG DevX, each module is meticulously crafted and encapsulated to function independently within its designated area of usage. This isolation ensures that modules remain self-contained and do not interfere with the functionality of other components. Consequently, any changes or updates made to one module have minimal impact on the overall system, promoting stability and reliability. Flexibility and Customizability The modular architecture of CG DevX empowers users to tailor their platform experience to meet their unique requirements. Since modules are decoupled and autonomous, users can easily customize individual components without affecting the integrity of the entire system. This flexibility enables teams to curate a platform that aligns precisely with their specific needs and preferences. Seamless Module Replacement In the dynamic landscape of technology, advancements and innovations are ever-evolving. CG DevX's modular design ensures that users can readily embrace new technologies by seamlessly replacing existing modules with improved alternatives. This empowers teams to stay at the forefront of technological progress without having to undergo cumbersome and disruptive system-wide changes. Efficient Troubleshooting and Maintenance Isolated modules simplify the troubleshooting and maintenance process in CG DevX. If an issue arises within a specific module, it can be addressed independently without necessitating an extensive system-wide investigation. This targeted approach accelerates the resolution of problems, reducing downtime and enhancing the overall platform experience. Source Code Management Source code repositories store your business logic services code, manifests, and Infrastructure as Code (IaC). They act as a ledger and provide a historical lineage of changes, allowing developers and operators to work collaboratively on common codebases. CG DevX standardizes workflows associated with code review and automation driven by Git, also known as \"GitOps.\" When managed properly, pull requests (and associated changes) can serve as a \"system of record\" for change control and approval in regulatory environments. CG DevX provides unified repository management via VCS modules. Added Value Enables asynchronous collaboration on codebases, including: Peer reviews Quality gates Change request approvals Centralizes collaboration workflows Provides additional control for peer reviews and change request approvals Extends VCS quality gate capabilities Infrastructure as Code (IaC) Infrastructure as Code (IaC) uses the foundational elements of Infrastructure as a Service (IaaS) to streamline and standardize the deployment of various cloud resources. By offering a programmatic approach to infrastructure management, IaC simplifies the development of systems that are both scalable and adaptable to specific organizational needs. Key Features of IaC at CG DevX Unified Template Library CG DevX provides a comprehensive collection of IaC templates, enabling quick deployment and management of infrastructure across various cloud environments. This library serves as a resource for both creating new projects and scaling existing applications. Development of Advanced Resources Our IaC framework allows teams to develop complex, higher-order resources that align closely with business objectives, going beyond basic cloud provider offerings to deliver tailored functionality. Enforcement of Best Practices By embedding sensible defaults and strong security protocols into infrastructure configurations, CG DevX ensures deployments comply with both industry standards and the organization's internal security policies. Approaches to Infrastructure Management CG DevX supports multiple reconciliation strategies to accommodate different team needs: Occasional Reconciliation Tools Platforms such as Terraform and AWS CloudFormation are integrated into our workflow to manage infrastructure with periodic updates, providing stability and control over when changes are applied. Continuous Reconciliation Tools Tools such as Crossplane are used for environments where continuous updates are crucial. These tools maintain constant synchronization with the desired infrastructure state, ideal for dynamic and rapidly evolving cloud landscapes. Preferred IaC Tools For Workload IaC : Terraform and Crossplane are recommended for their robustness and flexibility, supporting both static and dynamic environments respectively. For Platform IaC Management : Terraform is the primary tool, trusted for its comprehensive feature set and reliability in maintaining consistent infrastructure states across deployments. Integration with Kubernetes The use of Kubernetes alongside IaC tools like Crossplane enhances our capability to manage resources seamlessly and efficiently. This integration is pivotal in achieving real-time, automated updates and maintaining system integrity across our operational landscape. Continuous Integration (CI) Continuous Integration (CI) refers to the build and unit testing stages of the release process. As workflows become more complex, CI can also orchestrate tasks to make the application ready for delivery. This includes running tests (unit tests, smoke tests, integration tests, acceptance tests, etc.), validations, and verifications. Added Value Defines imperative steps to be completed Uses a DSL for describing directional workflow graphs Enables side effects like notifications or manual interventions Continuous Delivery (CD) The primary aim of Continuous Delivery (CD) is to prepare infrastructure and applications to efficiently handle production-level demands. Adoption of GitOps GitOps represents an evolving approach within CD, where automation ensures alignment between the intended and actual states of system resources. This connection is maintained through a Git repository, which acts as the definitive source for application and infrastructure definitions. Changes to this repository trigger updates via controllers such as ArgoCD and FluxCD, ensuring system consistency. Although ArgoCD and FluxCD are similar, they can operate together, improving both flexibility and coverage. Added Value Automation Benefits : Each successful code merge triggers automated processes to build, test, and release updates, streamlining the deployment cycle. Continuous Deployment : With this feature enabled, updates are seamlessly transitioned into live environments, achieving a state of constant readiness and deployment without manual intervention. Enhanced Testing : CD supports extensive testing regimes that extend well beyond basic checks, incorporating comprehensive E2E or functional testing, often within preview environments. Risk-Managed Deployments : Incorporates methods like Blue/Green or Canary deployments, which introduce new versions alongside the old, reducing rollout risks. Feature Management : Utilizes feature flags to manage the exposure of new functionalities, allowing controlled and incremental user exposure. Operational Efficiency : Accelerates delivery, reduces time to market, and enhances the feedback loop, leading to quicker identification of issues and adjustments. Secrets Management Secrets Management provides long-term secured storage and management for sensitive data (passwords, secrets, access keys, certificates, etc.). Similar to VCS, it offers versioning and metadata capabilities, combined with stricter access controls and auditing. The storage is also encrypted and can integrate with Hardware Security Modules (HSMs). Secrets Management may also offer the ability to generate, lease, rotate, and revoke secrets. CG DevX's reference implementation is based on Hashicorp Vault, integrated with cloud-specific services via cloud provider module abstraction, such as AWS Secrets Manager or Azure Key Vault. Added Value Secure and durable storage Uses a simple key-value pair data structure Data is encrypted Values can be extended with metadata Values can be versioned Provides tight access control May offer the ability to generate, lease, rotate, and revoke secrets Identity and Access CG DevX offers Identity and Access management as part of the platform. By unifying and centralizing this critical functionality, CG DevX allows other core services to offload proof of identity and access management to the platform. This improves the maintainability of the system and makes access policies more transparent. CG DevX uses Hashicorp Vault OIDC capabilities but can be integrated with other providers such as Active Directory, Okta, etc. Added Value Provides Single Sign-On (SSO) Reduces duplication of effort Identity can be federated Artifact Registries Artifact registries serve as centralized storage for artifacts produced by the CI pipeline. They can also act as proxy registries for common external registries, such as DockerHub, Quay, and GCR. This integration with external registries can be bidirectional, allowing offloading of artifacts to external repositories, contributing to Business Continuity and Disaster Recovery. It provides integration with static code analysis tools, enabling secure software supply chain (SSSC) best practices. Added Value Long-term artifact storage Provides built-in Role-Based Access Control (RBAC) to limit access to artifacts Versioned and immutable Integrated with static analysis tools to detect known vulnerabilities in artifacts Can be extended with cryptographic signing, enabling consistency and integrity verification Developer Portal The Developer Portal is a key productivity tool that serves as a high-order interface for all the underlying tooling and services, presenting them in a user-friendly manner. CG DevX integrates with Backstage to provide a single pane of glass for developer productivity solutions. Added Value Software catalog of all components, systems, and domains, and their cross-dependencies Documentation system using the docs-as-code approach, where docs are typically in Markdown and stored in code repositories Catalog of templates for creating new projects or consuming services Trust and security onboarding automation Observability Gain deep insights into the health and performance of your platform and workloads. Log and telemetry data collection and visualization tools track the health of the system. CG DevX integrates seamlessly with Prometheus, Loki, and Grafana. Added Value Provides insights on system behavior Enables near real-time visibility of system events Offers logs and metrics-based alerts GitOps and PR Automation CG DevX offers robust CI/CD capabilities to accelerate your software delivery lifecycle. Set up seamless pipelines to automate building, testing, and deploying your applications. Integrate with popular version control systems like Git, and choose from a range of deployment strategies such as rolling updates and canary deployments. Effortlessly deliver your applications with confidence. Governance and Validation While \"shift left\" empowers developers and gives them almost complete control over their workloads, this power comes with great responsibility. Without proper guardrails, it is impossible to protect the platform from human errors such as misconfiguration, wide access policies, and misplaced labels. Platforms can use specifications to validate all client interactions with the system. The end goal is to make suspicious or unwanted interactions with the platform visible to operators. For instance, Kubernetes offers a built-in policy-based validation engine when working with its APIs. However, such behavior can be detected and prevented early by providing additional checks within many services in the platform toolbox. Admission control can also be a common substrate for injecting policy controls or building guardrails within the platform to meet security or regulatory requirements. Added Value Ensures internal policies are obeyed Kubernetes can enable a common policy plane Prevents security issues by detecting issues in the early stages Workload Runtime This is the main platform runtime and can also be considered a deployment target for the applications that make up the platform. Kubernetes provides the compute substrate and foundation for platform capabilities. Incident Response and Automation The goal of incident response is to ensure quick and efficient resolution to incidents detected by the observability stack. It usually serves as a dedicated communication channel for collaborating on resolving incidents. It can also leverage run-books to automate repetitive tasks and improve incident response times. Added Value Detect, triage, and resolve incidents Foster collaboration among teams responsible for incident response Cost Management Cost Management allows transparent resource ownership and lineage tracking, improving overall cost visibility and enabling cost attribution. It provides insights into resource usage, helps identify cost-saving opportunities, and implements effective cost-optimization strategies. Some cost-saving opportunities like resource cleanup and on/off schedules can be automated. It integrates with governance and enables cloud and Kubernetes resource labeling. Added Value Cost attribution back to a service, team, or consumer of the service Rightsizing and cost optimization recommendations Visualize spending over time to detect unwanted changes","title":"Capabilities Overview"},{"location":"capabilities/capabilities/#capabilities","text":"","title":"Capabilities"},{"location":"capabilities/capabilities/#core-services","text":"In CG DevX, each module is meticulously crafted and encapsulated to function independently within its designated area of usage. This isolation ensures that modules remain self-contained and do not interfere with the functionality of other components. Consequently, any changes or updates made to one module have minimal impact on the overall system, promoting stability and reliability.","title":"Core Services"},{"location":"capabilities/capabilities/#flexibility-and-customizability","text":"The modular architecture of CG DevX empowers users to tailor their platform experience to meet their unique requirements. Since modules are decoupled and autonomous, users can easily customize individual components without affecting the integrity of the entire system. This flexibility enables teams to curate a platform that aligns precisely with their specific needs and preferences.","title":"Flexibility and Customizability"},{"location":"capabilities/capabilities/#seamless-module-replacement","text":"In the dynamic landscape of technology, advancements and innovations are ever-evolving. CG DevX's modular design ensures that users can readily embrace new technologies by seamlessly replacing existing modules with improved alternatives. This empowers teams to stay at the forefront of technological progress without having to undergo cumbersome and disruptive system-wide changes.","title":"Seamless Module Replacement"},{"location":"capabilities/capabilities/#efficient-troubleshooting-and-maintenance","text":"Isolated modules simplify the troubleshooting and maintenance process in CG DevX. If an issue arises within a specific module, it can be addressed independently without necessitating an extensive system-wide investigation. This targeted approach accelerates the resolution of problems, reducing downtime and enhancing the overall platform experience.","title":"Efficient Troubleshooting and Maintenance"},{"location":"capabilities/capabilities/#source-code-management","text":"Source code repositories store your business logic services code, manifests, and Infrastructure as Code (IaC). They act as a ledger and provide a historical lineage of changes, allowing developers and operators to work collaboratively on common codebases. CG DevX standardizes workflows associated with code review and automation driven by Git, also known as \"GitOps.\" When managed properly, pull requests (and associated changes) can serve as a \"system of record\" for change control and approval in regulatory environments. CG DevX provides unified repository management via VCS modules.","title":"Source Code Management"},{"location":"capabilities/capabilities/#added-value","text":"Enables asynchronous collaboration on codebases, including: Peer reviews Quality gates Change request approvals Centralizes collaboration workflows Provides additional control for peer reviews and change request approvals Extends VCS quality gate capabilities","title":"Added Value"},{"location":"capabilities/capabilities/#infrastructure-as-code-iac","text":"Infrastructure as Code (IaC) uses the foundational elements of Infrastructure as a Service (IaaS) to streamline and standardize the deployment of various cloud resources. By offering a programmatic approach to infrastructure management, IaC simplifies the development of systems that are both scalable and adaptable to specific organizational needs.","title":"Infrastructure as Code (IaC)"},{"location":"capabilities/capabilities/#key-features-of-iac-at-cg-devx","text":"Unified Template Library CG DevX provides a comprehensive collection of IaC templates, enabling quick deployment and management of infrastructure across various cloud environments. This library serves as a resource for both creating new projects and scaling existing applications. Development of Advanced Resources Our IaC framework allows teams to develop complex, higher-order resources that align closely with business objectives, going beyond basic cloud provider offerings to deliver tailored functionality. Enforcement of Best Practices By embedding sensible defaults and strong security protocols into infrastructure configurations, CG DevX ensures deployments comply with both industry standards and the organization's internal security policies.","title":"Key Features of IaC at CG DevX"},{"location":"capabilities/capabilities/#approaches-to-infrastructure-management","text":"CG DevX supports multiple reconciliation strategies to accommodate different team needs: Occasional Reconciliation Tools Platforms such as Terraform and AWS CloudFormation are integrated into our workflow to manage infrastructure with periodic updates, providing stability and control over when changes are applied. Continuous Reconciliation Tools Tools such as Crossplane are used for environments where continuous updates are crucial. These tools maintain constant synchronization with the desired infrastructure state, ideal for dynamic and rapidly evolving cloud landscapes.","title":"Approaches to Infrastructure Management"},{"location":"capabilities/capabilities/#preferred-iac-tools","text":"For Workload IaC : Terraform and Crossplane are recommended for their robustness and flexibility, supporting both static and dynamic environments respectively. For Platform IaC Management : Terraform is the primary tool, trusted for its comprehensive feature set and reliability in maintaining consistent infrastructure states across deployments.","title":"Preferred IaC Tools"},{"location":"capabilities/capabilities/#integration-with-kubernetes","text":"The use of Kubernetes alongside IaC tools like Crossplane enhances our capability to manage resources seamlessly and efficiently. This integration is pivotal in achieving real-time, automated updates and maintaining system integrity across our operational landscape.","title":"Integration with Kubernetes"},{"location":"capabilities/capabilities/#continuous-integration-ci","text":"Continuous Integration (CI) refers to the build and unit testing stages of the release process. As workflows become more complex, CI can also orchestrate tasks to make the application ready for delivery. This includes running tests (unit tests, smoke tests, integration tests, acceptance tests, etc.), validations, and verifications.","title":"Continuous Integration (CI)"},{"location":"capabilities/capabilities/#added-value_1","text":"Defines imperative steps to be completed Uses a DSL for describing directional workflow graphs Enables side effects like notifications or manual interventions","title":"Added Value"},{"location":"capabilities/capabilities/#continuous-delivery-cd","text":"The primary aim of Continuous Delivery (CD) is to prepare infrastructure and applications to efficiently handle production-level demands.","title":"Continuous Delivery (CD)"},{"location":"capabilities/capabilities/#adoption-of-gitops","text":"GitOps represents an evolving approach within CD, where automation ensures alignment between the intended and actual states of system resources. This connection is maintained through a Git repository, which acts as the definitive source for application and infrastructure definitions. Changes to this repository trigger updates via controllers such as ArgoCD and FluxCD, ensuring system consistency. Although ArgoCD and FluxCD are similar, they can operate together, improving both flexibility and coverage.","title":"Adoption of GitOps"},{"location":"capabilities/capabilities/#added-value_2","text":"Automation Benefits : Each successful code merge triggers automated processes to build, test, and release updates, streamlining the deployment cycle. Continuous Deployment : With this feature enabled, updates are seamlessly transitioned into live environments, achieving a state of constant readiness and deployment without manual intervention. Enhanced Testing : CD supports extensive testing regimes that extend well beyond basic checks, incorporating comprehensive E2E or functional testing, often within preview environments. Risk-Managed Deployments : Incorporates methods like Blue/Green or Canary deployments, which introduce new versions alongside the old, reducing rollout risks. Feature Management : Utilizes feature flags to manage the exposure of new functionalities, allowing controlled and incremental user exposure. Operational Efficiency : Accelerates delivery, reduces time to market, and enhances the feedback loop, leading to quicker identification of issues and adjustments.","title":"Added Value"},{"location":"capabilities/capabilities/#secrets-management","text":"Secrets Management provides long-term secured storage and management for sensitive data (passwords, secrets, access keys, certificates, etc.). Similar to VCS, it offers versioning and metadata capabilities, combined with stricter access controls and auditing. The storage is also encrypted and can integrate with Hardware Security Modules (HSMs). Secrets Management may also offer the ability to generate, lease, rotate, and revoke secrets. CG DevX's reference implementation is based on Hashicorp Vault, integrated with cloud-specific services via cloud provider module abstraction, such as AWS Secrets Manager or Azure Key Vault.","title":"Secrets Management"},{"location":"capabilities/capabilities/#added-value_3","text":"Secure and durable storage Uses a simple key-value pair data structure Data is encrypted Values can be extended with metadata Values can be versioned Provides tight access control May offer the ability to generate, lease, rotate, and revoke secrets","title":"Added Value"},{"location":"capabilities/capabilities/#identity-and-access","text":"CG DevX offers Identity and Access management as part of the platform. By unifying and centralizing this critical functionality, CG DevX allows other core services to offload proof of identity and access management to the platform. This improves the maintainability of the system and makes access policies more transparent. CG DevX uses Hashicorp Vault OIDC capabilities but can be integrated with other providers such as Active Directory, Okta, etc.","title":"Identity and Access"},{"location":"capabilities/capabilities/#added-value_4","text":"Provides Single Sign-On (SSO) Reduces duplication of effort Identity can be federated","title":"Added Value"},{"location":"capabilities/capabilities/#artifact-registries","text":"Artifact registries serve as centralized storage for artifacts produced by the CI pipeline. They can also act as proxy registries for common external registries, such as DockerHub, Quay, and GCR. This integration with external registries can be bidirectional, allowing offloading of artifacts to external repositories, contributing to Business Continuity and Disaster Recovery. It provides integration with static code analysis tools, enabling secure software supply chain (SSSC) best practices.","title":"Artifact Registries"},{"location":"capabilities/capabilities/#added-value_5","text":"Long-term artifact storage Provides built-in Role-Based Access Control (RBAC) to limit access to artifacts Versioned and immutable Integrated with static analysis tools to detect known vulnerabilities in artifacts Can be extended with cryptographic signing, enabling consistency and integrity verification","title":"Added Value"},{"location":"capabilities/capabilities/#developer-portal","text":"The Developer Portal is a key productivity tool that serves as a high-order interface for all the underlying tooling and services, presenting them in a user-friendly manner. CG DevX integrates with Backstage to provide a single pane of glass for developer productivity solutions.","title":"Developer Portal"},{"location":"capabilities/capabilities/#added-value_6","text":"Software catalog of all components, systems, and domains, and their cross-dependencies Documentation system using the docs-as-code approach, where docs are typically in Markdown and stored in code repositories Catalog of templates for creating new projects or consuming services Trust and security onboarding automation","title":"Added Value"},{"location":"capabilities/capabilities/#observability","text":"Gain deep insights into the health and performance of your platform and workloads. Log and telemetry data collection and visualization tools track the health of the system. CG DevX integrates seamlessly with Prometheus, Loki, and Grafana.","title":"Observability"},{"location":"capabilities/capabilities/#added-value_7","text":"Provides insights on system behavior Enables near real-time visibility of system events Offers logs and metrics-based alerts","title":"Added Value"},{"location":"capabilities/capabilities/#gitops-and-pr-automation","text":"CG DevX offers robust CI/CD capabilities to accelerate your software delivery lifecycle. Set up seamless pipelines to automate building, testing, and deploying your applications. Integrate with popular version control systems like Git, and choose from a range of deployment strategies such as rolling updates and canary deployments. Effortlessly deliver your applications with confidence.","title":"GitOps and PR Automation"},{"location":"capabilities/capabilities/#governance-and-validation","text":"While \"shift left\" empowers developers and gives them almost complete control over their workloads, this power comes with great responsibility. Without proper guardrails, it is impossible to protect the platform from human errors such as misconfiguration, wide access policies, and misplaced labels. Platforms can use specifications to validate all client interactions with the system. The end goal is to make suspicious or unwanted interactions with the platform visible to operators. For instance, Kubernetes offers a built-in policy-based validation engine when working with its APIs. However, such behavior can be detected and prevented early by providing additional checks within many services in the platform toolbox. Admission control can also be a common substrate for injecting policy controls or building guardrails within the platform to meet security or regulatory requirements.","title":"Governance and Validation"},{"location":"capabilities/capabilities/#added-value_8","text":"Ensures internal policies are obeyed Kubernetes can enable a common policy plane Prevents security issues by detecting issues in the early stages","title":"Added Value"},{"location":"capabilities/capabilities/#workload-runtime","text":"This is the main platform runtime and can also be considered a deployment target for the applications that make up the platform. Kubernetes provides the compute substrate and foundation for platform capabilities.","title":"Workload Runtime"},{"location":"capabilities/capabilities/#incident-response-and-automation","text":"The goal of incident response is to ensure quick and efficient resolution to incidents detected by the observability stack. It usually serves as a dedicated communication channel for collaborating on resolving incidents. It can also leverage run-books to automate repetitive tasks and improve incident response times.","title":"Incident Response and Automation"},{"location":"capabilities/capabilities/#added-value_9","text":"Detect, triage, and resolve incidents Foster collaboration among teams responsible for incident response","title":"Added Value"},{"location":"capabilities/capabilities/#cost-management","text":"Cost Management allows transparent resource ownership and lineage tracking, improving overall cost visibility and enabling cost attribution. It provides insights into resource usage, helps identify cost-saving opportunities, and implements effective cost-optimization strategies. Some cost-saving opportunities like resource cleanup and on/off schedules can be automated. It integrates with governance and enables cloud and Kubernetes resource labeling.","title":"Cost Management"},{"location":"capabilities/capabilities/#added-value_10","text":"Cost attribution back to a service, team, or consumer of the service Rightsizing and cost optimization recommendations Visualize spending over time to detect unwanted changes","title":"Added Value"},{"location":"capabilities/known_limitations/","text":"Known Challenges and Limitations There are known challenges and limitations in capabilities provided by CG DevX. Solving some of them will make the platform more opinionated and experience tailored to a specific case, which we want to avoid. This allows us to draw a fine line on things that we want to provide with the reference implementation and customization that should be done on top of it. You could think of it in the most complex cases as a fork of the platform, which we don't want to do. Those customizations could also make further updates of the \"fork\" hard or even impossible. This sets a first line in a shared responsibility model between us as builders of CG DevX, and users of CG DevX. When building an Internal Developer Platform using CG DevX, there is a second line dividing responsibility between operators (AKA a platform team) and developers, where some responsibilities could also be shared. This division of responsibility is organization-specific and should be defined by the user of the CG DevX. The list below describes known challenges and our vision on means to manage them, and responsibility. Runtime Data Persistence : It is still challenging even with persistent volumes. We do not recommend running non-fault-tolerant stateful components (DBs). Data Security : We provide encryption at rest and in transit for all CG DevX internal components and underlying services plus provide fine-grained RBAC policies, still storing sensitive data in containers or persistent volumes is not secure. Data Backup and Recovery : Currently we do not provide a built-in mechanism for Persistent Volumes (PV) data backup and recovery. To ensure that data is not lost in the event of a disaster, you should implement a backup and recovery strategy including offloading data to a separate location, and testing recovery procedures regularly. Data Management : CG DevX internal components like a logging and monitoring system have default lifecycle policies built-in. Responsibility for data produced by workloads is solely on the user of the platform, so you will need to implement data management practices (data lifecycle, archiving data, consistency monitoring, etc.) yourself. Data Compliance : Responsibility for data produced by workloads is solely on the user of the platform, so you may need to implement specific data privacy and security measures, including data masking, data retention policies, or data sovereignty. VCS Collaboration and Branching Strategy : Working in a distributed team is not possible without a proper VCS. While changes done to different parts of the system are quite easy to manage, when those changes are linked or depend on each other and done by different teams, it becomes really hard to track and integrate them without proper conventions. We expect teams to use the GitFlow approach for all the IaC code and GitOps manifests (GitOps repositories) and do a cross-team code review for such changes. We strongly suggest using GitFlow for the workload codebase as it's already supported by the platform, but teams could also use the Trunk-based development approach. Versioning : Tracking versions, especially when other services depend on your specific package, could be quite challenging and could become messy. To simplify a standardized versioning process, we utilize semantic versioning and enforce its usage for deployments and Helm charts and strongly recommend using it for your workloads. History : Version Control System (VCS) should provide a clean and human-readable history of changes. To enable this, all the developers should follow the same convention when working with the source code. We strongly recommend following the Conventional Commits convention for commit messages. Changelog : VCS should provide users a detailed view of changes associated with a specific version of artifacts, which enables developers to quickly identify potential issues associated with the change. With the usage of Conventional Commits, changelogs could be generated automatically. IaC Security & Compliance Requirements : Infrastructure is a matter of compliance fit. Thoughtful implementation of IaC enables security and compliance checks against code to detect potential issues faster. CG DevX provides a mechanism of policy/compliance as code checks according to security best practices. It\u2019s a user's responsibility to implement specific security and compliance checks using provided mechanisms. Infrastructure Ownership : Infrastructure is a core part of Business Service. CG DevX platform is a core infrastructure component holding multiple critical business products. The shift-left approach sets a clear assignment of infrastructure ownership, where operators are responsible for core platform services and development teams - for workloads and other cloud services provisioned via the platform those services depend on. Integration with Existing Resources : Customers may or may not have IaC used. Furthermore, if IaC is used, it may have various forms - Terraform, Crossplane, Pulumi, CloudFormation, etc. To be fully integrated with the platform when onboarding a workload, existing resources should be either imported into Terraform (or Crossplane for Workloads) or recreated. CG DevX provides an inventory of all resources and services in the form of remote Terraform state that could be consumed by other tools. Deployment into Existing Infrastructure : Some customers may already have existing Kubernetes cluster(s). While it\u2019s technically possible to install CG DevX components onto existing clusters, we cannot guarantee the integrity of the platform and uncompromised function of its core features, especially in the DevSecOps domain, thus decided not to support such a scenario. IaC Automation Identifying and Fixing Security Issues : Security misconfigurations are the #1 type of incident as per-security reports. In a pipeline, the earlier you perform security scanning and testing, the more freedom operators and developers will have to accommodate all security feedback. With CG DevX all the necessary checks are built into the pipeline, so security is an integral part of your code, not an add-on feature. Configuration of those features is up to the user of CG DevX. Access Management : In cloud-native deployments compared to classic ones, access management has higher associated security risks. Integrating the necessary security checks into a delivery pipeline will improve overall security and simplify migration towards zero trust. With CG DevX IAM policies (role bindings and policy metadata) are written down as code and managed using the same process as the rest of the infrastructure. Access to infrastructure could be restricted or reduced to read-only access as all the changes are applied using change management automation. Changes sign-off is done by peer review reducing security risks. Audit Logs : Audit trails are a big part of SOC2, HIPPA, and PCI compliance. There is a misconception that it could only be achieved by using specialized systems. When implemented correctly, your VCS change log could be leveraged as an audit trail. CG DevX IaC PR automation provides a full change log as part of VCS history that could also be offloaded to other storage. Upgrades/Migrations of Stateful Infrastructure Components : Stateful components, especially data stores and databases vs GitOps and immutable infrastructure. Given all pros and cons of approaches, with CG DevX we mostly rely on immutable infrastructure to get better repeatability, reduce chances of configuration drift, and have versioning with change history. We only support in-place updates and configuration management for K8s cluster upgrades. We strongly recommend that you use managed cloud resources where all the maintenance and update procedures are done by the cloud provider. Cross-Components Dependencies : Modern cloud-native applications look like a web of interdependent services, especially when adopting microservices architecture. On top of that, you have cloud-native services and external 3rd party services, like payment gateways, your application depends on. When any of these dependencies fail, it can have cascading impacts on the rest of your services and on the application as a whole. With CG DevX, your dependencies on cloud services are documented as IaC and tagged properly. It\u2019s your responsibility as a user to keep track of your application dependencies. CG DevX could be extended by Cloud Assets Catalog, as it's not a part of standard installation. Drift Detection : Drift refers to the deviation of actual infrastructure from its intended or expected state. This can occur due to various reasons, such as manual configuration changes, human error, etc. Drift can result in configuration inconsistencies, security vulnerabilities, and performance issues, making it essential to detect and correct it as soon as possible. CG DevX could detect drift for resources provisioned via platform IaC and highlight it in special reports using tools that scan and analyze the infrastructure. While we include resources not provisioned via IaC as part of those reports, we could not detect changes unless resources are imported to and managed by platform IaC solution. Workload CI Feature Velocity Issues : Multiple factors could contribute to delays in release cycles. It could happen due to lack of automation, code quality, team motivation, etc. CG DevX provides a fully automated delivery pipeline and provides a detailed view on Lead Time for Changes and Deployment Frequency as part of DORA key metrics. Flawed Automated Testing : Automated tests with no automation are being spoiled over time. They must be incorporated into CI pipelines and closely monitored. CG DevX provides well-defined extension points in CI pipelines to integrate quality gates such as automated tests. The code coverage metric is tracked using integration with a Static Code Analysis component. It\u2019s your responsibility as a user to track Functional coverage of your tests. Our default CI pipeline has predefined quality gate rules based on code coverage and the percentage of passed tests. Security Vulnerabilities : Vulnerabilities in contrast to bugs are usually unseen unless they were exposed. Shifting security to the left is the first and foremost priority. With CG DevX all the necessary checks are built into the pipeline, so security is an integral part of your code, not an add-on feature. We strongly recommend taking a step further and using Vulnerability Scanner plugins for IDE, like one for Trivy from Aqua Security. CI Process Visibility : CI may generate an excessive amount of logs that have little to no business value. Therefore, specific metrics (KPIs) and events must be identified and agreed on, and this list should be reviewed with each change to the pipeline. CG DevX has a predefined set of metrics and events collected by the system out of the box to enable DORA key metrics. Collaboration Process : Proper branching strategy and naming conventions are the prerequisites for successful CI implementation. Lack of those attributes increases mess in a software delivery process and sometimes becomes detrimental rather than helpful. To simplify and standardize the process, we recommend using GitFlow, semantic versioning, and conventional commits. Integration with Other Tools : Automation requires explicit flows across the sequence of tools in the software development lifecycle to ensure actionable feedback and improvements. CG DevX CI pipeline consists of pre-defined building blocks and provides clear and well-defined extension points. Workload CD Multi-Tenancy : In a K8s context, multi-tenancy is when multiple teams are working on different projects in the same environment. With highly configurable access control CG DevX will restrict deployments to a specific namespace and user access to specific pipelines. RBAC configurations are automatically generated when provisioning new workloads via CG DevX CLI. Monitoring and Alerting : Out of the box CD metrics such as application reconciliation performance, the controller queue depth, and the number of application sync operations will be available via CG DevX observability. Poor Communication Across Teams : Situations when teams rely on tribal knowledge of deployment details and dates could pose a serious threat to the quality of the service. With CG DevX we try to solve this by providing a high degree of automation and transparency of delivery pipelines. Adoption of proper change management and notification mechanisms is the responsibility of the user of the platform. Configuration Drift : Configuration differences between the target cluster of a deployment pipeline can cause it to fail. CG DevX CD module could detect configuration drifts. While out-of-the-box security policy will not allow users to change configuration directly in the cluster, it is the user's responsibility to ensure those policies are not altered. The CD module could auto-sync changes done directly in the cluster back to the GitOps repository, but we strongly suggest not using this option as it will reduce visibility and trackability of changes. Repository Management : According to many best practices, application code, Kubernetes manifests, and IaC code should be separated as it simplifies management, helps keep a clean audit trail, and controls access. There is no one size fits all approach here. To compromise between all pros and cons we provide two repositories per workload, one for application source code, and another for Kubernetes manifests, Helm charts, and IaC. Workload Code Quality No Common Code Quality Standards : To produce clean and maintainable code, the organization should understand the importance of and adopt code quality standards. CG DevX Code Quality module could facilitate standard adoption, but full responsibility lies on the organization. Code Quality : Each programming language has its definition of code quality. In some cases, those definitions should be adjusted or extended. CG DevX Code Quality ships with predefined code quality rules based on industry standards and best practices for each supported programming language. While there is a default one, developers should pick the set of rules they are going to use. Quality Gates : Quality Gates define a set of conditions to be met for code quality to be considered sufficient. We suggest using Quality Gates that mandate that all new code must include at least 80% test coverage, and no diagnosed security issues and provide a default configuration with those rules. We do not enforce this Quality Gate and developers should decide what exact rules and values they are going to use. False-Positives : Not all analysis rules are equally suitable in each situation and could produce false positives. Lots of them may overwhelm developers making it difficult to focus on other more important fixes. Each team should adjust default code quality rules to reduce the number of false positives. Unit Test Coverage Reports : Test coverage information could greatly improve understanding of code quality. CG DevX Code Quality module does not generate the coverage report itself. The user must set up a language-specific tool to calculate and produce a code coverage report as part of the delivery pipeline. We do provide configuration samples for GitHub and Argo Workflows that could be reused. Artifact Management Ownership and Lineage Tracking : To enable cost tracking and simplify the process of incident response, all the resources should have strict assigned owners, and other associated metadata. CG DevX Artifacts management stores artifact metadata such as build history, dependencies, readme, and other user-defined information. The user is responsible for setting all the necessary metadata in addition to the one set by default and required by the pipeline to operate. Lifecycle Management : CI/CD pipeline could produce dozens of artifacts that could require a significant amount of storage. Artifacts lifecycle management and storage quotas should be implemented to keep storage under control and comply with organizational standards. We provide an automated clean-up process and an easy way to set storage quotas per workload. Vulnerability Scanning : It\u2019s important to have a multi-layer vulnerability detection system and check artifacts on each step of the delivery pipeline. We provide static analysis of vulnerabilities with Trivy out of the box, and it\u2019s possible to connect additional scanners if required by compliance. Disaster Recovery : To enable recovery of the system in case of region failure, all the artifacts should be offloaded to a different region. With CG DevX Artifacts management, artifacts could be replicated to different artifact stores in different locations. Public Registry Request Limits and Latency : Certain artifacts must always be available for the production environment to pull them. Latency of downloading artifacts and public registry rate limits could affect production RTO. CG DevX Artifacts management could serve as a local cache for publicly available artifacts. This also covers a scenario when the public registry becomes unavailable. DevSecOps Issues Overload : High volumes of potential vulnerabilities and misconfigurations make it difficult to focus on the most important issues. Without risk-based prioritization, developers might spend time on issues that might not even represent a risk to the organization. CG DevX DevSecOps modules could provide priority for some issues, but they should only be considered as recommendations. Balancing Speed and Security : Modern development is all about speed and agility, and every team needs to keep pace. While CG DevX provides a security foundation that is agile, adaptable, and fast it could not cover all the potential risks and standards. The user is responsible for building those additional capabilities on top of the existing foundation. Lack of Resources and Knowledge Gap : Most organizations lack adequate working knowledge of DevSecOps practices. With limited resources, bridging this gap becomes even more difficult. CG DevX provides a common platform for operators and developers to collaborate with security teams or security consultants. While we provide the implementation of security best practices, it will not be able to fully compensate for knowledge gaps. Roles and Responsibility Alignment : It is challenging to align roles and responsibilities in dynamic environments. CG DevX follows a shift-left approach where developers and operators are responsible for the security of their workloads and the platform and provides a set of measures to give security feedback at all stages of the delivery pipeline and as early as possible. Workload Cloud Services Dependency : Cloud services have high complexity associated with them, especially when integrating with security tools that were not designed to be cloud-native. As CG DevX DevSecOps primarily focuses on workload code, its dependencies, container runtime, and associated manifests it could not provide full coverage for all workload-specific cloud-native services, especially when the workload relies on many of them. Monitoring Complexity : IDP is a system of services considered a Complex System and is composed of a large number of simple parts. There are a significant amount of interconnections and interactions between parts and at different space and time scales. The whole platform and workloads become adaptive and respond to changes in the environment as well as the system itself. With lots of dynamic monitoring solutions should adopt those changes and continuously track the state of the whole system. CG DevX Monitoring module provides a unified way of metric collection, automated services discovery, and health checks that are automatically applied when the platform scales in and out. Clustered Environment : When platform and workload components are located at different places (services, compute nodes, etc.) and packed together, monitoring becomes a challenge. It becomes critical to properly track and attribute resource usage as one application could cause failures in others, and operators and developers should be able to quickly identify and resolve the root cause. CG DevX Monitoring module provides an easy way to zoom into a specific cluster/node/namespace/pod helping you to identify \u201cnoisy neighbors\u201d. Data Volume and Retention Periods : When unmanaged data tend to grow and consume a tremendous amount of resources to store, process, and be searchable. Reducing the number of metrics you collect and process makes it easier to manage and analyze. CG DevX collects only the most critical metrics to reduce data volume, also metrics of internal components have a 14-day retention period by default. When introducing new metrics, users of the platform should understand and be accountable for the additional resources required to process new data. Shared Services : Sharing services enables optimum utilization of resources. On the other hand, monitoring shared applications is quite challenging. The risk lies in the fact that more consumption of resources by one application would affect the performance of other applications. This is especially true for multi-tenant applications. CG DevX provides continuous monitoring and labeling for all the services. For shared services as part of workloads, users are required to properly configure monitoring dashboards and labeling so that they could track and attribute usage to a specific workload. Monitor Workload Resources : Most workloads have dependencies on other cloud resources that due to reliability, availability, or compliance reasons could not be running on a Kubernetes-based runtime. CG DevX Monitoring module automatically collects metrics exposed by cloud-native services provisioned by CG DevX IaC module when they are provisioned using cloud-native monitoring mechanisms provided by the cloud provider and forwarding all the metrics into the Monitoring module. It also creates a minimal set of alarms as a backup circuit for the monitoring pipeline running within the platform. When resources are not provisioned via CG DevX IaC user takes full responsibility for monitoring them by using cloud-native or third-party monitoring solutions or plugging them into CG DevX Monitoring. Workload Rightsizing : The process of matching actual workload resource requirements with estimations is really hard especially for new applications. Also, requirements tend to change over time. CG DevX provides a detailed utilization dashboard on the cluster, cluster node, workload (Kubernetes namespace), and service within the workload ( Kubernetes pod) levels and recommendations on resource rightsizing and associated cost savings. More detailed cost visibility dashboards and cost-saving recommendations could be provided by CG DevX Cost Visibility. Log Management Sensitive Data : It's vital to obfuscate PII and other sensitive data in your logs because you can ensure confidentiality and integrity of the data while continuing to use logs to analyze system behavior and troubleshoot issues. You can mitigate the risk of data breaches and unauthorized access and still get your ongoing debugging work done. We remove sensitive data such as secrets from delivery pipeline logs. Users of the platform should implement additional log obfuscation rules and mechanisms for their workloads. Balance Security/Usability/Log Size : To make sure the logs remain a valuable tool for monitoring system performance and troubleshooting, it\u2019s crucial to balance security/usability and log size, and in most cases, you need to compromise. We remove all non-critical and sensitive data from CG DevX core component logs. Users of the platform are responsible for defining the amount of logs they want to store and implementing additional log obfuscation rules for their workloads. Log Size and Retention Periods : When unmanaged, logs data tend to grow and consume a tremendous amount of resources to store, process, and be searchable. In some cases, compliance requirements are the driving factor behind high retention periods. Reducing log sizes makes them easier to manage and analyze. Making logs non-indexable by moving them to cold storage could save resources and allow complying with store time requirements. Log data of CG DevX internal components has a 14-day retention period by default. Out of the box, we do not provide a mechanism to offload data to cold storage, such functionality could be easily added on top of the existing log management pipeline. Collecting Logs for Workload Resources Provisioned via IaC : To simplify troubleshooting all the logs should be stored and processed uniformly. When new cloud services are provisioned using CG DevX IaC, we will automatically configure cloud-native logging, and ingest those logs. Incident Response Lack of Context About the Incident : When the incident lacks contextual information, the response team struggles to understand the full scale of the problem, make the initial diagnosis, assess the priority, and communicate to the other responders, management, and customers. CG DevX provides environment, workload, and service ownership information, infrastructure and workload metrics, and logs. Users of the platform could extend that information by applying additional tags on resources and services that will be available during the incident. Extended metadata could also be provided with Cloud Asset Catalog that could be additionally implemented alongside CG DevX platform installation. Lack of Prioritization : Organizations operate with limited resources. Lack of a prioritization scheme can cause response teams to spend most of their time on alerts that may not involve any threat depleting those resources. With the reference implementation of the process, all the incidents in CG DevX have the highest priority, and an incident response team should prioritize incidents as they process them. Users of the platform could set a default priority based on incident type or source, or other meta-information to simplify their workflow and reduce the number of priority one incidents. Tools for Communication and Escalation : When a major incident occurs, you need to communicate quickly and effectively as success depends on getting the involvement of the right people at the right time. CG DevX provides out-of-the-box integration with a paging system, allowing to effectively patch in the right people when they are required. You could also notify people using a preferred communication channel via event notification integrations using emails or Slack chat.","title":"Shared Responsibility"},{"location":"capabilities/known_limitations/#known-challenges-and-limitations","text":"There are known challenges and limitations in capabilities provided by CG DevX. Solving some of them will make the platform more opinionated and experience tailored to a specific case, which we want to avoid. This allows us to draw a fine line on things that we want to provide with the reference implementation and customization that should be done on top of it. You could think of it in the most complex cases as a fork of the platform, which we don't want to do. Those customizations could also make further updates of the \"fork\" hard or even impossible. This sets a first line in a shared responsibility model between us as builders of CG DevX, and users of CG DevX. When building an Internal Developer Platform using CG DevX, there is a second line dividing responsibility between operators (AKA a platform team) and developers, where some responsibilities could also be shared. This division of responsibility is organization-specific and should be defined by the user of the CG DevX. The list below describes known challenges and our vision on means to manage them, and responsibility.","title":"Known Challenges and Limitations"},{"location":"capabilities/known_limitations/#runtime","text":"Data Persistence : It is still challenging even with persistent volumes. We do not recommend running non-fault-tolerant stateful components (DBs). Data Security : We provide encryption at rest and in transit for all CG DevX internal components and underlying services plus provide fine-grained RBAC policies, still storing sensitive data in containers or persistent volumes is not secure. Data Backup and Recovery : Currently we do not provide a built-in mechanism for Persistent Volumes (PV) data backup and recovery. To ensure that data is not lost in the event of a disaster, you should implement a backup and recovery strategy including offloading data to a separate location, and testing recovery procedures regularly. Data Management : CG DevX internal components like a logging and monitoring system have default lifecycle policies built-in. Responsibility for data produced by workloads is solely on the user of the platform, so you will need to implement data management practices (data lifecycle, archiving data, consistency monitoring, etc.) yourself. Data Compliance : Responsibility for data produced by workloads is solely on the user of the platform, so you may need to implement specific data privacy and security measures, including data masking, data retention policies, or data sovereignty.","title":"Runtime"},{"location":"capabilities/known_limitations/#vcs","text":"Collaboration and Branching Strategy : Working in a distributed team is not possible without a proper VCS. While changes done to different parts of the system are quite easy to manage, when those changes are linked or depend on each other and done by different teams, it becomes really hard to track and integrate them without proper conventions. We expect teams to use the GitFlow approach for all the IaC code and GitOps manifests (GitOps repositories) and do a cross-team code review for such changes. We strongly suggest using GitFlow for the workload codebase as it's already supported by the platform, but teams could also use the Trunk-based development approach. Versioning : Tracking versions, especially when other services depend on your specific package, could be quite challenging and could become messy. To simplify a standardized versioning process, we utilize semantic versioning and enforce its usage for deployments and Helm charts and strongly recommend using it for your workloads. History : Version Control System (VCS) should provide a clean and human-readable history of changes. To enable this, all the developers should follow the same convention when working with the source code. We strongly recommend following the Conventional Commits convention for commit messages. Changelog : VCS should provide users a detailed view of changes associated with a specific version of artifacts, which enables developers to quickly identify potential issues associated with the change. With the usage of Conventional Commits, changelogs could be generated automatically.","title":"VCS"},{"location":"capabilities/known_limitations/#iac","text":"Security & Compliance Requirements : Infrastructure is a matter of compliance fit. Thoughtful implementation of IaC enables security and compliance checks against code to detect potential issues faster. CG DevX provides a mechanism of policy/compliance as code checks according to security best practices. It\u2019s a user's responsibility to implement specific security and compliance checks using provided mechanisms. Infrastructure Ownership : Infrastructure is a core part of Business Service. CG DevX platform is a core infrastructure component holding multiple critical business products. The shift-left approach sets a clear assignment of infrastructure ownership, where operators are responsible for core platform services and development teams - for workloads and other cloud services provisioned via the platform those services depend on. Integration with Existing Resources : Customers may or may not have IaC used. Furthermore, if IaC is used, it may have various forms - Terraform, Crossplane, Pulumi, CloudFormation, etc. To be fully integrated with the platform when onboarding a workload, existing resources should be either imported into Terraform (or Crossplane for Workloads) or recreated. CG DevX provides an inventory of all resources and services in the form of remote Terraform state that could be consumed by other tools. Deployment into Existing Infrastructure : Some customers may already have existing Kubernetes cluster(s). While it\u2019s technically possible to install CG DevX components onto existing clusters, we cannot guarantee the integrity of the platform and uncompromised function of its core features, especially in the DevSecOps domain, thus decided not to support such a scenario.","title":"IaC"},{"location":"capabilities/known_limitations/#iac-automation","text":"Identifying and Fixing Security Issues : Security misconfigurations are the #1 type of incident as per-security reports. In a pipeline, the earlier you perform security scanning and testing, the more freedom operators and developers will have to accommodate all security feedback. With CG DevX all the necessary checks are built into the pipeline, so security is an integral part of your code, not an add-on feature. Configuration of those features is up to the user of CG DevX. Access Management : In cloud-native deployments compared to classic ones, access management has higher associated security risks. Integrating the necessary security checks into a delivery pipeline will improve overall security and simplify migration towards zero trust. With CG DevX IAM policies (role bindings and policy metadata) are written down as code and managed using the same process as the rest of the infrastructure. Access to infrastructure could be restricted or reduced to read-only access as all the changes are applied using change management automation. Changes sign-off is done by peer review reducing security risks. Audit Logs : Audit trails are a big part of SOC2, HIPPA, and PCI compliance. There is a misconception that it could only be achieved by using specialized systems. When implemented correctly, your VCS change log could be leveraged as an audit trail. CG DevX IaC PR automation provides a full change log as part of VCS history that could also be offloaded to other storage. Upgrades/Migrations of Stateful Infrastructure Components : Stateful components, especially data stores and databases vs GitOps and immutable infrastructure. Given all pros and cons of approaches, with CG DevX we mostly rely on immutable infrastructure to get better repeatability, reduce chances of configuration drift, and have versioning with change history. We only support in-place updates and configuration management for K8s cluster upgrades. We strongly recommend that you use managed cloud resources where all the maintenance and update procedures are done by the cloud provider. Cross-Components Dependencies : Modern cloud-native applications look like a web of interdependent services, especially when adopting microservices architecture. On top of that, you have cloud-native services and external 3rd party services, like payment gateways, your application depends on. When any of these dependencies fail, it can have cascading impacts on the rest of your services and on the application as a whole. With CG DevX, your dependencies on cloud services are documented as IaC and tagged properly. It\u2019s your responsibility as a user to keep track of your application dependencies. CG DevX could be extended by Cloud Assets Catalog, as it's not a part of standard installation. Drift Detection : Drift refers to the deviation of actual infrastructure from its intended or expected state. This can occur due to various reasons, such as manual configuration changes, human error, etc. Drift can result in configuration inconsistencies, security vulnerabilities, and performance issues, making it essential to detect and correct it as soon as possible. CG DevX could detect drift for resources provisioned via platform IaC and highlight it in special reports using tools that scan and analyze the infrastructure. While we include resources not provisioned via IaC as part of those reports, we could not detect changes unless resources are imported to and managed by platform IaC solution.","title":"IaC Automation"},{"location":"capabilities/known_limitations/#workload-ci","text":"Feature Velocity Issues : Multiple factors could contribute to delays in release cycles. It could happen due to lack of automation, code quality, team motivation, etc. CG DevX provides a fully automated delivery pipeline and provides a detailed view on Lead Time for Changes and Deployment Frequency as part of DORA key metrics. Flawed Automated Testing : Automated tests with no automation are being spoiled over time. They must be incorporated into CI pipelines and closely monitored. CG DevX provides well-defined extension points in CI pipelines to integrate quality gates such as automated tests. The code coverage metric is tracked using integration with a Static Code Analysis component. It\u2019s your responsibility as a user to track Functional coverage of your tests. Our default CI pipeline has predefined quality gate rules based on code coverage and the percentage of passed tests. Security Vulnerabilities : Vulnerabilities in contrast to bugs are usually unseen unless they were exposed. Shifting security to the left is the first and foremost priority. With CG DevX all the necessary checks are built into the pipeline, so security is an integral part of your code, not an add-on feature. We strongly recommend taking a step further and using Vulnerability Scanner plugins for IDE, like one for Trivy from Aqua Security. CI Process Visibility : CI may generate an excessive amount of logs that have little to no business value. Therefore, specific metrics (KPIs) and events must be identified and agreed on, and this list should be reviewed with each change to the pipeline. CG DevX has a predefined set of metrics and events collected by the system out of the box to enable DORA key metrics. Collaboration Process : Proper branching strategy and naming conventions are the prerequisites for successful CI implementation. Lack of those attributes increases mess in a software delivery process and sometimes becomes detrimental rather than helpful. To simplify and standardize the process, we recommend using GitFlow, semantic versioning, and conventional commits. Integration with Other Tools : Automation requires explicit flows across the sequence of tools in the software development lifecycle to ensure actionable feedback and improvements. CG DevX CI pipeline consists of pre-defined building blocks and provides clear and well-defined extension points.","title":"Workload CI"},{"location":"capabilities/known_limitations/#workload-cd","text":"Multi-Tenancy : In a K8s context, multi-tenancy is when multiple teams are working on different projects in the same environment. With highly configurable access control CG DevX will restrict deployments to a specific namespace and user access to specific pipelines. RBAC configurations are automatically generated when provisioning new workloads via CG DevX CLI. Monitoring and Alerting : Out of the box CD metrics such as application reconciliation performance, the controller queue depth, and the number of application sync operations will be available via CG DevX observability. Poor Communication Across Teams : Situations when teams rely on tribal knowledge of deployment details and dates could pose a serious threat to the quality of the service. With CG DevX we try to solve this by providing a high degree of automation and transparency of delivery pipelines. Adoption of proper change management and notification mechanisms is the responsibility of the user of the platform. Configuration Drift : Configuration differences between the target cluster of a deployment pipeline can cause it to fail. CG DevX CD module could detect configuration drifts. While out-of-the-box security policy will not allow users to change configuration directly in the cluster, it is the user's responsibility to ensure those policies are not altered. The CD module could auto-sync changes done directly in the cluster back to the GitOps repository, but we strongly suggest not using this option as it will reduce visibility and trackability of changes. Repository Management : According to many best practices, application code, Kubernetes manifests, and IaC code should be separated as it simplifies management, helps keep a clean audit trail, and controls access. There is no one size fits all approach here. To compromise between all pros and cons we provide two repositories per workload, one for application source code, and another for Kubernetes manifests, Helm charts, and IaC.","title":"Workload CD"},{"location":"capabilities/known_limitations/#workload-code-quality","text":"No Common Code Quality Standards : To produce clean and maintainable code, the organization should understand the importance of and adopt code quality standards. CG DevX Code Quality module could facilitate standard adoption, but full responsibility lies on the organization. Code Quality : Each programming language has its definition of code quality. In some cases, those definitions should be adjusted or extended. CG DevX Code Quality ships with predefined code quality rules based on industry standards and best practices for each supported programming language. While there is a default one, developers should pick the set of rules they are going to use. Quality Gates : Quality Gates define a set of conditions to be met for code quality to be considered sufficient. We suggest using Quality Gates that mandate that all new code must include at least 80% test coverage, and no diagnosed security issues and provide a default configuration with those rules. We do not enforce this Quality Gate and developers should decide what exact rules and values they are going to use. False-Positives : Not all analysis rules are equally suitable in each situation and could produce false positives. Lots of them may overwhelm developers making it difficult to focus on other more important fixes. Each team should adjust default code quality rules to reduce the number of false positives. Unit Test Coverage Reports : Test coverage information could greatly improve understanding of code quality. CG DevX Code Quality module does not generate the coverage report itself. The user must set up a language-specific tool to calculate and produce a code coverage report as part of the delivery pipeline. We do provide configuration samples for GitHub and Argo Workflows that could be reused.","title":"Workload Code Quality"},{"location":"capabilities/known_limitations/#artifact-management","text":"Ownership and Lineage Tracking : To enable cost tracking and simplify the process of incident response, all the resources should have strict assigned owners, and other associated metadata. CG DevX Artifacts management stores artifact metadata such as build history, dependencies, readme, and other user-defined information. The user is responsible for setting all the necessary metadata in addition to the one set by default and required by the pipeline to operate. Lifecycle Management : CI/CD pipeline could produce dozens of artifacts that could require a significant amount of storage. Artifacts lifecycle management and storage quotas should be implemented to keep storage under control and comply with organizational standards. We provide an automated clean-up process and an easy way to set storage quotas per workload. Vulnerability Scanning : It\u2019s important to have a multi-layer vulnerability detection system and check artifacts on each step of the delivery pipeline. We provide static analysis of vulnerabilities with Trivy out of the box, and it\u2019s possible to connect additional scanners if required by compliance. Disaster Recovery : To enable recovery of the system in case of region failure, all the artifacts should be offloaded to a different region. With CG DevX Artifacts management, artifacts could be replicated to different artifact stores in different locations. Public Registry Request Limits and Latency : Certain artifacts must always be available for the production environment to pull them. Latency of downloading artifacts and public registry rate limits could affect production RTO. CG DevX Artifacts management could serve as a local cache for publicly available artifacts. This also covers a scenario when the public registry becomes unavailable.","title":"Artifact Management"},{"location":"capabilities/known_limitations/#devsecops","text":"Issues Overload : High volumes of potential vulnerabilities and misconfigurations make it difficult to focus on the most important issues. Without risk-based prioritization, developers might spend time on issues that might not even represent a risk to the organization. CG DevX DevSecOps modules could provide priority for some issues, but they should only be considered as recommendations. Balancing Speed and Security : Modern development is all about speed and agility, and every team needs to keep pace. While CG DevX provides a security foundation that is agile, adaptable, and fast it could not cover all the potential risks and standards. The user is responsible for building those additional capabilities on top of the existing foundation. Lack of Resources and Knowledge Gap : Most organizations lack adequate working knowledge of DevSecOps practices. With limited resources, bridging this gap becomes even more difficult. CG DevX provides a common platform for operators and developers to collaborate with security teams or security consultants. While we provide the implementation of security best practices, it will not be able to fully compensate for knowledge gaps. Roles and Responsibility Alignment : It is challenging to align roles and responsibilities in dynamic environments. CG DevX follows a shift-left approach where developers and operators are responsible for the security of their workloads and the platform and provides a set of measures to give security feedback at all stages of the delivery pipeline and as early as possible. Workload Cloud Services Dependency : Cloud services have high complexity associated with them, especially when integrating with security tools that were not designed to be cloud-native. As CG DevX DevSecOps primarily focuses on workload code, its dependencies, container runtime, and associated manifests it could not provide full coverage for all workload-specific cloud-native services, especially when the workload relies on many of them.","title":"DevSecOps"},{"location":"capabilities/known_limitations/#monitoring","text":"Complexity : IDP is a system of services considered a Complex System and is composed of a large number of simple parts. There are a significant amount of interconnections and interactions between parts and at different space and time scales. The whole platform and workloads become adaptive and respond to changes in the environment as well as the system itself. With lots of dynamic monitoring solutions should adopt those changes and continuously track the state of the whole system. CG DevX Monitoring module provides a unified way of metric collection, automated services discovery, and health checks that are automatically applied when the platform scales in and out. Clustered Environment : When platform and workload components are located at different places (services, compute nodes, etc.) and packed together, monitoring becomes a challenge. It becomes critical to properly track and attribute resource usage as one application could cause failures in others, and operators and developers should be able to quickly identify and resolve the root cause. CG DevX Monitoring module provides an easy way to zoom into a specific cluster/node/namespace/pod helping you to identify \u201cnoisy neighbors\u201d. Data Volume and Retention Periods : When unmanaged data tend to grow and consume a tremendous amount of resources to store, process, and be searchable. Reducing the number of metrics you collect and process makes it easier to manage and analyze. CG DevX collects only the most critical metrics to reduce data volume, also metrics of internal components have a 14-day retention period by default. When introducing new metrics, users of the platform should understand and be accountable for the additional resources required to process new data. Shared Services : Sharing services enables optimum utilization of resources. On the other hand, monitoring shared applications is quite challenging. The risk lies in the fact that more consumption of resources by one application would affect the performance of other applications. This is especially true for multi-tenant applications. CG DevX provides continuous monitoring and labeling for all the services. For shared services as part of workloads, users are required to properly configure monitoring dashboards and labeling so that they could track and attribute usage to a specific workload. Monitor Workload Resources : Most workloads have dependencies on other cloud resources that due to reliability, availability, or compliance reasons could not be running on a Kubernetes-based runtime. CG DevX Monitoring module automatically collects metrics exposed by cloud-native services provisioned by CG DevX IaC module when they are provisioned using cloud-native monitoring mechanisms provided by the cloud provider and forwarding all the metrics into the Monitoring module. It also creates a minimal set of alarms as a backup circuit for the monitoring pipeline running within the platform. When resources are not provisioned via CG DevX IaC user takes full responsibility for monitoring them by using cloud-native or third-party monitoring solutions or plugging them into CG DevX Monitoring. Workload Rightsizing : The process of matching actual workload resource requirements with estimations is really hard especially for new applications. Also, requirements tend to change over time. CG DevX provides a detailed utilization dashboard on the cluster, cluster node, workload (Kubernetes namespace), and service within the workload ( Kubernetes pod) levels and recommendations on resource rightsizing and associated cost savings. More detailed cost visibility dashboards and cost-saving recommendations could be provided by CG DevX Cost Visibility.","title":"Monitoring"},{"location":"capabilities/known_limitations/#log-management","text":"Sensitive Data : It's vital to obfuscate PII and other sensitive data in your logs because you can ensure confidentiality and integrity of the data while continuing to use logs to analyze system behavior and troubleshoot issues. You can mitigate the risk of data breaches and unauthorized access and still get your ongoing debugging work done. We remove sensitive data such as secrets from delivery pipeline logs. Users of the platform should implement additional log obfuscation rules and mechanisms for their workloads. Balance Security/Usability/Log Size : To make sure the logs remain a valuable tool for monitoring system performance and troubleshooting, it\u2019s crucial to balance security/usability and log size, and in most cases, you need to compromise. We remove all non-critical and sensitive data from CG DevX core component logs. Users of the platform are responsible for defining the amount of logs they want to store and implementing additional log obfuscation rules for their workloads. Log Size and Retention Periods : When unmanaged, logs data tend to grow and consume a tremendous amount of resources to store, process, and be searchable. In some cases, compliance requirements are the driving factor behind high retention periods. Reducing log sizes makes them easier to manage and analyze. Making logs non-indexable by moving them to cold storage could save resources and allow complying with store time requirements. Log data of CG DevX internal components has a 14-day retention period by default. Out of the box, we do not provide a mechanism to offload data to cold storage, such functionality could be easily added on top of the existing log management pipeline. Collecting Logs for Workload Resources Provisioned via IaC : To simplify troubleshooting all the logs should be stored and processed uniformly. When new cloud services are provisioned using CG DevX IaC, we will automatically configure cloud-native logging, and ingest those logs.","title":"Log Management"},{"location":"capabilities/known_limitations/#incident-response","text":"Lack of Context About the Incident : When the incident lacks contextual information, the response team struggles to understand the full scale of the problem, make the initial diagnosis, assess the priority, and communicate to the other responders, management, and customers. CG DevX provides environment, workload, and service ownership information, infrastructure and workload metrics, and logs. Users of the platform could extend that information by applying additional tags on resources and services that will be available during the incident. Extended metadata could also be provided with Cloud Asset Catalog that could be additionally implemented alongside CG DevX platform installation. Lack of Prioritization : Organizations operate with limited resources. Lack of a prioritization scheme can cause response teams to spend most of their time on alerts that may not involve any threat depleting those resources. With the reference implementation of the process, all the incidents in CG DevX have the highest priority, and an incident response team should prioritize incidents as they process them. Users of the platform could set a default priority based on incident type or source, or other meta-information to simplify their workflow and reduce the number of priority one incidents. Tools for Communication and Escalation : When a major incident occurs, you need to communicate quickly and effectively as success depends on getting the involvement of the right people at the right time. CG DevX provides out-of-the-box integration with a paging system, allowing to effectively patch in the right people when they are required. You could also notify people using a preferred communication channel via event notification integrations using emails or Slack chat.","title":"Incident Response"},{"location":"capabilities/reference_implementation/","text":"Reference Implementation The goal of CG DevX is to provide a reference implementation of a Platform with the ability to switch the implementation of core services to make the reference implementation less opinionated. Currently, we allow the selection of three components for the reference implementation template before rendering/materialization. These components are: Hosting (cloud) provider Git provider DNS registrar For some of the other components, it is possible to change or turn them on/off using a \"feature flags\" like mechanism post-rendering. Later, this functionality will be available before rendering via cgdevxcli .","title":"Reference Implementation"},{"location":"capabilities/reference_implementation/#reference-implementation","text":"The goal of CG DevX is to provide a reference implementation of a Platform with the ability to switch the implementation of core services to make the reference implementation less opinionated. Currently, we allow the selection of three components for the reference implementation template before rendering/materialization. These components are: Hosting (cloud) provider Git provider DNS registrar For some of the other components, it is possible to change or turn them on/off using a \"feature flags\" like mechanism post-rendering. Later, this functionality will be available before rendering via cgdevxcli .","title":"Reference Implementation"},{"location":"developers_guide/concept/","text":"The goal of CG DevX is to improve developer productivity by enhancing the developer experience. One of the biggest challenges in a modern cloud-native environment is the constantly increasing cognitive load. Working with microservice architectures in a cloud-native setup often requires end-to-end knowledge of Kubernetes (K8s), infrastructure provisioning, deployment pipelines, and configuration management. CG DevX provides layers of abstraction to simplify day-to-day tasks for developers. CG DevX: Hides complexity associated with the shift-left paradigm and enables a PaaS-like experience: K8s manifests and configuration management are simplified with pre-built templates and code generation. IaC management is limited to application-specific resources only and covered by ready-to-use templates; cloud complexity is abstracted away by GitOps. Implements security scanning and testing at early stages of the pipeline, enabling developers to accommodate security feedback faster. Reduces operational friction in CI/CD with better outcomes: Working pipelines out of the box. Well-defined extension points. Automated rollbacks. Provides observability and log management out of the box: Automated discovery. Pre-built dashboards.","title":"Concept"},{"location":"developers_guide/artifacts/registry/","text":"The CG DevX reference implementation provides image and helm chart repository capabilities using Harbor. To access Harbor, follow the link in the platform GitOps repository readme file ( README.md ), or provided by operators (AKA a platform team). Harbor is configured to use Vault as its OIDC provider and will automatically redirect you to the Vault login page, which will look like this: Each workload in CG DevX has a project in Harbor associated with it. A project contains all the repositories for a workload. Control (RBAC) is applied to projects, so that only users who belong to a specific team can access it. Delivery pipelines provided by CG DevX are pre-configured to work with the registry. Also, the registry provides static analysis of vulnerabilities in images through the integration with Aqua Security Trivy.","title":"Registry"},{"location":"developers_guide/artifacts/registry_proxy/","text":"Registry proxy To speed up the delivery process and avoid hitting external image registry limits, CG DevX provides proxy registries for popular Docker Image Registries: dockerhub-proxy https://hub.docker.com gcr-proxy https://gcr.io k8s-gcr-proxy https://k8s.gcr.io quay-proxy https://quay.io To start using the proxy cache, configure your docker pull commands or pod manifests to reference the proxy cache project by adding / / as a prefix to the image tag. For example: docker pull <harbor_server_name>/<proxy_project_name>/goharbor/harbor-core:dev To pull official images or from single level repositories, make sure to include the \u2018library\u2019 namespace. docker pull <harbor_server_name>/<proxy_project_name>/library/awesome-image:latest All pipelines supplied with CG DevX use proxy.","title":"Registry proxy"},{"location":"developers_guide/artifacts/registry_proxy/#registry-proxy","text":"To speed up the delivery process and avoid hitting external image registry limits, CG DevX provides proxy registries for popular Docker Image Registries: dockerhub-proxy https://hub.docker.com gcr-proxy https://gcr.io k8s-gcr-proxy https://k8s.gcr.io quay-proxy https://quay.io To start using the proxy cache, configure your docker pull commands or pod manifests to reference the proxy cache project by adding / / as a prefix to the image tag. For example: docker pull <harbor_server_name>/<proxy_project_name>/goharbor/harbor-core:dev To pull official images or from single level repositories, make sure to include the \u2018library\u2019 namespace. docker pull <harbor_server_name>/<proxy_project_name>/library/awesome-image:latest All pipelines supplied with CG DevX use proxy.","title":"Registry proxy"},{"location":"developers_guide/cd/delivery/","text":"The CG DevX reference implementation provides continuous delivery using a tool called ArgoCD. ArgoCD is K8s and GitOps centric, and provides detailed visualization. To access ArgoCD, follow the link in the platform GitOps repository readme file ( README.md ), or provided by operators (AKA a platform team). ArgoCD is configured to use Vault as its OIDC provider. You will need to press the Log in via Vault button, which will redirect you to Vault login page, which will look like this: CG DevX creates one project per workload and applies RBAC to limit access to workload dashboards and management. All workload environments will be listed on the ArgoCD project page. ArgoCD also provides a detailed view of all K8s resources associated with a workload, statistics on the pod and node level, and ingress configuration.","title":"Delivery"},{"location":"developers_guide/cd/promotion/","text":"The CG DevX provides workload promotion as a part of workload GitOps repository. Promotion is done using a combination of Git provider native workflow and Argo Workflow. Argo Workflow provides platform wide workflow templates, detailed visualization of the process, integrated with secret management, and scoped down to a workload. To access Argo Workflow, follow the link in the platform GitOps repository readme file ( README.md ), or provided by operators (AKA a platform team). Argo Workflow is configured to use Vault as its OIDC provider. CG DevX also provides templates to use only Git provider for promotions. . \u251c\u2500\u2500 .argo \u2502 \u2514\u2500\u2500 promote-wf.yaml \u251c\u2500\u2500 .github \u2502 \u251c\u2500\u2500 terraform.yaml \u2502 \u2514\u2500\u2500 workflows \u2502 \u251c\u2500\u2500 promote_gha_only.yaml_gha \u2502 \u2514\u2500\u2500 promote.yaml ... Promotion is done by copying of manifest files from source to target environment. For more details please see workload environment modeling Source and target environment definitions could be extended by updating promote.yaml workflow. Options should map to your environment names (folder names), or you could implement additional transformer. --- # Promote a GitOps workload from one environment to the next name: Promote workload # Controls when the workflow will run on: # Allows you to run this workflow manually from the Actions tab workflow_dispatch: inputs: source_env: description: Source environment required: true default: dev type: choice options: [ <your environment folder names> ] target_env: description: Target environment required: true default: sta type: choice options: [ <your environment folder names> ] ...","title":"Promotion"},{"location":"developers_guide/ci/argo_workflows/","text":"Argo workflows: basic flows and templates CG DEVX's argo workflows approach implements following execution scheme: flowchart TD node_A[\"GitHub Action Job Step\"] --> node_B[\"argo workflow in .argo/\"] node_B --> node_C[\"cwft\"] node_C --> node_D[\"DAG steps\"] linkStyle 1 stroke:#000000 Basic workflows build , which includes build , lint and check steps registry_put places successfully builded image into the Harbor image repository crane_img_tag tags images for unchanged services in the same apps structure version_change promotes version in the version.yaml file in workload's gitops-repo. Cluster workflow templates For every action workflows use cluster workflow templates consist of one or several steps to perform. In case of a list of parameters to process templates from clusterworkflow templates can be called iteratively. Cluster workflow templates are used to implement atomic phases of services images build process, except build_chain_p_cwft , which have a DAG inside and called in a special way: inside so-called workflow-of-workflows structure in .argo/build-wow-wf.yaml . build_chain_p_cwft megalinter-cwft \u2014 lints service code using Megalinter trivy-fs-s3-cwft \u2014 checks service fs with Trivy kaniko-s3-p-cwft \u2014 builds tar-image with Kaniko crane-s3-p-cwft \u2014 tags and puts image into the Harbor image repo crane-img-tag-cwft \u2014 tags unchanged (if any) services latest images and tag it with the same tag as changed version-changer-cwft \u2014 change version in version.yaml in gitops-repo","title":"Argo workflows: basic flows and templates"},{"location":"developers_guide/ci/argo_workflows/#argo-workflows-basic-flows-and-templates","text":"CG DEVX's argo workflows approach implements following execution scheme: flowchart TD node_A[\"GitHub Action Job Step\"] --> node_B[\"argo workflow in .argo/\"] node_B --> node_C[\"cwft\"] node_C --> node_D[\"DAG steps\"] linkStyle 1 stroke:#000000","title":"Argo workflows: basic flows and templates"},{"location":"developers_guide/ci/argo_workflows/#basic-workflows","text":"build , which includes build , lint and check steps registry_put places successfully builded image into the Harbor image repository crane_img_tag tags images for unchanged services in the same apps structure version_change promotes version in the version.yaml file in workload's gitops-repo.","title":"Basic workflows"},{"location":"developers_guide/ci/argo_workflows/#cluster-workflow-templates","text":"For every action workflows use cluster workflow templates consist of one or several steps to perform. In case of a list of parameters to process templates from clusterworkflow templates can be called iteratively. Cluster workflow templates are used to implement atomic phases of services images build process, except build_chain_p_cwft , which have a DAG inside and called in a special way: inside so-called workflow-of-workflows structure in .argo/build-wow-wf.yaml . build_chain_p_cwft megalinter-cwft \u2014 lints service code using Megalinter trivy-fs-s3-cwft \u2014 checks service fs with Trivy kaniko-s3-p-cwft \u2014 builds tar-image with Kaniko crane-s3-p-cwft \u2014 tags and puts image into the Harbor image repo crane-img-tag-cwft \u2014 tags unchanged (if any) services latest images and tag it with the same tag as changed version-changer-cwft \u2014 change version in version.yaml in gitops-repo","title":"Cluster workflow templates"},{"location":"developers_guide/ci/build_routine/","text":"Build routine execution GHA workflow's step build is used to submit the argo workflow build-loop-xxxx (-xxxx and similar parts of workload names here and further are auto-generated), which in it's turn generate series of workflows (one for each service) based on build-chain-p-cwft clusterworkflow template. build-chain-p-cwft has DAG workflow structure and run templates from these clusterworkflow templates : kaniko-s3-p-cwft megalinter-cwft trivy-fs-s3-cwft for kaniko , megalinter and trivy rescpectively. All the workflow parameters, once calculated in Github Action workflow , are translated inside argo workflows templates structures sequentially. --- title: \"Build routine invocation\" --- flowchart n1[\"`Github Action Step __build__`\"] subgraph \"./argo/build-wow-wf.yaml\" n2(build-loop-xxxx) end subgraph \"`build-service-SN-xxxx`\" n3([\"`build-chain-p-cwft: DAG: - kaniko-s3-p-cwft - megalinter-cwft - trivy-fs-s3-cwft `\"]) end subgraph \"`build-service-SN-yyyy`\" n4([\"`build-chain-p-cwft: DAG: - kaniko-s3-p-cwft - megalinter-cwft - trivy-fs-s3-cwft `\"]) end subgraph \"`build-service-SN-zzzz`\" n5([\"`build-chain-p-cwft: DAG: - kaniko-s3-p-cwft - megalinter-cwft - trivy-fs-s3-cwft `\"]) end n1-->n2-->n3 & n4 & n5","title":"Build routine execution"},{"location":"developers_guide/ci/build_routine/#build-routine-execution","text":"GHA workflow's step build is used to submit the argo workflow build-loop-xxxx (-xxxx and similar parts of workload names here and further are auto-generated), which in it's turn generate series of workflows (one for each service) based on build-chain-p-cwft clusterworkflow template. build-chain-p-cwft has DAG workflow structure and run templates from these clusterworkflow templates : kaniko-s3-p-cwft megalinter-cwft trivy-fs-s3-cwft for kaniko , megalinter and trivy rescpectively. All the workflow parameters, once calculated in Github Action workflow , are translated inside argo workflows templates structures sequentially. --- title: \"Build routine invocation\" --- flowchart n1[\"`Github Action Step __build__`\"] subgraph \"./argo/build-wow-wf.yaml\" n2(build-loop-xxxx) end subgraph \"`build-service-SN-xxxx`\" n3([\"`build-chain-p-cwft: DAG: - kaniko-s3-p-cwft - megalinter-cwft - trivy-fs-s3-cwft `\"]) end subgraph \"`build-service-SN-yyyy`\" n4([\"`build-chain-p-cwft: DAG: - kaniko-s3-p-cwft - megalinter-cwft - trivy-fs-s3-cwft `\"]) end subgraph \"`build-service-SN-zzzz`\" n5([\"`build-chain-p-cwft: DAG: - kaniko-s3-p-cwft - megalinter-cwft - trivy-fs-s3-cwft `\"]) end n1-->n2-->n3 & n4 & n5","title":"Build routine execution"},{"location":"developers_guide/ci/code_quality/","text":"The CG DevX reference implementation provides static code analysis capabilities using SonarQube. To access SonarQube, follow the link in the platform GitOps repository readme file ( README.md ), or provided by operators (AKA a platform team). SonarQube is configured to use Vault as its OIDC provider. You need to press the Login with OpenID SSO Connect button, which will redirect you to the Vault login page, which will look like this: Each workload in CG DevX has a project in SonarQube associated with it. Control (RBAC) is applied to projects, so that only users that belong to a specific team can access it. CG DevX delivery pipelines are pre-configured to work with SonarQube. However, as SonarQube rules are language-specific, they should be configured by the team working with the workload.","title":"Code quality"},{"location":"developers_guide/ci/devsecops/","text":"The CG DevX provides integration with Vulnerability and misconfiguration scaner as part of CI process. Aqua Security Trivy is sed as default scaner. Result of the scan are stored in SARIF format and uploaded to Argo Workflow build artifact storage and linked to Git.","title":"Devsecops"},{"location":"developers_guide/ci/github_action_workflow/","text":"Github Action Workflow Structure CI Github Action start is triggered by push new tag to the workload repo. All the CI chain elements run from the steps of job multi_service_parallel_build described in multi_service_parallel_build.yml . If one of the steps fails, the Github action fails completely. Some steps are simple bash executed on the runner, another are submissions of relatively complex argo workflows. The workflow structure is linear: steps are executed one after another. --- title: multi_service_parallel_build workflow chart --- flowchart n1[\"`**semver_chk**`\" cheks if tag matchs SEMVER format ]--if not SEMVER tag, exit--> n2[\"`**actions/checkout@v4**`\" ]--> n3[\"`**argo_cli_install** installs _argo cli_ on runner`\"]--> n4[\"`**build**`\" checks build conditions, detects repo changes and submits workflow of services build workflows ]-- if any build fails, exit --> n41[\"`**trivy_libs** if _#quot;apps#quot;_ structure, submits _trivy-libs-wf_ workflow to scan shared libs source code in _libs_ directory. `\"]--> n5[\"`**registry_put** submits _crane_ workflow to tag and put images to Harbor`\" ]-- if any put fails, exit --> n6[\"`**up_tags** submits _crane-img-tag_ workflow to up the tag for unchanged services (if any) `\"]--> n7[\"`**version_change** submits _version_change_ workflow to change version in gitops repo `\"]","title":"Github Action Workflow Structure"},{"location":"developers_guide/ci/github_action_workflow/#github-action-workflow-structure","text":"CI Github Action start is triggered by push new tag to the workload repo. All the CI chain elements run from the steps of job multi_service_parallel_build described in multi_service_parallel_build.yml . If one of the steps fails, the Github action fails completely. Some steps are simple bash executed on the runner, another are submissions of relatively complex argo workflows. The workflow structure is linear: steps are executed one after another. --- title: multi_service_parallel_build workflow chart --- flowchart n1[\"`**semver_chk**`\" cheks if tag matchs SEMVER format ]--if not SEMVER tag, exit--> n2[\"`**actions/checkout@v4**`\" ]--> n3[\"`**argo_cli_install** installs _argo cli_ on runner`\"]--> n4[\"`**build**`\" checks build conditions, detects repo changes and submits workflow of services build workflows ]-- if any build fails, exit --> n41[\"`**trivy_libs** if _#quot;apps#quot;_ structure, submits _trivy-libs-wf_ workflow to scan shared libs source code in _libs_ directory. `\"]--> n5[\"`**registry_put** submits _crane_ workflow to tag and put images to Harbor`\" ]-- if any put fails, exit --> n6[\"`**up_tags** submits _crane-img-tag_ workflow to up the tag for unchanged services (if any) `\"]--> n7[\"`**version_change** submits _version_change_ workflow to change version in gitops repo `\"]","title":"Github Action Workflow Structure"},{"location":"developers_guide/ci/integration/","text":"The CG DevX reference implementation provides continuous integration capabilities using Git native pipelines (GitHub Actions, GitLab Pipelines), and ArgoWorkflow. ArgoWorkflow is used to integrate natively with K8s services. To access ArgoWorkflow, follow the link in the platform GitOps repository readme file ( README.md ), or provided by operators (AKA a platform team). ArgoWorkflow is configured to use Vault as OIDC provider. You need to press the Login button on the left side of the page, which will redirect you to the Vault login page, which will look like this: Each workload in CG DevX has a namespace in ArgoWorkflow associated with it. Control (RBAC) is applied to projects, so that only users of a specific team can access it. CG DevX ArgoWorkflow pipelines are pre-configured to work with static code analysis tools, linters, security scanners, and image registries. They also use automated cleanup to remove old artifacts.","title":"Integration"},{"location":"developers_guide/ci/kaniko_build/","text":"Image build: Kaniko The main step of CI chain is a Kaniko build. Kaniko is a well-known tool for building docker images inside a container without docker itself. Kaniko is called from the GitHub Action workflow using a cascade of Argo workflows and their templates. Kaniko cwft kaniko clusterworkflow template performs the checkout of workload's repo as a hardwired artifact, builds a tar-image of service and put it as an output artifact to the pre-configured artifact registry (S3-bucket in AWS usage case). While it's possible to utilize Kaniko itself to check out the repository and place the final artifact in the Harbor registry, this approach was intentionally avoided for the following reasons: reduce dependence from Kaniko; logically separate build and push processes. By default, cache for RUN layers and mirrors of popular repositories for base images are used, but this behavior could be switched using corresponding env variables in GitHub Action workflow . Cache stores on per-service basis in Harbor, as well as proxy-repositories are also special projects in Harbor. To bypass the long-term Kaniko bug with registry URLs dockerfiles are edited right in place during workflow execution to replace the basic images names with a combination of Harbor proxy project name and the name of the image. A snippet from kaniko clusterworkflow template (Note the values transferred through workflow parameters): --dockerfile={{workflow.parameters.dockerfile}} \\ --context=dir:///build/{{workflow.parameters.build-context}} \\ --no-push \\ --tar-path=/tmp/{{workflow.parameters.wl-service-name}}.tar \\ --registry-mirror={{workflow.parameters.kaniko-registry-mirror}} \\ --snapshot-mode=time \\ --use-new-run \\ --compressed-caching=false \\ --cache={{workflow.parameters.kaniko-cache}} \\ --cache-run-layers \\ --cache-repo={{workflow.parameters.kaniko-cache-repo}}/kaniko-cache/{{workflow.parameters.workload-name}}-{{workflow.parameters.wl-service-name}}","title":"Image build: Kaniko"},{"location":"developers_guide/ci/kaniko_build/#image-build-kaniko","text":"The main step of CI chain is a Kaniko build. Kaniko is a well-known tool for building docker images inside a container without docker itself. Kaniko is called from the GitHub Action workflow using a cascade of Argo workflows and their templates.","title":"Image build: Kaniko"},{"location":"developers_guide/ci/kaniko_build/#kaniko-cwft","text":"kaniko clusterworkflow template performs the checkout of workload's repo as a hardwired artifact, builds a tar-image of service and put it as an output artifact to the pre-configured artifact registry (S3-bucket in AWS usage case). While it's possible to utilize Kaniko itself to check out the repository and place the final artifact in the Harbor registry, this approach was intentionally avoided for the following reasons: reduce dependence from Kaniko; logically separate build and push processes. By default, cache for RUN layers and mirrors of popular repositories for base images are used, but this behavior could be switched using corresponding env variables in GitHub Action workflow . Cache stores on per-service basis in Harbor, as well as proxy-repositories are also special projects in Harbor. To bypass the long-term Kaniko bug with registry URLs dockerfiles are edited right in place during workflow execution to replace the basic images names with a combination of Harbor proxy project name and the name of the image. A snippet from kaniko clusterworkflow template (Note the values transferred through workflow parameters): --dockerfile={{workflow.parameters.dockerfile}} \\ --context=dir:///build/{{workflow.parameters.build-context}} \\ --no-push \\ --tar-path=/tmp/{{workflow.parameters.wl-service-name}}.tar \\ --registry-mirror={{workflow.parameters.kaniko-registry-mirror}} \\ --snapshot-mode=time \\ --use-new-run \\ --compressed-caching=false \\ --cache={{workflow.parameters.kaniko-cache}} \\ --cache-run-layers \\ --cache-repo={{workflow.parameters.kaniko-cache-repo}}/kaniko-cache/{{workflow.parameters.workload-name}}-{{workflow.parameters.wl-service-name}}","title":"Kaniko cwft"},{"location":"developers_guide/ci/megalinter/","text":"Megalinter To lint all the source code Megalinter by OX Security is used. Linters configuration could be changed in supplied .mega-linter.yml using the configuration notes at https://megalinter.io/latest/config-file . To avoid trying miriades of supported linters it's good idead to disable some of them in .mega-linter.yml or use ' flavor' image as described . ENV variables and image could be changed in megalinter cluster workflow template JSON report as well as SARIF are saved as S3 output artifacts for further observation. Inputs: {{workflow.parameters.repo}} {{workflow.parameters.tag}} {{workflow.parameters.dockerhub-registry-proxy}} {{workflow.parameters.workload-name}} {{workflow.parameters.wl-service-name}} {{workflow.parameters.wl-service-dir}} Outputs: - name: megalinter-report-sarif path: /tmp/megalinter-report.sarif s3: key: \"{{workflow.parameters.workload-name}}/{{workflow.parameters.tag}}/{{workflow.parameters.wl-service-name}}-megalinter-report-sarif\" - name: megalinter-report-json path: /tmp/mega-linter-report.json s3: key: \"{{workflow.parameters.workload-name}}/{{workflow.parameters.tag}}/{{workflow.parameters.wl-service-name}}-megalinter-report-json\"","title":"Megalinter"},{"location":"developers_guide/ci/megalinter/#megalinter","text":"To lint all the source code Megalinter by OX Security is used. Linters configuration could be changed in supplied .mega-linter.yml using the configuration notes at https://megalinter.io/latest/config-file . To avoid trying miriades of supported linters it's good idead to disable some of them in .mega-linter.yml or use ' flavor' image as described . ENV variables and image could be changed in megalinter cluster workflow template JSON report as well as SARIF are saved as S3 output artifacts for further observation.","title":"Megalinter"},{"location":"developers_guide/ci/megalinter/#inputs","text":"{{workflow.parameters.repo}} {{workflow.parameters.tag}} {{workflow.parameters.dockerhub-registry-proxy}} {{workflow.parameters.workload-name}} {{workflow.parameters.wl-service-name}} {{workflow.parameters.wl-service-dir}}","title":"Inputs:"},{"location":"developers_guide/ci/megalinter/#outputs","text":"- name: megalinter-report-sarif path: /tmp/megalinter-report.sarif s3: key: \"{{workflow.parameters.workload-name}}/{{workflow.parameters.tag}}/{{workflow.parameters.wl-service-name}}-megalinter-report-sarif\" - name: megalinter-report-json path: /tmp/mega-linter-report.json s3: key: \"{{workflow.parameters.workload-name}}/{{workflow.parameters.tag}}/{{workflow.parameters.wl-service-name}}-megalinter-report-json\"","title":"Outputs:"},{"location":"developers_guide/ci/trivy/","text":"Trivy Trivy is used to scan workload fs for vulnerabilities. Trivy cluster workflowtemplate trivy-fs-s3-cwft takes the workload's repo checkout as an input hardwired artifact, scans the service directory and put report in appropriate location in S3 artifactory storage as an output artifacts in SARIF format for further observation. Inputs: {{workflow.parameters.repo}} {{workflow.parameters.tag}} {{workflow.parameters.dockerhub-registry-proxy}} {{workflow.parameters.workload-name}} {{workflow.parameters.wl-service-name}} {{workflow.parameters.wl-service-dir}} Outputs: - name: trivy-fs-report-sarif path: /trivy-fs-report.sarif s3: key: \"{{workflow.parameters.workload-name}}/{{workflow.parameters.tag}}/{{workflow.parameters.wl-service-name}}-trivy-fs-report-sarif\"","title":"Trivy"},{"location":"developers_guide/ci/trivy/#trivy","text":"Trivy is used to scan workload fs for vulnerabilities. Trivy cluster workflowtemplate trivy-fs-s3-cwft takes the workload's repo checkout as an input hardwired artifact, scans the service directory and put report in appropriate location in S3 artifactory storage as an output artifacts in SARIF format for further observation.","title":"Trivy"},{"location":"developers_guide/ci/trivy/#inputs","text":"{{workflow.parameters.repo}} {{workflow.parameters.tag}} {{workflow.parameters.dockerhub-registry-proxy}} {{workflow.parameters.workload-name}} {{workflow.parameters.wl-service-name}} {{workflow.parameters.wl-service-dir}}","title":"Inputs:"},{"location":"developers_guide/ci/trivy/#outputs","text":"- name: trivy-fs-report-sarif path: /trivy-fs-report.sarif s3: key: \"{{workflow.parameters.workload-name}}/{{workflow.parameters.tag}}/{{workflow.parameters.wl-service-name}}-trivy-fs-report-sarif\"","title":"Outputs:"},{"location":"developers_guide/observability/dashboards/","text":"List of Dashboards K8s A modern set of Grafana dashboards for Kubernetes . Dashboard for Prometheus Dashboard for the API Server Kubernetes component Show information on the CoreDNS Kubernetes component Global level view dashboard for Kubernetes Namespaces level view dashboard for Kubernetes Nodes level view dashboard for Kubernetes Pods level view dashboard for Kubernetes USE The Utilization Saturation and Errors (USE) Method dashboard that can be used to quickly identify resource bottlenecks or errors. Security Trivy Dashboard for the Aqua Security Trivy Operator Kyverno Cost KubeCost cluster metrics Dashboard integrated with KubeCost providing a view on findings and insights. This dashboard gives you Kubernetes cluster costs: Cluster Wide (Live and Estimative) Relative price of spot instances Namespace (Live and Estimative) Price variation between days and weeks APP (Live and average) Price comparison with 7 days ago PVC Costs Node.js Node.js metrics Based on this template This dashboard works with the metrics exported by the prom-client package for node.js. CG DevX Log management is pre-configured to automatically collect metrics exported by node.js application.","title":"List of Dashboards"},{"location":"developers_guide/observability/dashboards/#list-of-dashboards","text":"","title":"List of Dashboards"},{"location":"developers_guide/observability/dashboards/#k8s","text":"A modern set of Grafana dashboards for Kubernetes . Dashboard for Prometheus Dashboard for the API Server Kubernetes component Show information on the CoreDNS Kubernetes component Global level view dashboard for Kubernetes Namespaces level view dashboard for Kubernetes Nodes level view dashboard for Kubernetes Pods level view dashboard for Kubernetes","title":"K8s"},{"location":"developers_guide/observability/dashboards/#use","text":"The Utilization Saturation and Errors (USE) Method dashboard that can be used to quickly identify resource bottlenecks or errors.","title":"USE"},{"location":"developers_guide/observability/dashboards/#security","text":"","title":"Security"},{"location":"developers_guide/observability/dashboards/#trivy","text":"Dashboard for the Aqua Security Trivy Operator","title":"Trivy"},{"location":"developers_guide/observability/dashboards/#kyverno","text":"","title":"Kyverno"},{"location":"developers_guide/observability/dashboards/#cost","text":"","title":"Cost"},{"location":"developers_guide/observability/dashboards/#kubecost-cluster-metrics","text":"Dashboard integrated with KubeCost providing a view on findings and insights. This dashboard gives you Kubernetes cluster costs: Cluster Wide (Live and Estimative) Relative price of spot instances Namespace (Live and Estimative) Price variation between days and weeks APP (Live and average) Price comparison with 7 days ago PVC Costs","title":"KubeCost cluster metrics"},{"location":"developers_guide/observability/dashboards/#nodejs","text":"","title":"Node.js"},{"location":"developers_guide/observability/dashboards/#nodejs-metrics","text":"Based on this template This dashboard works with the metrics exported by the prom-client package for node.js. CG DevX Log management is pre-configured to automatically collect metrics exported by node.js application.","title":"Node.js metrics"},{"location":"developers_guide/observability/log_management/","text":"Default log management implementation is based on Grafana Loki . Loki is by design optimized to work with K8s pod logs. It allows you to seamlessly switch between metrics and logs using the same labels greatly improving user experience. Loki is integrated with Grafana used for monitoring, and Grafana is used as default user interface to query logs. Log data visualization is done using Grafana. You could browse data by navigating to Grafana => Explore, and pick Loki as Datasource.","title":"Log management"},{"location":"developers_guide/observability/monitoring/","text":"The CG DevX reference implementation provides monitoring and log management capabilities using Grafana, Prometheus, and Loki. Grafana serves as a visualization tool for both metrics and logs. To access Grafana, follow the link in the platform GitOps repository readme file ( README.md ), or provided by operators (AKA a platform team). Grafana is configured to use Vault as its OIDC provider. You need to press the Sign in with Vault button, which will redirect you to the Vault login page, which will look like this: Control (RBAC) is applied to limit access to workload dashboards. CG DevX has pre-installed K8s specific dashboards , plus language- / framework-specific dashboards that can be used as templates when creating a workload-specific dashboard.","title":"Monitoring"},{"location":"developers_guide/workloads/concept/","text":"Workload concept A Workload can be described as one or more applications, systems, or solutions fulfilling specific business goals. In some cases, those terms can be used interchangeably. An application / system / solution is composed of services, where a service is the smallest independently deployable and managed unit performing an isolated set of business functions. Prerequisites To be successfully integrated with CG DevX workload, all of an application's services should: Be single process, or managed by a single process Be stateless (we do not recommend putting workloads working with or storing business-critical data that require strong consistency), meaning No in memory state that cannot be recovered/restored/recalculated No local file system/storage that cannot be recovered/restored/recreated Support graceful shutdown Be packed in a container according to guidelines regarding: Base image Log streams Metrics exposure Liveness/readiness probes Config is stored in the environment What's included Workloads created by CG DevX come with complete CI/CD processes including automated builds, container build & publishing, linting templates, integration with different test frameworks, GitOps definition and deployments , and version and release management. CG DevX saves time, doing all the heavy-lifting by handling all the integrations, and all you need to do is to fine-tune templates to your specific needs.","title":"Workload concept"},{"location":"developers_guide/workloads/concept/#workload-concept","text":"A Workload can be described as one or more applications, systems, or solutions fulfilling specific business goals. In some cases, those terms can be used interchangeably. An application / system / solution is composed of services, where a service is the smallest independently deployable and managed unit performing an isolated set of business functions.","title":"Workload concept"},{"location":"developers_guide/workloads/concept/#prerequisites","text":"To be successfully integrated with CG DevX workload, all of an application's services should: Be single process, or managed by a single process Be stateless (we do not recommend putting workloads working with or storing business-critical data that require strong consistency), meaning No in memory state that cannot be recovered/restored/recalculated No local file system/storage that cannot be recovered/restored/recreated Support graceful shutdown Be packed in a container according to guidelines regarding: Base image Log streams Metrics exposure Liveness/readiness probes Config is stored in the environment","title":"Prerequisites"},{"location":"developers_guide/workloads/concept/#whats-included","text":"Workloads created by CG DevX come with complete CI/CD processes including automated builds, container build & publishing, linting templates, integration with different test frameworks, GitOps definition and deployments , and version and release management. CG DevX saves time, doing all the heavy-lifting by handling all the integrations, and all you need to do is to fine-tune templates to your specific needs.","title":"What's included"},{"location":"developers_guide/workloads/gitops_environments/","text":"How to Model Your Environments This is a reference implementation of a GitOps approach to environments management using Kustomize folders. The release promotion model relies on this structure. For more details, please see this great example Understanding Workload configuration types: These are the most common configuration types in the context of a Kubernetes application: Application version in the form of the container tag used. This is the most important setting from a promotion perspective. Often a change to the application version triggered by a source code change also requires a change to the runtime environment. Kubernetes-specific settings This includes the replica count, resource limits, health checks, persistent volumes, affinity rules, etc. (mostly) Static business settings . Application settings that are unrelated to Kubernetes, such as external URLs, authentication providers, connection strings, etc. These are \"mostly static\" \u2014 settings that are defined once for each environment and then never change. You don't want to promote these settings between environments. Non-static business settings . The same thing as static business settings, but you do want to promote them between environments. Folder structure The base folder holds configurations that are common to all environments. It is not expected to change often. If you want to make changes to multiple environments at the same time please use variants . The variants folder holds common characteristics between environments. It is up to you to define those commonalities. The envs folder holds individual environment configurations that are built from base and variants , and also includes environment-specific settings that are not common. As for business settings, both static and non-static can come from envs, ConfigMap, Secret, and ExternalSecret, and are Workload-specific; this reference implementation should be updated according to the needs of your application. . \u251c\u2500\u2500 base # <= common configuration \u2502 \u251c\u2500\u2500 deployment.yaml # <= kuernetes settings \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 service.yaml # <= kuernetes settings \u2502 \u2514\u2500\u2500 serviceaccount.yaml # <= kuernetes settings \u251c\u2500\u2500 envs # <= environment specific configuration \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u251c\u2500\u2500 cm.yaml # <= business settings \u2502 \u2502 \u251c\u2500\u2500 deployment.yaml # <= kuernetes settings \u2502 \u2502 \u251c\u2500\u2500 external-secrets.yaml # <= business settings \u2502 \u2502 \u251c\u2500\u2500 ingress.yaml # <= kuernetes settings \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 settings.yaml # <= business settings \u2502 \u2502 \u2514\u2500\u2500 version.yaml # <= application version \u2502 \u251c\u2500\u2500 prod \u2502 \u2502 \u251c\u2500\u2500 cm.yaml # <= business settings \u2502 \u2502 \u251c\u2500\u2500 deployment.yaml # <= kuernetes settings \u2502 \u2502 \u251c\u2500\u2500 external-secrets.yaml # <= business settings \u2502 \u2502 \u251c\u2500\u2500 ingress.yaml # <= kuernetes settings \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 settings.yaml # <= business settings \u2502 \u2502 \u2514\u2500\u2500 version.yaml # <= application version \u2502 \u2514\u2500\u2500 sta \u2502 \u251c\u2500\u2500 cm.yaml # <= business settings \u2502 \u251c\u2500\u2500 deployment.yaml # <= kuernetes settings \u2502 \u251c\u2500\u2500 external-secrets.yaml # <= business settings \u2502 \u251c\u2500\u2500 ingress.yaml # <= kuernetes settings \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 settings.yaml # <= business settings \u2502 \u2514\u2500\u2500 version.yaml # <= application version \u2514\u2500\u2500 variants # <= common configuration between environments \u251c\u2500\u2500 dev \u2502 \u251c\u2500\u2500 dev.yaml # <= kuernetes settings \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 replicas.yaml # <= kuernetes settings replicas count \u2502 \u2514\u2500\u2500 settings.yaml # <= static settings \u251c\u2500\u2500 prod \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 prod.yaml # <= kuernetes settings \u2502 \u251c\u2500\u2500 replicas.yaml # <= kuernetes settings replicas count \u2502 \u2514\u2500\u2500 settings.yaml # <= static settings \u2514\u2500\u2500 uat \u251c\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 replicas.yaml # <= kuernetes settings replicas count \u251c\u2500\u2500 settings.yaml # <= static settings \u2514\u2500\u2500 uat.yaml # <= other kuernetes settings You can generate and compare effective manifests for each environment using the commands below: # Current directory: GITROOT/gitops/environments # make temp dir for file comparison mkdir .tmp # generate effective manifest kustomize build envs/dev/> .tmp/dev.yaml kustomize build envs/sta/ > .tmp/sta.yaml kustomize build envs/prod/ > .tmp/prod.yaml # diff envs manifests vimdiff .tmp/dev.yaml .tmp/sta.yaml .tmp/prod.yaml # drop temp dir rm -rf .tmp/","title":"How to Model Your Environments"},{"location":"developers_guide/workloads/gitops_environments/#how-to-model-your-environments","text":"This is a reference implementation of a GitOps approach to environments management using Kustomize folders. The release promotion model relies on this structure. For more details, please see this great example","title":"How to Model Your Environments"},{"location":"developers_guide/workloads/gitops_environments/#understanding-workload-configuration-types","text":"These are the most common configuration types in the context of a Kubernetes application: Application version in the form of the container tag used. This is the most important setting from a promotion perspective. Often a change to the application version triggered by a source code change also requires a change to the runtime environment. Kubernetes-specific settings This includes the replica count, resource limits, health checks, persistent volumes, affinity rules, etc. (mostly) Static business settings . Application settings that are unrelated to Kubernetes, such as external URLs, authentication providers, connection strings, etc. These are \"mostly static\" \u2014 settings that are defined once for each environment and then never change. You don't want to promote these settings between environments. Non-static business settings . The same thing as static business settings, but you do want to promote them between environments.","title":"Understanding Workload configuration types:"},{"location":"developers_guide/workloads/gitops_environments/#folder-structure","text":"The base folder holds configurations that are common to all environments. It is not expected to change often. If you want to make changes to multiple environments at the same time please use variants . The variants folder holds common characteristics between environments. It is up to you to define those commonalities. The envs folder holds individual environment configurations that are built from base and variants , and also includes environment-specific settings that are not common. As for business settings, both static and non-static can come from envs, ConfigMap, Secret, and ExternalSecret, and are Workload-specific; this reference implementation should be updated according to the needs of your application. . \u251c\u2500\u2500 base # <= common configuration \u2502 \u251c\u2500\u2500 deployment.yaml # <= kuernetes settings \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 service.yaml # <= kuernetes settings \u2502 \u2514\u2500\u2500 serviceaccount.yaml # <= kuernetes settings \u251c\u2500\u2500 envs # <= environment specific configuration \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u251c\u2500\u2500 cm.yaml # <= business settings \u2502 \u2502 \u251c\u2500\u2500 deployment.yaml # <= kuernetes settings \u2502 \u2502 \u251c\u2500\u2500 external-secrets.yaml # <= business settings \u2502 \u2502 \u251c\u2500\u2500 ingress.yaml # <= kuernetes settings \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 settings.yaml # <= business settings \u2502 \u2502 \u2514\u2500\u2500 version.yaml # <= application version \u2502 \u251c\u2500\u2500 prod \u2502 \u2502 \u251c\u2500\u2500 cm.yaml # <= business settings \u2502 \u2502 \u251c\u2500\u2500 deployment.yaml # <= kuernetes settings \u2502 \u2502 \u251c\u2500\u2500 external-secrets.yaml # <= business settings \u2502 \u2502 \u251c\u2500\u2500 ingress.yaml # <= kuernetes settings \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 settings.yaml # <= business settings \u2502 \u2502 \u2514\u2500\u2500 version.yaml # <= application version \u2502 \u2514\u2500\u2500 sta \u2502 \u251c\u2500\u2500 cm.yaml # <= business settings \u2502 \u251c\u2500\u2500 deployment.yaml # <= kuernetes settings \u2502 \u251c\u2500\u2500 external-secrets.yaml # <= business settings \u2502 \u251c\u2500\u2500 ingress.yaml # <= kuernetes settings \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 settings.yaml # <= business settings \u2502 \u2514\u2500\u2500 version.yaml # <= application version \u2514\u2500\u2500 variants # <= common configuration between environments \u251c\u2500\u2500 dev \u2502 \u251c\u2500\u2500 dev.yaml # <= kuernetes settings \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 replicas.yaml # <= kuernetes settings replicas count \u2502 \u2514\u2500\u2500 settings.yaml # <= static settings \u251c\u2500\u2500 prod \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 prod.yaml # <= kuernetes settings \u2502 \u251c\u2500\u2500 replicas.yaml # <= kuernetes settings replicas count \u2502 \u2514\u2500\u2500 settings.yaml # <= static settings \u2514\u2500\u2500 uat \u251c\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 replicas.yaml # <= kuernetes settings replicas count \u251c\u2500\u2500 settings.yaml # <= static settings \u2514\u2500\u2500 uat.yaml # <= other kuernetes settings You can generate and compare effective manifests for each environment using the commands below: # Current directory: GITROOT/gitops/environments # make temp dir for file comparison mkdir .tmp # generate effective manifest kustomize build envs/dev/> .tmp/dev.yaml kustomize build envs/sta/ > .tmp/sta.yaml kustomize build envs/prod/ > .tmp/prod.yaml # diff envs manifests vimdiff .tmp/dev.yaml .tmp/sta.yaml .tmp/prod.yaml # drop temp dir rm -rf .tmp/","title":"Folder structure"},{"location":"operators_guide/concept/","text":"Main Benefits to Operators Better visibility and faster validation of changes : Identify and fix security issues before they reach production. Health checks and rollback systems allow quick resolution of deployment failures. Higher quality by : Standardizing cloud building blocks. Automating configuration and infrastructure to lower the potential for environment-specific problems. Logging, monitoring, and alerting available out of the box increase the operability of services. Full automation of the provisioning process enables business continuity. Built-in guardrails and compliance checks : Policies defined in code and validated against infrastructure and applications both prior to deployment and continuously at runtime. Enables developer teams to be accountable for the security of their services. GitOps CG DevX uses a GitOps approach for platform management. The Platform GitOps repository created by the setup process has two main parts: /gitops_pipelines : Clusters and core services GitOps configurations. /terraform : Infrastructure as Code & Configuration as Code for all the cloud services, Git provider, secrets, user management, and core platform services. We find such a combination of IaC and K8s manifests in the same repository feasible. For those who may have strict requirements on access control, it could be divided into two separate repositories. Runtime CG DevX services can be virtually divided into two groups: core services responsible for platform functionality and orchestration (control plane), and runtime where users' workloads will be executed. Runtime contains only the services responsible for workload delivery and configuration management. CG DevX supports different runtime deployment options based on workload needs. The most common options are: Using one cluster : Same cluster for control plane and runtime. This cluster should be considered the Control Center Cluster (CC Cluster). Default option provided with reference implementation. Multiple K8S clusters : CC Cluster (management cluster) plus runtime cluster(s) within one region. Multi-region installation : Usually with one control plane cluster per region. Multi-cloud installation : Can combine all the options above. Additional K8s cluster runtimes are usually used when a workload requires \"hard isolation,\" e.g., dedicated production environment, dedicated tenant installation, etc. In this deployment option, runtime has its own services responsible for workload delivery and configuration management. Depending on workload isolation requirements, other services like log management could be instantiated. Workload Isolation CG DevX uses a combination of Role-Based Access Control (RBAC) and Attribute-Based Access Control (ABAC) to ensure proper access control within the cluster and outside the cluster when connecting to non-cluster resources. Core components are provisioned within a secure boundary with management access limited to the operators (platform team), while using fine-grained access to the underlying infrastructure. We strongly recommend operators apply changes to the platform or underlying cloud infrastructure using the GitOps approach, and not to use users with \"write\" access to the infrastructure. While we provide drift detection mechanisms, the potential impact of manual infrastructure and configuration changes could not be fully mitigated. The default workload management strategy is \"soft multi-tenancy,\" where a cluster is shared by multiple teams and workloads, meaning that you trust your teams and their workloads. Each workload is provisioned within a virtual environment forming a secure perimeter isolated from other workloads. Each workload has its own set of secrets accessible only by the workload and teams responsible for it. Developer teams do not have direct modify access to underlying infrastructure or live environment configuration settings and should apply changes using workload GitOps. BC/DR CG DevX is designed following High Availability and Cloud Elasticity principles and supports single region multiple availability zones. The default Disaster Recovery (DR) strategy is Backup & Restore, with all artifacts offloaded out of the cluster to 1+ regions. RTO and RPO depend on stateful resources used by the workload. The CG DevX Platform can be provisioned in under 1 hour, while the app runtime module itself most typically could accept and serve workloads in ~15 minutes. Cold (pilot light) and Warm standby can be achieved by provisioning the CG DevX module set in another region and changing default autoscaling policies. Most CG DevX components support active-active scenarios. The feasibility of those scenarios depends on workload requirements for stateful resources, which are considered to be non-platform resources, as CG DevX is designed to primarily serve stateless workloads.","title":"Concept"},{"location":"operators_guide/concept/#main-benefits-to-operators","text":"Better visibility and faster validation of changes : Identify and fix security issues before they reach production. Health checks and rollback systems allow quick resolution of deployment failures. Higher quality by : Standardizing cloud building blocks. Automating configuration and infrastructure to lower the potential for environment-specific problems. Logging, monitoring, and alerting available out of the box increase the operability of services. Full automation of the provisioning process enables business continuity. Built-in guardrails and compliance checks : Policies defined in code and validated against infrastructure and applications both prior to deployment and continuously at runtime. Enables developer teams to be accountable for the security of their services.","title":"Main Benefits to Operators"},{"location":"operators_guide/concept/#gitops","text":"CG DevX uses a GitOps approach for platform management. The Platform GitOps repository created by the setup process has two main parts: /gitops_pipelines : Clusters and core services GitOps configurations. /terraform : Infrastructure as Code & Configuration as Code for all the cloud services, Git provider, secrets, user management, and core platform services. We find such a combination of IaC and K8s manifests in the same repository feasible. For those who may have strict requirements on access control, it could be divided into two separate repositories.","title":"GitOps"},{"location":"operators_guide/concept/#runtime","text":"CG DevX services can be virtually divided into two groups: core services responsible for platform functionality and orchestration (control plane), and runtime where users' workloads will be executed. Runtime contains only the services responsible for workload delivery and configuration management. CG DevX supports different runtime deployment options based on workload needs. The most common options are: Using one cluster : Same cluster for control plane and runtime. This cluster should be considered the Control Center Cluster (CC Cluster). Default option provided with reference implementation. Multiple K8S clusters : CC Cluster (management cluster) plus runtime cluster(s) within one region. Multi-region installation : Usually with one control plane cluster per region. Multi-cloud installation : Can combine all the options above. Additional K8s cluster runtimes are usually used when a workload requires \"hard isolation,\" e.g., dedicated production environment, dedicated tenant installation, etc. In this deployment option, runtime has its own services responsible for workload delivery and configuration management. Depending on workload isolation requirements, other services like log management could be instantiated.","title":"Runtime"},{"location":"operators_guide/concept/#workload-isolation","text":"CG DevX uses a combination of Role-Based Access Control (RBAC) and Attribute-Based Access Control (ABAC) to ensure proper access control within the cluster and outside the cluster when connecting to non-cluster resources. Core components are provisioned within a secure boundary with management access limited to the operators (platform team), while using fine-grained access to the underlying infrastructure. We strongly recommend operators apply changes to the platform or underlying cloud infrastructure using the GitOps approach, and not to use users with \"write\" access to the infrastructure. While we provide drift detection mechanisms, the potential impact of manual infrastructure and configuration changes could not be fully mitigated. The default workload management strategy is \"soft multi-tenancy,\" where a cluster is shared by multiple teams and workloads, meaning that you trust your teams and their workloads. Each workload is provisioned within a virtual environment forming a secure perimeter isolated from other workloads. Each workload has its own set of secrets accessible only by the workload and teams responsible for it. Developer teams do not have direct modify access to underlying infrastructure or live environment configuration settings and should apply changes using workload GitOps.","title":"Workload Isolation"},{"location":"operators_guide/concept/#bcdr","text":"CG DevX is designed following High Availability and Cloud Elasticity principles and supports single region multiple availability zones. The default Disaster Recovery (DR) strategy is Backup & Restore, with all artifacts offloaded out of the cluster to 1+ regions. RTO and RPO depend on stateful resources used by the workload. The CG DevX Platform can be provisioned in under 1 hour, while the app runtime module itself most typically could accept and serve workloads in ~15 minutes. Cold (pilot light) and Warm standby can be achieved by provisioning the CG DevX module set in another region and changing default autoscaling policies. Most CG DevX components support active-active scenarios. The feasibility of those scenarios depends on workload requirements for stateful resources, which are considered to be non-platform resources, as CG DevX is designed to primarily serve stateless workloads.","title":"BC/DR"},{"location":"operators_guide/supported_platforms/","text":"Platforms and providers Currently, the following platforms and providers are supported, or being developed, or are part of the roadmap. CLI OS Support Linux - Supported Mac (ARM) - Supported Mac (Intel) - Supported Windows - N/A Git Providers GitHub - Supported GitLab - Will be added with future updates Bitbucket - Will be added with future updates Cloud Providers AWS - Supported Azure - Supported GCP - Will be added with future updates DNS registrars AWS Route53 - Supported Azure DNS - Supported Google DNS - Will be added with future updates","title":"Platforms and providers"},{"location":"operators_guide/supported_platforms/#platforms-and-providers","text":"Currently, the following platforms and providers are supported, or being developed, or are part of the roadmap.","title":"Platforms and providers"},{"location":"operators_guide/supported_platforms/#cli-os-support","text":"Linux - Supported Mac (ARM) - Supported Mac (Intel) - Supported Windows - N/A","title":"CLI OS Support"},{"location":"operators_guide/supported_platforms/#git-providers","text":"GitHub - Supported GitLab - Will be added with future updates Bitbucket - Will be added with future updates","title":"Git Providers"},{"location":"operators_guide/supported_platforms/#cloud-providers","text":"AWS - Supported Azure - Supported GCP - Will be added with future updates","title":"Cloud Providers"},{"location":"operators_guide/supported_platforms/#dns-registrars","text":"AWS Route53 - Supported Azure DNS - Supported Google DNS - Will be added with future updates","title":"DNS registrars"},{"location":"operators_guide/delivery_pipelines/core_services_delivery/","text":"Delivery Pipelines The platform repository provides two flows of platform delivery: one for IaC changes, and another for K8s manifests of core services. IaC Pipeline Often, when using Terraform or other IaC tools, changes to the infrastructure are applied directly from the local machine using the CLI (which we find to be a terrible practice), or via Infrastructure as Code (IaC), which is better but still lacks collaboration. CG DevX changes the IaC practice to be more GitOps-oriented by managing (planning and applying) Terraform changes using an automation tool called Atlantis. These changes allow implementing most GitOps principles as: Terraform is declarative. By storing Terraform files in Git, we make them versioned and immutable. Changes will be tracked and applied automatically. Note: It's not possible to be 100% true to GitOps principles while using Terraform or any other tool that has a state not stored in Git. By using the GitOps approach and Atlantis, we provide better version history, enhance collaboration, and give better visibility of changes. By keeping a complete history of the changes, we also make it easier to roll back. Atlantis also provides a locking mechanism, which prevents possible conflicts when working in a shared environment. Automatic Plans With Atlantis Any change request that includes a .tf file will trigger Atlantis to run your Terraform plan. Atlantis will post the plan's result to your PR as a comment. At that point, you can review and approve the PR. The default requirement for a PR to be processed by Atlantis is to have one approval from anyone who is not the owner of the PR. You can always run a new Terraform plan for your PR by adding a comment that says atlantis plan . Apply and Merge To merge the PR, add the comment atlantis apply . This will prompt Atlantis to run terraform apply . The results of the apply operation will be added to the PR as comments. If the apply is successful, your change will be automatically merged into main , the PR will be closed, the branch deleted, and the state lock will be removed in Atlantis. Core Services CG DevX core services use GitOps and are delivered by the continuous delivery tool ArgoCD. It manages all core services and workloads across all K8s clusters. ArgoCD is a straightforward mechanism for managing: Helm charts, versions, configuration overrides, etc. Plain K8s manifests K8s manifests with Kustomize The configuration for all the services installed on your K8s cluster can be found in the platform GitOps repository under the path /gitops-pipelines/delivery/clusters/cc-cluster/core-services . Files are organized in a logical sequence and contain definitions for a service source, destination, and configuration overrides. Core services are built using an \"App of Apps\" pattern to form one registry. As the desired state is described declaratively in the GitOps repository, once you change it, it will be detected by ArgoCD, which will reconcile the state. In other words, any apps in Kubernetes that need to be added or adjusted will automatically sync with the state defined in Git. For more details, please check the ArgoCD documentation .","title":"Delivery Pipelines"},{"location":"operators_guide/delivery_pipelines/core_services_delivery/#delivery-pipelines","text":"The platform repository provides two flows of platform delivery: one for IaC changes, and another for K8s manifests of core services.","title":"Delivery Pipelines"},{"location":"operators_guide/delivery_pipelines/core_services_delivery/#iac-pipeline","text":"Often, when using Terraform or other IaC tools, changes to the infrastructure are applied directly from the local machine using the CLI (which we find to be a terrible practice), or via Infrastructure as Code (IaC), which is better but still lacks collaboration. CG DevX changes the IaC practice to be more GitOps-oriented by managing (planning and applying) Terraform changes using an automation tool called Atlantis. These changes allow implementing most GitOps principles as: Terraform is declarative. By storing Terraform files in Git, we make them versioned and immutable. Changes will be tracked and applied automatically. Note: It's not possible to be 100% true to GitOps principles while using Terraform or any other tool that has a state not stored in Git. By using the GitOps approach and Atlantis, we provide better version history, enhance collaboration, and give better visibility of changes. By keeping a complete history of the changes, we also make it easier to roll back. Atlantis also provides a locking mechanism, which prevents possible conflicts when working in a shared environment.","title":"IaC Pipeline"},{"location":"operators_guide/delivery_pipelines/core_services_delivery/#automatic-plans-with-atlantis","text":"Any change request that includes a .tf file will trigger Atlantis to run your Terraform plan. Atlantis will post the plan's result to your PR as a comment. At that point, you can review and approve the PR. The default requirement for a PR to be processed by Atlantis is to have one approval from anyone who is not the owner of the PR. You can always run a new Terraform plan for your PR by adding a comment that says atlantis plan .","title":"Automatic Plans With Atlantis"},{"location":"operators_guide/delivery_pipelines/core_services_delivery/#apply-and-merge","text":"To merge the PR, add the comment atlantis apply . This will prompt Atlantis to run terraform apply . The results of the apply operation will be added to the PR as comments. If the apply is successful, your change will be automatically merged into main , the PR will be closed, the branch deleted, and the state lock will be removed in Atlantis.","title":"Apply and Merge"},{"location":"operators_guide/delivery_pipelines/core_services_delivery/#core-services","text":"CG DevX core services use GitOps and are delivered by the continuous delivery tool ArgoCD. It manages all core services and workloads across all K8s clusters. ArgoCD is a straightforward mechanism for managing: Helm charts, versions, configuration overrides, etc. Plain K8s manifests K8s manifests with Kustomize The configuration for all the services installed on your K8s cluster can be found in the platform GitOps repository under the path /gitops-pipelines/delivery/clusters/cc-cluster/core-services . Files are organized in a logical sequence and contain definitions for a service source, destination, and configuration overrides. Core services are built using an \"App of Apps\" pattern to form one registry. As the desired state is described declaratively in the GitOps repository, once you change it, it will be detected by ArgoCD, which will reconcile the state. In other words, any apps in Kubernetes that need to be added or adjusted will automatically sync with the state defined in Git. For more details, please check the ArgoCD documentation .","title":"Core Services"},{"location":"operators_guide/installation/building_cli/","text":"Building CG DevX CLI This will allow you to build CG DevX CLI from sources Pre-requisites You should have: python 3.10 + pip poetry 1.6.* If you don't have poetry installed, please follow official installation instructions here . Preparing the environment # Assumed directory: GITROOT/tools # NOTE: Poetry configuration and lock files are stored in the 'cli' directory. # To install dependencies, use: poetry install # Activate the virtual environment with: # By default, Poetry creates a virtual environment in {cache-dir}/virtualenvs poetry shell To find more on poetry commands, please see . Building the CLI tool To build the CLI tool, please run PyInstaller directly: # Current directory: GITROOT/tools python -m PyInstaller --onefile cli/__main__.py --name cgdevxcli or via the python script: # Current directory: GITROOT/tools poetry run build After that you can use and distribute the cgdexvcli binary located at GITROOT/dist/cgdevxcli .","title":"Building CG DevX CLI"},{"location":"operators_guide/installation/building_cli/#building-cg-devx-cli","text":"This will allow you to build CG DevX CLI from sources","title":"Building CG DevX CLI"},{"location":"operators_guide/installation/building_cli/#pre-requisites","text":"You should have: python 3.10 + pip poetry 1.6.* If you don't have poetry installed, please follow official installation instructions here .","title":"Pre-requisites"},{"location":"operators_guide/installation/building_cli/#preparing-the-environment","text":"# Assumed directory: GITROOT/tools # NOTE: Poetry configuration and lock files are stored in the 'cli' directory. # To install dependencies, use: poetry install # Activate the virtual environment with: # By default, Poetry creates a virtual environment in {cache-dir}/virtualenvs poetry shell To find more on poetry commands, please see .","title":"Preparing the environment"},{"location":"operators_guide/installation/building_cli/#building-the-cli-tool","text":"To build the CLI tool, please run PyInstaller directly: # Current directory: GITROOT/tools python -m PyInstaller --onefile cli/__main__.py --name cgdevxcli or via the python script: # Current directory: GITROOT/tools poetry run build After that you can use and distribute the cgdexvcli binary located at GITROOT/dist/cgdevxcli .","title":"Building the CLI tool"},{"location":"operators_guide/installation/cli_commands/","text":"CG DevX CLI commands CG DevX CLI provides a \"one-click\" experience for platform installation and workload management. The usage pattern is [OPTIONS] COMMAND [ARGS] CG DevX CLI support following: Options: --help Show help message Commands: setup Creates a new CG DevX installation destroy Destroys an existing CG DevX installation workload Commands related to Workload Management create Generates a configuration of key Workload resources bootstrap Bootstraps a Workload with configuration templates delete Removes a configuration of key Workload resources Arguments: Arguments are command-specific and can be supplied via command lime, environment variables, or file. Setup Creates and configures the reference implementation of CG DevX K8s Internal Developer Platform (IDP) kit. The CLI tool saves the intermediate state to a local file when running through multiple steps of the setup proces (AKA checkpoints) and can be re-run. setup command checks: Cloud CLI tools presence Cloud account permissions using profile or access keys you provided DNS provider permission using token or access keys you provided Domain ownership setup command creates: SSH key pairs Remote backend storage (e.g., AWS S3) used for IaC GitOps repository created under Git provider of your choice K8s cluster and supporting cloud resources provisioned by CG DevX CLI The setup command can be executed using arguments, environment variables, or input file. Arguments : Name (short, full) Type Description -e, --email TEXT Email address used for alerts -c, --cloud-provider [aws] Cloud provider type -cp, --cloud-profile TEXT Cloud account profile -cc, --cloud-account-key TEXT Cloud account access key -cs, --cloud-account-secret TEXT Cloud account access secret -r, --cloud-region TEXT Cloud regions -n, --cluster-name TEXT Cluster name -d, --dns-registrar [route53] DNS registrar -dt, --dns-registrar-token TEXT DNS registrar token -dk, --dns-registrar-key TEXT DNS registrar key -ds, --dns-registrar-secret TEXT DNS registrar secret -dn, --domain-name TEXT Domain-name used by K8s cluster -g, --git-provider [github] Git provider -go, --git-org TEXT Git organization name -gt, --git-access-token TEXT Git access token -grn, --gitops-repo-name TEXT GitOps repository name -gtu, --gitops-template-url TEXT GitOps repository template url -gtb, --gitops-template-branch TEXT GitOps repository template branch -dw, --setup-demo-workload Flag Setup demo workload -f, --config-file FILENAME Load parameters from file --verbosity [DEBUG, INFO, WARNING, ERROR, CRITICAL] Set the logging verbosity level, default CRITICAL Note! : For all names use kebab-case. parameters.yaml file example: email: user@cgdevx.io cloud-provider: aws cloud-profile: profile-name cloud-region: eu-west-1 cluster-name: cluster-name dns-registrar: route53 domain-name: demo.cgdevx.io git-provider: github git-org: CGDevX git-access-token: ghp_xxx gitops-repo-name: cgdevx-gitops Command snippet Using command arguments: cgdevxcli setup --email user@cgdevx.io \\ --cloud-provider aws --cloud-profile your-profile-name \\ --cluster-name cluster-name \\ --dns-registrar route53 \\ --domain-name example.com \\ --git-provider github \\ --git-org acmeinc \\ --git-access-token ghp_xxx \\ --gitops-repo-name gitops-repo-name Using a parameter file: cgdevxcli setup -f path/to/your/parameters.yaml Troubleshooting Installation of a reference architecture is a complex process depending on multiple factors, e.g., cloud resource availability, connection speed, image registry rate limits, etc. While we do our best to handle the most common problems and provide uninterrupted experience, the setup process can still fail. If you have connectivity or resource availability errors, please try restarting the setup. It should resume from the step where it first failed. Destroy Destroys all the resources created by setup process (AKA reverse setup). It uses local state data created by setup process. destroy command deletes: The K8s cluster and supporting cloud resources provisioned by the CG DevX CLI The GitOps repository created under the Git provider of your choice Remote backend storage (e.g., AWS S3) used for IaC All local files created by CG DevX CLI NOTE! : this process is irreversible NOTE! : This operation will delete all workload repositories if you have them. If workloads have any out of the cluster (cloud provider) resources, they will become orphaned, and should be deleted manually. It is highly recommended that prior to destroying your installation, you delete all active workloads first, also deleting all their resources. Please see more on workload delete command with --destroy-resources flag here . Arguments : Name (short, full) Type Description --verbosity [DEBUG, INFO, WARNING, ERROR, CRITICAL] Set the logging verbosity level, default CRITICAL Command snippet cgdevxcli destroy Troubleshooting Some of the resources used by the reference architecture are created dynamically in a run time. When doing cleanup, we are trying to destroy those temporary resources, and then all other resources created by our automation. The cleanup process could still fail. If you have any issues, please try restarting the process. If it fails to delete your K8s cluster, please try deleting Load Balancer(s) manually and restart the process. For GitHub, external action runners should be removed prior to repository deletion. If it fails to delete your GitOps repo - please check and remove runners and restart the process.","title":"CG DevX CLI commands"},{"location":"operators_guide/installation/cli_commands/#cg-devx-cli-commands","text":"CG DevX CLI provides a \"one-click\" experience for platform installation and workload management. The usage pattern is [OPTIONS] COMMAND [ARGS] CG DevX CLI support following: Options: --help Show help message Commands: setup Creates a new CG DevX installation destroy Destroys an existing CG DevX installation workload Commands related to Workload Management create Generates a configuration of key Workload resources bootstrap Bootstraps a Workload with configuration templates delete Removes a configuration of key Workload resources Arguments: Arguments are command-specific and can be supplied via command lime, environment variables, or file.","title":"CG DevX CLI commands"},{"location":"operators_guide/installation/cli_commands/#setup","text":"Creates and configures the reference implementation of CG DevX K8s Internal Developer Platform (IDP) kit. The CLI tool saves the intermediate state to a local file when running through multiple steps of the setup proces (AKA checkpoints) and can be re-run. setup command checks: Cloud CLI tools presence Cloud account permissions using profile or access keys you provided DNS provider permission using token or access keys you provided Domain ownership setup command creates: SSH key pairs Remote backend storage (e.g., AWS S3) used for IaC GitOps repository created under Git provider of your choice K8s cluster and supporting cloud resources provisioned by CG DevX CLI The setup command can be executed using arguments, environment variables, or input file. Arguments : Name (short, full) Type Description -e, --email TEXT Email address used for alerts -c, --cloud-provider [aws] Cloud provider type -cp, --cloud-profile TEXT Cloud account profile -cc, --cloud-account-key TEXT Cloud account access key -cs, --cloud-account-secret TEXT Cloud account access secret -r, --cloud-region TEXT Cloud regions -n, --cluster-name TEXT Cluster name -d, --dns-registrar [route53] DNS registrar -dt, --dns-registrar-token TEXT DNS registrar token -dk, --dns-registrar-key TEXT DNS registrar key -ds, --dns-registrar-secret TEXT DNS registrar secret -dn, --domain-name TEXT Domain-name used by K8s cluster -g, --git-provider [github] Git provider -go, --git-org TEXT Git organization name -gt, --git-access-token TEXT Git access token -grn, --gitops-repo-name TEXT GitOps repository name -gtu, --gitops-template-url TEXT GitOps repository template url -gtb, --gitops-template-branch TEXT GitOps repository template branch -dw, --setup-demo-workload Flag Setup demo workload -f, --config-file FILENAME Load parameters from file --verbosity [DEBUG, INFO, WARNING, ERROR, CRITICAL] Set the logging verbosity level, default CRITICAL Note! : For all names use kebab-case. parameters.yaml file example: email: user@cgdevx.io cloud-provider: aws cloud-profile: profile-name cloud-region: eu-west-1 cluster-name: cluster-name dns-registrar: route53 domain-name: demo.cgdevx.io git-provider: github git-org: CGDevX git-access-token: ghp_xxx gitops-repo-name: cgdevx-gitops Command snippet Using command arguments: cgdevxcli setup --email user@cgdevx.io \\ --cloud-provider aws --cloud-profile your-profile-name \\ --cluster-name cluster-name \\ --dns-registrar route53 \\ --domain-name example.com \\ --git-provider github \\ --git-org acmeinc \\ --git-access-token ghp_xxx \\ --gitops-repo-name gitops-repo-name Using a parameter file: cgdevxcli setup -f path/to/your/parameters.yaml","title":"Setup"},{"location":"operators_guide/installation/cli_commands/#troubleshooting","text":"Installation of a reference architecture is a complex process depending on multiple factors, e.g., cloud resource availability, connection speed, image registry rate limits, etc. While we do our best to handle the most common problems and provide uninterrupted experience, the setup process can still fail. If you have connectivity or resource availability errors, please try restarting the setup. It should resume from the step where it first failed.","title":"Troubleshooting"},{"location":"operators_guide/installation/cli_commands/#destroy","text":"Destroys all the resources created by setup process (AKA reverse setup). It uses local state data created by setup process. destroy command deletes: The K8s cluster and supporting cloud resources provisioned by the CG DevX CLI The GitOps repository created under the Git provider of your choice Remote backend storage (e.g., AWS S3) used for IaC All local files created by CG DevX CLI NOTE! : this process is irreversible NOTE! : This operation will delete all workload repositories if you have them. If workloads have any out of the cluster (cloud provider) resources, they will become orphaned, and should be deleted manually. It is highly recommended that prior to destroying your installation, you delete all active workloads first, also deleting all their resources. Please see more on workload delete command with --destroy-resources flag here . Arguments : Name (short, full) Type Description --verbosity [DEBUG, INFO, WARNING, ERROR, CRITICAL] Set the logging verbosity level, default CRITICAL Command snippet cgdevxcli destroy","title":"Destroy"},{"location":"operators_guide/installation/cli_commands/#troubleshooting_1","text":"Some of the resources used by the reference architecture are created dynamically in a run time. When doing cleanup, we are trying to destroy those temporary resources, and then all other resources created by our automation. The cleanup process could still fail. If you have any issues, please try restarting the process. If it fails to delete your K8s cluster, please try deleting Load Balancer(s) manually and restart the process. For GitHub, external action runners should be removed prior to repository deletion. If it fails to delete your GitOps repo - please check and remove runners and restart the process.","title":"Troubleshooting"},{"location":"operators_guide/installation/installing_cli/","text":"Installing CG DevX CLI You can download the CG DevX CLI pre-built binaries from the following sources: PyPI Use pip to download it from PyPI : pip install cgdevxcli GitHub Or download pre-packaged binary from GitHub Releases","title":"Installing CG DevX CLI"},{"location":"operators_guide/installation/installing_cli/#installing-cg-devx-cli","text":"You can download the CG DevX CLI pre-built binaries from the following sources:","title":"Installing CG DevX CLI"},{"location":"operators_guide/installation/installing_cli/#pypi","text":"Use pip to download it from PyPI : pip install cgdevxcli","title":"PyPI"},{"location":"operators_guide/installation/installing_cli/#github","text":"Or download pre-packaged binary from GitHub Releases","title":"GitHub"},{"location":"operators_guide/installation/quickstart/","text":"Installation process Installation should be done from a dedicated machine (local machine or host) that will be running the CG DevX CLI ( cgdevxcli ) interactive tool. With this installation approach, infrastructure is handled in terraform, and core services are handled in a GitOps way, with ArgoCD. When the initial installation of ArgoCD is done by the CLI, ArgoCD will self-manage its own configuration using GitOps. Configuration management of some core services is done using terraform's capabilities. While some users may prefer using Ansible or other tools, we find terraform well-suited for this purpose. The main reason behind this approach is to enable use of the same configuration with different cloud/hosting and git providers, while retaining the ability to further customize installation without affecting reference implementation. flowchart LR A(CLI on local machine) -->|1 Invokes| B(Terraform) B -->|3 Creates| D(Cloud Resources) B -->|2 Creates| C(Git repository) C -->|5 GitOps reconciliation| E A -->|4 Installs| E(ArgoCD) E -->|6 Manages| F(Core Components) Configure Cloud provider Prepare your cloud account ( AWS , Azure , GCP ). Configure Git Provider Prepare your Git provider ( GitHub ). Get CG DevX CLI The CG DevX CLI simplifies initial setup of the CG DevX reference architecture and further management of the platform. The setup process is intended to be executed from an operator's machine and will create a local folder ( ~/.cgdevx ) containing tools, a local version of GitOps repository, and configuration files. All subsequent commands should be executed from the same machine, as they will rely on a data created by setup process. You can allow other users to manage CG DevX installation by transferring the state file ( state.yaml ) and certificates ( cgdevx_ed* , cgdevx_k8s* ) from the local folder to another machine. Please note that this user should have the same access level to K8s cluster as one who provisioned the system. Additional permissions and configuration changes may be required depending on authentication options used for the cloud provider. Installing the CG DevX reference architecture Once you install the CLI, you should be able to provision a cluster using the setup command. The CG DevX CLI tool will provide detailed information on setup process progress, and provide you with platform user credentials. Please store them safely. You could get credentials by re-running the setup command on the same machine used for installation. Your kubeconfig file will be located in CG DevX local folder ( ~/.cgdevx/kubeconfig ) The platform GitOps repository readme file ( README.md ) will contain links to all the core services. Managing Workloads Once you have a running cluster, you can create your first workload using this guide .","title":"Installation process"},{"location":"operators_guide/installation/quickstart/#installation-process","text":"Installation should be done from a dedicated machine (local machine or host) that will be running the CG DevX CLI ( cgdevxcli ) interactive tool. With this installation approach, infrastructure is handled in terraform, and core services are handled in a GitOps way, with ArgoCD. When the initial installation of ArgoCD is done by the CLI, ArgoCD will self-manage its own configuration using GitOps. Configuration management of some core services is done using terraform's capabilities. While some users may prefer using Ansible or other tools, we find terraform well-suited for this purpose. The main reason behind this approach is to enable use of the same configuration with different cloud/hosting and git providers, while retaining the ability to further customize installation without affecting reference implementation. flowchart LR A(CLI on local machine) -->|1 Invokes| B(Terraform) B -->|3 Creates| D(Cloud Resources) B -->|2 Creates| C(Git repository) C -->|5 GitOps reconciliation| E A -->|4 Installs| E(ArgoCD) E -->|6 Manages| F(Core Components)","title":"Installation process"},{"location":"operators_guide/installation/quickstart/#configure-cloud-provider","text":"Prepare your cloud account ( AWS , Azure , GCP ).","title":"Configure Cloud provider"},{"location":"operators_guide/installation/quickstart/#configure-git-provider","text":"Prepare your Git provider ( GitHub ).","title":"Configure Git Provider"},{"location":"operators_guide/installation/quickstart/#get-cg-devx-cli","text":"The CG DevX CLI simplifies initial setup of the CG DevX reference architecture and further management of the platform. The setup process is intended to be executed from an operator's machine and will create a local folder ( ~/.cgdevx ) containing tools, a local version of GitOps repository, and configuration files. All subsequent commands should be executed from the same machine, as they will rely on a data created by setup process. You can allow other users to manage CG DevX installation by transferring the state file ( state.yaml ) and certificates ( cgdevx_ed* , cgdevx_k8s* ) from the local folder to another machine. Please note that this user should have the same access level to K8s cluster as one who provisioned the system. Additional permissions and configuration changes may be required depending on authentication options used for the cloud provider.","title":"Get CG DevX CLI"},{"location":"operators_guide/installation/quickstart/#installing-the-cg-devx-reference-architecture","text":"Once you install the CLI, you should be able to provision a cluster using the setup command. The CG DevX CLI tool will provide detailed information on setup process progress, and provide you with platform user credentials. Please store them safely. You could get credentials by re-running the setup command on the same machine used for installation. Your kubeconfig file will be located in CG DevX local folder ( ~/.cgdevx/kubeconfig ) The platform GitOps repository readme file ( README.md ) will contain links to all the core services.","title":"Installing the CG DevX reference architecture"},{"location":"operators_guide/installation/quickstart/#managing-workloads","text":"Once you have a running cluster, you can create your first workload using this guide .","title":"Managing Workloads"},{"location":"operators_guide/installation/cloud/aws_setup/","text":"Deploying on AWS When deploying to AWS, ensure that you have: An AWS account with billing enabled. (Remember, deploying clusters will incur charges. Make sure to destroy resources when you're finished with them!) A public hosted zone with DNS routing. To set this up, you can follow this guide . A user account with AdministratorAccess . We recommend that rather than using your root account, you set up a new IAM user, then grant it AdministratorAccess. You can use this guide to set up an IAM account, and this guide to grant it AdministratorAccess . The security credentials for this account, which enables CGDevX to use it. Use this guide to get your access keys. The AWS CLI installed and configured to use this user. You can use this guide to install the CLI.","title":"Deploying on AWS"},{"location":"operators_guide/installation/cloud/aws_setup/#deploying-on-aws","text":"When deploying to AWS, ensure that you have: An AWS account with billing enabled. (Remember, deploying clusters will incur charges. Make sure to destroy resources when you're finished with them!) A public hosted zone with DNS routing. To set this up, you can follow this guide . A user account with AdministratorAccess . We recommend that rather than using your root account, you set up a new IAM user, then grant it AdministratorAccess. You can use this guide to set up an IAM account, and this guide to grant it AdministratorAccess . The security credentials for this account, which enables CGDevX to use it. Use this guide to get your access keys. The AWS CLI installed and configured to use this user. You can use this guide to install the CLI.","title":"Deploying on AWS"},{"location":"operators_guide/installation/cloud/azure_setup/","text":"Deploying on Azure When deploying to Azure, ensure that you have: An Azure account with billing enabled. (Remember, deploying clusters will incur charges. Make sure to destroy resources when you're finished with them!) A public DNS zone hosted in Azure DNS. To set this up, you can follow this guide . A user account with Owner access. You can use this guide to set it up, or this guide to grant permissions to an existing user. The Azure CLI ( az ) and kubelogin installed and configured to use this user. You can use this and this guides to install the CLI.","title":"Deploying on Azure"},{"location":"operators_guide/installation/cloud/azure_setup/#deploying-on-azure","text":"When deploying to Azure, ensure that you have: An Azure account with billing enabled. (Remember, deploying clusters will incur charges. Make sure to destroy resources when you're finished with them!) A public DNS zone hosted in Azure DNS. To set this up, you can follow this guide . A user account with Owner access. You can use this guide to set it up, or this guide to grant permissions to an existing user. The Azure CLI ( az ) and kubelogin installed and configured to use this user. You can use this and this guides to install the CLI.","title":"Deploying on Azure"},{"location":"operators_guide/installation/cloud/gcp_setup/","text":"Deploying on Google Cloud When deploying to Google Compute Platform, ensure that you have: An account with billing enabled. (Remember, deploying clusters will incur charges. Make sure to destroy resources when you're finished with them!) Google Cloud Project you will be deploying to. You could follow this guide to create one. Project ID will be used as cloud-profile during setup. A public DNS zone hosted in Cloud DNS. To set this up, you can follow this guide . A user account with Owner permissions. You can use this guide to grant permissions. The Google Cloud CLI ( gcloud ) and google-cloud-cli-gke-gcloud-auth-plugin plugin installed and configured to use this user. You can use this and this guides to install and configure the CLI.","title":"Deploying on Google Cloud"},{"location":"operators_guide/installation/cloud/gcp_setup/#deploying-on-google-cloud","text":"When deploying to Google Compute Platform, ensure that you have: An account with billing enabled. (Remember, deploying clusters will incur charges. Make sure to destroy resources when you're finished with them!) Google Cloud Project you will be deploying to. You could follow this guide to create one. Project ID will be used as cloud-profile during setup. A public DNS zone hosted in Cloud DNS. To set this up, you can follow this guide . A user account with Owner permissions. You can use this guide to grant permissions. The Google Cloud CLI ( gcloud ) and google-cloud-cli-gke-gcloud-auth-plugin plugin installed and configured to use this user. You can use this and this guides to install and configure the CLI.","title":"Deploying on Google Cloud"},{"location":"operators_guide/installation/vcs/github_setup/","text":"Using GitHub When using GitHub as your Version Control System (VCS), ensure that you have: A User Account for Automation : This account will be used to create and manage repositories, make commits, manage users, and perform other tasks, such as executing Terraform scripts. You can follow this guide to create a new GitHub account. A GitHub Organization : Organizations are used to group repositories, and CG DevX will create new repositories within a specific organization so that it's easy to find and manage later should you decide to stop using CG DevX. If you don't have one, please follow this guide to create one. The user from step 1 should be a member of this organization. A Personal Access Token for the Account from Step 1 : This token will enable CG DevX to take action on the user's behalf, such as creating and managing repositories. To get a personal access token, also known as a \"developer token\", please follow the steps described in this guide . The GitHub token will be used to authenticate with the GitHub API and perform various actions such as creating and deleting repositories, groups, and other users. To provide permission for these actions, make sure you select the following set of scopes: - [x] **repo** Full control of private repositories - [x] **repo:status** Access commit status - [x] **repo_deployment** Access deployment status - [x] **public_repo** Access public repositories - [x] **repo:invite** Access repository invitations - [x] **security_events** Read and write security events - [x] **workflow** Update GitHub Action workflows - [x] **write:packages** Upload packages to GitHub Package Registry - [x] **read:packages** Download packages from GitHub Package Registry - [x] **admin:org** Full control of orgs and teams, read and write org projects - [x] **write:org** Read and write org and team membership, read and write org projects - [x] **read:org** Read org and team membership, read org projects - [x] **manage_runners:org** Manage org runners and runner groups - [x] **admin:public_key** Full control of user public keys - [x] **write:public_key** Write user public keys - [x] **read:public_key** Read user public keys - [x] **admin:repo_hook** Full control of repository hooks - [x] **write:repo_hook** Write repository hooks - [x] **read:repo_hook** Read repository hooks - [x] **admin:org_hook** Full control of organization hooks - [x] **user** Update ALL user data - [x] **read:user** Read ALL user profile data - [x] **user:email** Access user email addresses (read-only) - [x] **user:follow** Follow and unfollow users - [x] **delete_repo** Delete repositories - [x] **admin:ssh_signing_key** Full control of public user SSH signing keys - [x] **write:ssh_signing_key** Write public user SSH signing keys - [x] **read:ssh_signing_key** Read public user SSH signing keys","title":"Using GitHub"},{"location":"operators_guide/installation/vcs/github_setup/#using-github","text":"When using GitHub as your Version Control System (VCS), ensure that you have: A User Account for Automation : This account will be used to create and manage repositories, make commits, manage users, and perform other tasks, such as executing Terraform scripts. You can follow this guide to create a new GitHub account. A GitHub Organization : Organizations are used to group repositories, and CG DevX will create new repositories within a specific organization so that it's easy to find and manage later should you decide to stop using CG DevX. If you don't have one, please follow this guide to create one. The user from step 1 should be a member of this organization. A Personal Access Token for the Account from Step 1 : This token will enable CG DevX to take action on the user's behalf, such as creating and managing repositories. To get a personal access token, also known as a \"developer token\", please follow the steps described in this guide . The GitHub token will be used to authenticate with the GitHub API and perform various actions such as creating and deleting repositories, groups, and other users. To provide permission for these actions, make sure you select the following set of scopes: - [x] **repo** Full control of private repositories - [x] **repo:status** Access commit status - [x] **repo_deployment** Access deployment status - [x] **public_repo** Access public repositories - [x] **repo:invite** Access repository invitations - [x] **security_events** Read and write security events - [x] **workflow** Update GitHub Action workflows - [x] **write:packages** Upload packages to GitHub Package Registry - [x] **read:packages** Download packages from GitHub Package Registry - [x] **admin:org** Full control of orgs and teams, read and write org projects - [x] **write:org** Read and write org and team membership, read and write org projects - [x] **read:org** Read org and team membership, read org projects - [x] **manage_runners:org** Manage org runners and runner groups - [x] **admin:public_key** Full control of user public keys - [x] **write:public_key** Write user public keys - [x] **read:public_key** Read user public keys - [x] **admin:repo_hook** Full control of repository hooks - [x] **write:repo_hook** Write repository hooks - [x] **read:repo_hook** Read repository hooks - [x] **admin:org_hook** Full control of organization hooks - [x] **user** Update ALL user data - [x] **read:user** Read ALL user profile data - [x] **user:email** Access user email addresses (read-only) - [x] **user:follow** Follow and unfollow users - [x] **delete_repo** Delete repositories - [x] **admin:ssh_signing_key** Full control of public user SSH signing keys - [x] **write:ssh_signing_key** Write public user SSH signing keys - [x] **read:ssh_signing_key** Read public user SSH signing keys","title":"Using GitHub"},{"location":"operators_guide/installation/vcs/gitlab_setup/","text":"Using GitLab Create or use an existing GitLab account. Create a GitLab group with developer permissions. Create a GitLab personal access token following the steps described here . The token will be used to authenticate with the GitLab API and perform various actions such as creating and deleting repositories, groups, and other users. To provide permission for these actions, make sure you select the following set of scopes: [x] api Grants complete read/write access to the API, including all groups and projects, the container registry, and the package registry.","title":"Using GitLab"},{"location":"operators_guide/installation/vcs/gitlab_setup/#using-gitlab","text":"Create or use an existing GitLab account. Create a GitLab group with developer permissions. Create a GitLab personal access token following the steps described here . The token will be used to authenticate with the GitLab API and perform various actions such as creating and deleting repositories, groups, and other users. To provide permission for these actions, make sure you select the following set of scopes: [x] api Grants complete read/write access to the API, including all groups and projects, the container registry, and the package registry.","title":"Using GitLab"},{"location":"operators_guide/platform_management/platform_repo/","text":"CG DevX platform GitOps repository When setting up CG DevX, the CLI tool creates your platform repository, which serves as the basis for GitOps operations. CG DevX relies on the GitOps approach for platform management. The GitOps repository has two main sections /gitops_pipelines : delivery pipeline configurations /terraform : infrastructure as code and configuration as code for all the cloud services, git provider, secrets and user management The repository root readme.md file contains links to all core services: Application Namespace Description URL (where applicable) Vault vault Secrets Management https://vault. . Argo CD argocd GitOps Continuous Delivery https://argocd. . Argo Workflows argo Application Continuous Integration https://argo. . Atlantis atlantis Terraform Workflow Automation https://atlantis. . Harbor harbor Image & Helm Chart Registry https://harbor. . Grafana monitoring Observability https://grafana. . SonarQube sonarqube Code Quality https://sonarqube. . Backstage backstage Portal https://backstage. . Cert-manager with a Let's Encrypt ClusterIssuer for TLS encryption is used to secure all public-facing services on Ingress. Certificates are requested automatically and will be auto-renewed. GitOps pipelines The GitOps pipeline contains cluster configurations for multi cluster support (to be added later), and ArgoCD manifests for all the core services. The core services configuration is located at /gitops-pipelines/delivery/clusters/cc-cluster/core-services/ . An ArgoCD App of Apps pattern is used here. Workloads are managed via an ApplicationSet that monitors the /gitops-pipelines/delivery/clusters/cc-cluster/workloads/ directory. The default workload template is located at /gitops-pipelines/delivery/clusters/cc-cluster/workloads/workload-template.yaml , and is used by the workload bootstrap command IaC Terraform is used as an implementation of IaC for CG DevX platform. The Terraform code is located at ./terraform/ . The configuration consists of five logical blocks providing resource and configuration management for: git provider configuration and repositories cloud resources secrets user management core services Logical blocks rely on high-order terraform modules defined under /terraform/modules/ . Those modules abstract away specifics of block implementation by providing a simple-to-use interface. When the default behavior of a module cannot be changed through configuration, you can edit modules achieve the expected behavior. Terraform state files will be stored in a cloud provider specific storage backend. The CLI tool creates storage using the cloud API, and is hardened, where implementation is cloud provider specific, but will always try to: enable versioning enable delete protection limit access to the user whose credentials are used to install CG DevX, plus the role assumed by the PR Automation service (Atlantis). IaC PR Automation All of our terraform management is automated with a tool called Atlantis, which integrates with your git pull requests. When you make a change to a *.tf file, even a whitespace change, Atlantis will pick up that change. Atlantis automatically runs terraform plan and comment back to PR with the details. The pull request goes through the review proces, and when approved, changes can be applied by commenting atlantis apply . Atlantis will run terraform apply , merge the PR, and delete the branch. Atlantis: enables code review for infrastructure changes allows doing infrastructure changes without credentials serves as audit log Atlantis' configuration is described in atlantis.yaml file setting the correct execution order of logical blocks. CG DevX default settings require at least one person besides the author of the PR to approve it. This is not a default Atlantis configuration and is done as a part of security hardening. You could change Atlantis settings /gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/atlantis/application.yaml For more details, please see Atlantis official documentation","title":"CG DevX platform GitOps repository"},{"location":"operators_guide/platform_management/platform_repo/#cg-devx-platform-gitops-repository","text":"When setting up CG DevX, the CLI tool creates your platform repository, which serves as the basis for GitOps operations. CG DevX relies on the GitOps approach for platform management. The GitOps repository has two main sections /gitops_pipelines : delivery pipeline configurations /terraform : infrastructure as code and configuration as code for all the cloud services, git provider, secrets and user management The repository root readme.md file contains links to all core services: Application Namespace Description URL (where applicable) Vault vault Secrets Management https://vault. . Argo CD argocd GitOps Continuous Delivery https://argocd. . Argo Workflows argo Application Continuous Integration https://argo. . Atlantis atlantis Terraform Workflow Automation https://atlantis. . Harbor harbor Image & Helm Chart Registry https://harbor. . Grafana monitoring Observability https://grafana. . SonarQube sonarqube Code Quality https://sonarqube. . Backstage backstage Portal https://backstage. . Cert-manager with a Let's Encrypt ClusterIssuer for TLS encryption is used to secure all public-facing services on Ingress. Certificates are requested automatically and will be auto-renewed.","title":"CG DevX platform GitOps repository"},{"location":"operators_guide/platform_management/platform_repo/#gitops-pipelines","text":"The GitOps pipeline contains cluster configurations for multi cluster support (to be added later), and ArgoCD manifests for all the core services. The core services configuration is located at /gitops-pipelines/delivery/clusters/cc-cluster/core-services/ . An ArgoCD App of Apps pattern is used here. Workloads are managed via an ApplicationSet that monitors the /gitops-pipelines/delivery/clusters/cc-cluster/workloads/ directory. The default workload template is located at /gitops-pipelines/delivery/clusters/cc-cluster/workloads/workload-template.yaml , and is used by the workload bootstrap command","title":"GitOps pipelines"},{"location":"operators_guide/platform_management/platform_repo/#iac","text":"Terraform is used as an implementation of IaC for CG DevX platform. The Terraform code is located at ./terraform/ . The configuration consists of five logical blocks providing resource and configuration management for: git provider configuration and repositories cloud resources secrets user management core services Logical blocks rely on high-order terraform modules defined under /terraform/modules/ . Those modules abstract away specifics of block implementation by providing a simple-to-use interface. When the default behavior of a module cannot be changed through configuration, you can edit modules achieve the expected behavior. Terraform state files will be stored in a cloud provider specific storage backend. The CLI tool creates storage using the cloud API, and is hardened, where implementation is cloud provider specific, but will always try to: enable versioning enable delete protection limit access to the user whose credentials are used to install CG DevX, plus the role assumed by the PR Automation service (Atlantis).","title":"IaC"},{"location":"operators_guide/platform_management/platform_repo/#iac-pr-automation","text":"All of our terraform management is automated with a tool called Atlantis, which integrates with your git pull requests. When you make a change to a *.tf file, even a whitespace change, Atlantis will pick up that change. Atlantis automatically runs terraform plan and comment back to PR with the details. The pull request goes through the review proces, and when approved, changes can be applied by commenting atlantis apply . Atlantis will run terraform apply , merge the PR, and delete the branch. Atlantis: enables code review for infrastructure changes allows doing infrastructure changes without credentials serves as audit log Atlantis' configuration is described in atlantis.yaml file setting the correct execution order of logical blocks. CG DevX default settings require at least one person besides the author of the PR to approve it. This is not a default Atlantis configuration and is done as a part of security hardening. You could change Atlantis settings /gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/atlantis/application.yaml For more details, please see Atlantis official documentation","title":"IaC PR Automation"},{"location":"operators_guide/platform_management/iac/core_infrastructure_management/","text":"Platform infrastructure Core infrastructure required by the platform is provisioned by a cloud-provider-specific module defined in /terraform/hosting_provider/main.tf . The minimal set of parameters for the module includes: cluster_name : K8s cluster name region : Location of the cluster, e.g., region name for AWS, location for Azure, cloud-provider-specific alert_emails : Email(s) that will be subscribed for alerts from cloud native monitoring cluster_ssh_public_key : The SSH key used to access K8s nodes when possible, auto generated by CLI, cloud-provider-specific tags : The default tags/labels applied to cloud resources; cloud-provider-specific cluster_node_labels : The default labels applied to K8s cluster nodes domain_name : The FQDN used by the CC cluster workloads : Workload definitions loaded from the workloads.tfvars.json file. Provisioning uses cloud-provider module specifics for the rest of parameters. Defaults are set to be the most cost-efficient with respect to instance types availability and resource limits of a new cloud account. You should adjust defaults based on your requirements. cluster_network_cidr : K8s cluster network CIDR, defaults to 10.0.0.0/16 az_count : Number of availability zones cluster_version : K8s cluster version node_groups : Array of objects representing K8s node groups name : Node group name, defaults to default instance_types : A compute node type (size), will fall back to the next value when the first specified instance type is not available. Can be used in combination with capacity_type = \"spot\"; cloud-provider-specific min_size : Minimum node count for K8s cluster max_size : Maximum node count for K8s cluster desired_size : Desired node count for K8s cluster; capacity_type : Determines compute node capacity type, e.g. \"on-demand\" or \"spot\", defaults to \"on-demand\"; cloud-provider-specific The infrastructure module uses workload definitions to manage access to shared resources. Definitions are provided using the workloads variable passed via the terraform.tfvars.json file. Below is an example of a terraform.tfvars.json file containing one workload called demo-workload . { \"workloads\": { \"demo-workload\": { \"description\": \"CG DevX Demo-Workload workload definition\" } } }","title":"Platform infrastructure"},{"location":"operators_guide/platform_management/iac/core_infrastructure_management/#platform-infrastructure","text":"Core infrastructure required by the platform is provisioned by a cloud-provider-specific module defined in /terraform/hosting_provider/main.tf . The minimal set of parameters for the module includes: cluster_name : K8s cluster name region : Location of the cluster, e.g., region name for AWS, location for Azure, cloud-provider-specific alert_emails : Email(s) that will be subscribed for alerts from cloud native monitoring cluster_ssh_public_key : The SSH key used to access K8s nodes when possible, auto generated by CLI, cloud-provider-specific tags : The default tags/labels applied to cloud resources; cloud-provider-specific cluster_node_labels : The default labels applied to K8s cluster nodes domain_name : The FQDN used by the CC cluster workloads : Workload definitions loaded from the workloads.tfvars.json file. Provisioning uses cloud-provider module specifics for the rest of parameters. Defaults are set to be the most cost-efficient with respect to instance types availability and resource limits of a new cloud account. You should adjust defaults based on your requirements. cluster_network_cidr : K8s cluster network CIDR, defaults to 10.0.0.0/16 az_count : Number of availability zones cluster_version : K8s cluster version node_groups : Array of objects representing K8s node groups name : Node group name, defaults to default instance_types : A compute node type (size), will fall back to the next value when the first specified instance type is not available. Can be used in combination with capacity_type = \"spot\"; cloud-provider-specific min_size : Minimum node count for K8s cluster max_size : Maximum node count for K8s cluster desired_size : Desired node count for K8s cluster; capacity_type : Determines compute node capacity type, e.g. \"on-demand\" or \"spot\", defaults to \"on-demand\"; cloud-provider-specific The infrastructure module uses workload definitions to manage access to shared resources. Definitions are provided using the workloads variable passed via the terraform.tfvars.json file. Below is an example of a terraform.tfvars.json file containing one workload called demo-workload . { \"workloads\": { \"demo-workload\": { \"description\": \"CG DevX Demo-Workload workload definition\" } } }","title":"Platform infrastructure"},{"location":"operators_guide/platform_management/iac/core_services/","text":"Core platform services Configuration management of core platform services is done using the core_services module. This module acts as an umbrella, managing the following services: Registry : Harbor - Image registry Code Quality : SonarQube - Code quality inspection tool Configuration is described in /terraform/core_services/main.tf . The set of parameters for the module includes: code_quality_oidc_client_id : Code Quality service OIDC client id code_quality_oidc_client_secret : Code Quality service OIDC client secret code_quality_admin_password : Code Quality service admin user password, auto-generated and stored in Vault registry_oidc_client_id : Registry services OIDC client id registry_oidc_client_secret : Registry services OIDC client secret registry_main_robot_password : Registry services admin user password, auto-generated and stored in Vault workloads : workloads definitions loaded from the workloads.tfvars.json file The core services module uses workload definitions to enable a personalized experience for each workload team, isolating workloads using abstraction, for example, a project in each of the services, and limiting access to those abstractions. Definitions are provided using the workloads variable passed via the terraform.tfvars.json file. Below is an example of a terraform.tfvars.json file containing one workload called demo-workload . { \"workloads\": { \"demo-workload\": { \"description\": \"CG DevX Demo-Workload workload definition\" } } }","title":"Core platform services"},{"location":"operators_guide/platform_management/iac/core_services/#core-platform-services","text":"Configuration management of core platform services is done using the core_services module. This module acts as an umbrella, managing the following services: Registry : Harbor - Image registry Code Quality : SonarQube - Code quality inspection tool Configuration is described in /terraform/core_services/main.tf . The set of parameters for the module includes: code_quality_oidc_client_id : Code Quality service OIDC client id code_quality_oidc_client_secret : Code Quality service OIDC client secret code_quality_admin_password : Code Quality service admin user password, auto-generated and stored in Vault registry_oidc_client_id : Registry services OIDC client id registry_oidc_client_secret : Registry services OIDC client secret registry_main_robot_password : Registry services admin user password, auto-generated and stored in Vault workloads : workloads definitions loaded from the workloads.tfvars.json file The core services module uses workload definitions to enable a personalized experience for each workload team, isolating workloads using abstraction, for example, a project in each of the services, and limiting access to those abstractions. Definitions are provided using the workloads variable passed via the terraform.tfvars.json file. Below is an example of a terraform.tfvars.json file containing one workload called demo-workload . { \"workloads\": { \"demo-workload\": { \"description\": \"CG DevX Demo-Workload workload definition\" } } }","title":"Core platform services"},{"location":"operators_guide/platform_management/iac/secrets_management/","text":"Secrets management CG DevX manages secrets (platform and workloads), and provides SSO functionality by using the secrets management provider (Hashicorp Vault by default) OIDC capabilities The secrets module is defined in /terraform/secrets/main.tf . The secrets management module securely stores a set of parameters required to \"execute\" (plan and apply changes) terraform by PR Automation (Atlantis): cluster_name : K8s cluster name workloads : Workloads definitions loaded from the workloads.tfvars.json file vcs_bot_ssh_public_key : Git machine user SSH public key, generated by CLI vcs_bot_ssh_private_key : Git machine user SSH private key, generated by CLI vcs_token : Git access token atlantis_repo_webhook_secret : IaC PR automation webhook secret atlantis_repo_webhook_url : IaC PR automation webhook URL vault_token : Secrets Manager root access token cluster_endpoint : K8s cluster control plane endpoint cluster_ssh_public_key : SSH key used to access K8s nodes when possible, auto generated by CLI, cloud-provider-specific tf_backend_storage_access_key : Access key for IaC storage backend, limited to specific cloud providers The secrets management module uses workload definitions to manage per-workload secret spaces and access. Definitions are provided using the workloads variable passed via the terraform.tfvars.json file. Below is an example of a terraform.tfvars.json file containing one workload called demo-workload . { \"workloads\": { \"demo-workload\": { \"description\": \"CG DevX Demo-Workload workload definition\" } } }","title":"Secrets management"},{"location":"operators_guide/platform_management/iac/secrets_management/#secrets-management","text":"CG DevX manages secrets (platform and workloads), and provides SSO functionality by using the secrets management provider (Hashicorp Vault by default) OIDC capabilities The secrets module is defined in /terraform/secrets/main.tf . The secrets management module securely stores a set of parameters required to \"execute\" (plan and apply changes) terraform by PR Automation (Atlantis): cluster_name : K8s cluster name workloads : Workloads definitions loaded from the workloads.tfvars.json file vcs_bot_ssh_public_key : Git machine user SSH public key, generated by CLI vcs_bot_ssh_private_key : Git machine user SSH private key, generated by CLI vcs_token : Git access token atlantis_repo_webhook_secret : IaC PR automation webhook secret atlantis_repo_webhook_url : IaC PR automation webhook URL vault_token : Secrets Manager root access token cluster_endpoint : K8s cluster control plane endpoint cluster_ssh_public_key : SSH key used to access K8s nodes when possible, auto generated by CLI, cloud-provider-specific tf_backend_storage_access_key : Access key for IaC storage backend, limited to specific cloud providers The secrets management module uses workload definitions to manage per-workload secret spaces and access. Definitions are provided using the workloads variable passed via the terraform.tfvars.json file. Below is an example of a terraform.tfvars.json file containing one workload called demo-workload . { \"workloads\": { \"demo-workload\": { \"description\": \"CG DevX Demo-Workload workload definition\" } } }","title":"Secrets management"},{"location":"operators_guide/platform_management/iac/users_management/","text":"User management CG DevX manages users using a module defined in /terraform/users/main.tf . By default, a CG DevX installation will have one admin level machine user named CG DEVX-bot . You can create additional users by editing the users variable that has the following structure: users = { \"<user_name>\" = { vcs_username = \"Git user name\" email = \"User email\" first_name = \"User first name\" last_name = \"User last name\" # Git team slugs generated by VCS module, e.g. vcs_team_slugs = [\"${local.gitops_repo_name}-admins\", \"<workload>-maintainers\"] # ACL policies, e.g. acl_policies = [\"admin\", \"developers\", \"default\", \"<workload>-admins\"] # OIDC access group, e.g. oidc_groups_for_user = [\"admins\", \"developers\", \"<workload>-admins\"] }, }","title":"User management"},{"location":"operators_guide/platform_management/iac/users_management/#user-management","text":"CG DevX manages users using a module defined in /terraform/users/main.tf . By default, a CG DevX installation will have one admin level machine user named CG DEVX-bot . You can create additional users by editing the users variable that has the following structure: users = { \"<user_name>\" = { vcs_username = \"Git user name\" email = \"User email\" first_name = \"User first name\" last_name = \"User last name\" # Git team slugs generated by VCS module, e.g. vcs_team_slugs = [\"${local.gitops_repo_name}-admins\", \"<workload>-maintainers\"] # ACL policies, e.g. acl_policies = [\"admin\", \"developers\", \"default\", \"<workload>-admins\"] # OIDC access group, e.g. oidc_groups_for_user = [\"admins\", \"developers\", \"<workload>-admins\"] }, }","title":"User management"},{"location":"operators_guide/platform_management/iac/vcs_management/","text":"VCS management CG DevX manages all the repositories (platform and workloads) and associated user groups, and repository settings using the Git provider-specific module defined in /terraform/vcs/main.tf . The minimal set of parameters for the module is the following: gitops_repo_name : Platform GitOps repository name; atlantis_url : IaC PR automation webhook URL; atlantis_repo_webhook_secret : IaC PR automation (Atlantis) webhook secret, generated by CLI; vcs_bot_ssh_public_key : Git machine user SSH public key, generated by CLI; workloads : workloads definitions loaded from workloads.tfvars.json file cluster_name : K8s cluster name; vcs_owner : Git organization owner, provider specific; vcs_subscription_plan : Git subscription plan type, automatically determined by CLI. Workload repositories are defined as part of workload using the workloads variable passed via the terraform.tfvars.json file. Below is an example of a terraform.tfvars.json file containing one workload called demo-workload . Additional repositories can be added by updating the repos object. For more details, please see workload management { \"workloads\": { \"demo-workload\": { \"description\": \"CG DevX Demo-Workload workload definition\", \"repos\": { \"demo-workload\": {}, \"demo-workload-gitops\": { \"atlantis_enabled\": true } } } } }","title":"VCS management"},{"location":"operators_guide/platform_management/iac/vcs_management/#vcs-management","text":"CG DevX manages all the repositories (platform and workloads) and associated user groups, and repository settings using the Git provider-specific module defined in /terraform/vcs/main.tf . The minimal set of parameters for the module is the following: gitops_repo_name : Platform GitOps repository name; atlantis_url : IaC PR automation webhook URL; atlantis_repo_webhook_secret : IaC PR automation (Atlantis) webhook secret, generated by CLI; vcs_bot_ssh_public_key : Git machine user SSH public key, generated by CLI; workloads : workloads definitions loaded from workloads.tfvars.json file cluster_name : K8s cluster name; vcs_owner : Git organization owner, provider specific; vcs_subscription_plan : Git subscription plan type, automatically determined by CLI. Workload repositories are defined as part of workload using the workloads variable passed via the terraform.tfvars.json file. Below is an example of a terraform.tfvars.json file containing one workload called demo-workload . Additional repositories can be added by updating the repos object. For more details, please see workload management { \"workloads\": { \"demo-workload\": { \"description\": \"CG DevX Demo-Workload workload definition\", \"repos\": { \"demo-workload\": {}, \"demo-workload-gitops\": { \"atlantis_enabled\": true } } } } }","title":"VCS management"},{"location":"operators_guide/platform_management/services/delivery/","text":"TO BE WRITTEN (Watch this space!)","title":"Delivery"},{"location":"operators_guide/platform_management/services/integration/","text":"TO BE WRITTEN (Watch this space!)","title":"Integration"},{"location":"operators_guide/platform_management/services/monitoring/","text":"TO BE WRITTEN (Watch this space!)","title":"Monitoring"},{"location":"operators_guide/platform_management/services/registry/","text":"TO BE WRITTEN (Watch this space!)","title":"Registry"},{"location":"operators_guide/platform_management/services/secrets/","text":"Secrets Management Vault CG DevX uses Vault , an open source secrets manager and identity provider. Authentication You can log in into Vault using platform user credentials provided by CG DevX CLI. You can also log in using the root token, which has full administrative permission--but we strongly advise against it. Vault is also used as an OIDC provider, so all other services are using it to provide an SSO experience. Initially, there will be only a single user created, representing the CG DevX machine user account. You can create additional accounts by updating IaC and creating a PR, and you can find more details here . You will be able to get additional user account passwords from Vault once the PR is merged. Note, it's only possible to access user account passwords when using the root token to log in. Additional authentication methods can be configured for Vault as described here . Kubernetes The external-secrets-operator service is preconfigured with a service account that can pull secrets from Vault. This makes secrets stored in Vault available as native Kubernetes Secrets for your workload to consume. Secrets setup Secrets for platform services are stored by the secrets engine -- by default, the vault kv v2 backend. When working with K8s manifests, you can reference a secret in Vault using the following definition. Good practice is to use external-secrets.yaml files. apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: awesome-secret spec: target: name: awesome-secret secretStoreRef: kind: ClusterSecretStore name: vault-kv-secret refreshInterval: 10s dataFrom: - extract: key: secret/awesome-service/awesome-secret This resource will be deployed with the service, connecting to vault-kv-secret , and pulling secrets from Vault to K8s. You can pull all the key value pairs, or specify exactly which specific pairs should be pulled.","title":"Secrets Management"},{"location":"operators_guide/platform_management/services/secrets/#secrets-management","text":"","title":"Secrets Management"},{"location":"operators_guide/platform_management/services/secrets/#vault","text":"CG DevX uses Vault , an open source secrets manager and identity provider.","title":"Vault"},{"location":"operators_guide/platform_management/services/secrets/#authentication","text":"You can log in into Vault using platform user credentials provided by CG DevX CLI. You can also log in using the root token, which has full administrative permission--but we strongly advise against it. Vault is also used as an OIDC provider, so all other services are using it to provide an SSO experience. Initially, there will be only a single user created, representing the CG DevX machine user account. You can create additional accounts by updating IaC and creating a PR, and you can find more details here . You will be able to get additional user account passwords from Vault once the PR is merged. Note, it's only possible to access user account passwords when using the root token to log in. Additional authentication methods can be configured for Vault as described here .","title":"Authentication"},{"location":"operators_guide/platform_management/services/secrets/#kubernetes","text":"The external-secrets-operator service is preconfigured with a service account that can pull secrets from Vault. This makes secrets stored in Vault available as native Kubernetes Secrets for your workload to consume.","title":"Kubernetes"},{"location":"operators_guide/platform_management/services/secrets/#secrets-setup","text":"Secrets for platform services are stored by the secrets engine -- by default, the vault kv v2 backend. When working with K8s manifests, you can reference a secret in Vault using the following definition. Good practice is to use external-secrets.yaml files. apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: awesome-secret spec: target: name: awesome-secret secretStoreRef: kind: ClusterSecretStore name: vault-kv-secret refreshInterval: 10s dataFrom: - extract: key: secret/awesome-service/awesome-secret This resource will be deployed with the service, connecting to vault-kv-secret , and pulling secrets from Vault to K8s. You can pull all the key value pairs, or specify exactly which specific pairs should be pulled.","title":"Secrets setup"},{"location":"operators_guide/platform_management/services/security/","text":"TO BE WRITTEN (Watch this space!)","title":"Security"},{"location":"operators_guide/workload_management/bootstrap_templates/","text":"Bootstrap templates CG DevX provides a set of default bootstrap templates for workloads, which are set as a default value for the workload bootstrap command . Those templates provide a user with a pre-defined logical structure of repositories and allow seamless integration with platform core services, like delivery pipelines (AKA CI/CD), IaC PR automation, etc. Templates can be cloned and customized by the user and used for bootstrapping new workloads. The default templates are Workload code repository Workload GitOps repository Those templates provide you with: The Workload repository structure A Pre-defined Docker file CI process CD process Release promotion process GitOps style environment definition IaC for out of the cluster cloud resources Additional templates will be provided later. Generic Workload template Code repository The Workload code repository uses the following structure: . \u251c\u2500\u2500 .argo \u2502 \u251c\u2500\u2500 build-wf.yaml \u2502 \u251c\u2500\u2500 crane-wf.yaml \u2502 \u251c\u2500\u2500 version-changer-wf.yaml \u2502 \u2514\u2500\u2500 wl-service-wf.yaml \u251c\u2500\u2500 .github \u2502 \u2514\u2500\u2500 workflows \u2502 \u251c\u2500\u2500 multi_service_image_build.yml \u2502 \u2514\u2500\u2500 multi_service_parallel_build.yml \u2514\u2500\u2500 <service folder> \u251c\u2500\u2500 Dockerfile \u2514\u2500\u2500 src \u2514\u2500\u2500 <your service code> The repository already contains pre-configured GitHub Actions (in the .github folder) and Argo Workflows (in the .argo folder) to build new images and update workload manifests. The service folder will be named after the workload. When using a monorepo for multiple services, each service should have its own folder and its own Dockerfile. GitOps repository The Workload GitOps repository is using the following structure: . \u251c\u2500\u2500 .argo \u2502 \u2514\u2500\u2500 promote-wf.yaml \u251c\u2500\u2500 .github \u2502 \u251c\u2500\u2500 terraform.yaml \u2502 \u2514\u2500\u2500 workflows \u2502 \u251c\u2500\u2500 promote.yaml \u2502 \u2514\u2500\u2500 promote_gha_only.yaml_gha \u251c\u2500\u2500 atlantis.yaml \u251c\u2500\u2500 gitops \u2502 \u251c\u2500\u2500 .gitignore \u2502 \u2514\u2500\u2500 environments \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 base \u2502 \u2502 ... \u2502 \u251c\u2500\u2500 envs \u2502 \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2502 ... \u2502 \u2502 \u251c\u2500\u2500 prod \u2502 \u2502 \u2502 ... \u2502 \u2502 \u2514\u2500\u2500 sta \u2502 \u2502 ... \u2502 \u2514\u2500\u2500 variants \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 ... \u2502 \u251c\u2500\u2500 prod \u2502 \u2502 ... \u2502 \u2514\u2500\u2500 uat \u2502 ... \u2514\u2500\u2500 terraform \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 infrastructure \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 main.tf \u2502 \u251c\u2500\u2500 outputs.tf \u2502 \u251c\u2500\u2500 samples \u2502 \u2502 ... \u2502 \u2514\u2500\u2500 variables.tf \u2514\u2500\u2500 secrets \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 outputs.tf \u251c\u2500\u2500 secrets.tf \u2514\u2500\u2500 variables.tf The repository already contains pre-configured GitHub Actions (in the .github folder) and Argo Workflows (in the .argo folder) required for GitOps-style workload version promotion.","title":"Bootstrap templates"},{"location":"operators_guide/workload_management/bootstrap_templates/#bootstrap-templates","text":"CG DevX provides a set of default bootstrap templates for workloads, which are set as a default value for the workload bootstrap command . Those templates provide a user with a pre-defined logical structure of repositories and allow seamless integration with platform core services, like delivery pipelines (AKA CI/CD), IaC PR automation, etc. Templates can be cloned and customized by the user and used for bootstrapping new workloads. The default templates are Workload code repository Workload GitOps repository Those templates provide you with: The Workload repository structure A Pre-defined Docker file CI process CD process Release promotion process GitOps style environment definition IaC for out of the cluster cloud resources Additional templates will be provided later.","title":"Bootstrap templates"},{"location":"operators_guide/workload_management/bootstrap_templates/#generic-workload-template","text":"","title":"Generic Workload template"},{"location":"operators_guide/workload_management/bootstrap_templates/#code-repository","text":"The Workload code repository uses the following structure: . \u251c\u2500\u2500 .argo \u2502 \u251c\u2500\u2500 build-wf.yaml \u2502 \u251c\u2500\u2500 crane-wf.yaml \u2502 \u251c\u2500\u2500 version-changer-wf.yaml \u2502 \u2514\u2500\u2500 wl-service-wf.yaml \u251c\u2500\u2500 .github \u2502 \u2514\u2500\u2500 workflows \u2502 \u251c\u2500\u2500 multi_service_image_build.yml \u2502 \u2514\u2500\u2500 multi_service_parallel_build.yml \u2514\u2500\u2500 <service folder> \u251c\u2500\u2500 Dockerfile \u2514\u2500\u2500 src \u2514\u2500\u2500 <your service code> The repository already contains pre-configured GitHub Actions (in the .github folder) and Argo Workflows (in the .argo folder) to build new images and update workload manifests. The service folder will be named after the workload. When using a monorepo for multiple services, each service should have its own folder and its own Dockerfile.","title":"Code repository"},{"location":"operators_guide/workload_management/bootstrap_templates/#gitops-repository","text":"The Workload GitOps repository is using the following structure: . \u251c\u2500\u2500 .argo \u2502 \u2514\u2500\u2500 promote-wf.yaml \u251c\u2500\u2500 .github \u2502 \u251c\u2500\u2500 terraform.yaml \u2502 \u2514\u2500\u2500 workflows \u2502 \u251c\u2500\u2500 promote.yaml \u2502 \u2514\u2500\u2500 promote_gha_only.yaml_gha \u251c\u2500\u2500 atlantis.yaml \u251c\u2500\u2500 gitops \u2502 \u251c\u2500\u2500 .gitignore \u2502 \u2514\u2500\u2500 environments \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 base \u2502 \u2502 ... \u2502 \u251c\u2500\u2500 envs \u2502 \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2502 ... \u2502 \u2502 \u251c\u2500\u2500 prod \u2502 \u2502 \u2502 ... \u2502 \u2502 \u2514\u2500\u2500 sta \u2502 \u2502 ... \u2502 \u2514\u2500\u2500 variants \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 ... \u2502 \u251c\u2500\u2500 prod \u2502 \u2502 ... \u2502 \u2514\u2500\u2500 uat \u2502 ... \u2514\u2500\u2500 terraform \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 infrastructure \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 main.tf \u2502 \u251c\u2500\u2500 outputs.tf \u2502 \u251c\u2500\u2500 samples \u2502 \u2502 ... \u2502 \u2514\u2500\u2500 variables.tf \u2514\u2500\u2500 secrets \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 outputs.tf \u251c\u2500\u2500 secrets.tf \u2514\u2500\u2500 variables.tf The repository already contains pre-configured GitHub Actions (in the .github folder) and Argo Workflows (in the .argo folder) required for GitOps-style workload version promotion.","title":"GitOps repository"},{"location":"operators_guide/workload_management/cli_commands/","text":"CG DevX CLI Workload Management commands Below is a list of Workload Management commands supported by the CG DevX CLI tool. Workload Management commands depend on local cluster metadata and: a) can only be executed when the CG DevX cluster is already provisioned, and b) should be executed from the same machine where the cluster was provisioned. For more details on cluster provisioning, please check the setup command documentation Create The create command generates a declarative configuration of resources required for the workload to function. The configuration is placed in the platform GitOps repository. The CLI automatically pushes changes to a feature branch and creates a Pull Request (PR). Changes introduced with PR should be reviewed and adjusted when necessary. All the changes to the platform GitOps repository should be applied via Atlantis by typing atlantis apply in the PR comments section. The workload create command creates: The configuration for the VCS module to create the Workload source code monorepo and Workload GitOps repo The configuration for the Secrets module to create the Workload secrets namespace and RBAC The configuration for the Core Services module to create the Image Registry, Code Quality, etc. projects for the Workload The ArgoCD dedicated Workload Project ApplicationSet to automatically deliver Workload services The workload create command can be executed using arguments or environment variables. Arguments : Name (short, full) Type Description -wl, --workload-name TEXT Workload name -wlrn, --workload-repository-name TEXT Name for Workload repository -wlgrn, --workload-gitops-repository-name TEXT Name for Workload GitOps repository --verbosity [DEBUG, INFO, WARNING, ERROR, CRITICAL] Set the logging verbosity level, default CRITICAL Note! : For all names use kebab-case. Command snippet Using command arguments cgdevxcli workload create --workload-name your-workload-name \\ --workload-repository-name your-workload-repository-name --workload-gitops-repository-name your-workload-gitops-repository-name Bootstrap This command creates the folder structure and injects all the necessary configurations for your Workload into repositories created by the create command . By default, the bootstrap command uses: The CG DevX Workload template repository The CG DevX Workload GitOps template repository For other template options, please see bootstrap templates You can also fork and customize existing template repositories and use them by providing custom template repository URLs and branches. Note! : The boostrap command must be executed using the same workload and workload repository names. For example, if your --workload-name is my-workload your --workload-repository-name should be my-workload-repo . The workload bootstrap command can be executed using arguments or environment variables. Arguments : Name (short, full) Type Description -wl, --workload-name TEXT Workload name -wlrn, --workload-repository-name TEXT Name for Workload repository -wlgrn, --workload-gitops-repository-name TEXT Name for Workload GitOps repository -wltu, --workload-template-url TEXT Workload repository template -wltb, --workload-template-branch TEXT Workload repository template branch -wlgu, --workload-gitops-template-url TEXT Workload GitOps repository template -wlgb, --workload-gitops-template-branch TEXT Workload GitOps repository template branch -wls, --workload-service-name TEXT Workload service name -wlsp, --workload-service-port NUMBER Workload service port, default 3000 --verbosity [DEBUG, INFO, WARNING, ERROR, CRITICAL] Set the logging verbosity level, default CRITICAL Note! : For all names, use kebab-case. Command snippet Using command arguments cgdevxcli workload bootstrap --workload-name your-workload-name \\ --workload-repository-name your-workload-repository-name --workload-gitops-repository-name your-workload-gitops-repository-name --workload-service-name your-first-service-name --workload-service-port your-first-service-name-port Delete This command removes the declarative configuration of resources required for the workload to function. The CLI automatically pushes changes to a feature branch and creates a Pull Request (PR). Changes introduced with PR should be reviewed and adjusted when necessary. All the changes to platform GitOps repository should be applied via Atlantis by typing atlantis apply in the PR comments section. The workload delete command deletes all the configurations generated by workload create command : When executed with the --destroy-resources flag it will also destroy all the resources created for a specific workload. Please note that the workload GitOps repository name should match the one for the workload. When executing with the --destroy-resources flag enabled it must be executed by the cluster owner. Under the hood, it will execute tf destroy locally, and tf state storage is protected and accessible only by the owner. NOTE! : This process is irreversible! The workload delete command can be executed using arguments or environment variables. Arguments : Name (short, full) Type Description -wl, --workload-names TEXT Workload name(s), could be multiple args --all Flag Destroy all existing workloads -wldr, --destroy-resources Flag Destroy workload resources -wlgrn, --workload-gitops-repository-name TEXT Name for Workload GitOps repository --verbosity [DEBUG, INFO, WARNING, ERROR, CRITICAL] Set the logging verbosity level, default CRITICAL Note! : For all names, use kebab-case. Command snippet Using command arguments cgdevxcli workload delete --workload-name your-workload-name","title":"CG DevX CLI Workload Management commands"},{"location":"operators_guide/workload_management/cli_commands/#cg-devx-cli-workload-management-commands","text":"Below is a list of Workload Management commands supported by the CG DevX CLI tool. Workload Management commands depend on local cluster metadata and: a) can only be executed when the CG DevX cluster is already provisioned, and b) should be executed from the same machine where the cluster was provisioned. For more details on cluster provisioning, please check the setup command documentation","title":"CG DevX CLI Workload Management commands"},{"location":"operators_guide/workload_management/cli_commands/#create","text":"The create command generates a declarative configuration of resources required for the workload to function. The configuration is placed in the platform GitOps repository. The CLI automatically pushes changes to a feature branch and creates a Pull Request (PR). Changes introduced with PR should be reviewed and adjusted when necessary. All the changes to the platform GitOps repository should be applied via Atlantis by typing atlantis apply in the PR comments section. The workload create command creates: The configuration for the VCS module to create the Workload source code monorepo and Workload GitOps repo The configuration for the Secrets module to create the Workload secrets namespace and RBAC The configuration for the Core Services module to create the Image Registry, Code Quality, etc. projects for the Workload The ArgoCD dedicated Workload Project ApplicationSet to automatically deliver Workload services The workload create command can be executed using arguments or environment variables. Arguments : Name (short, full) Type Description -wl, --workload-name TEXT Workload name -wlrn, --workload-repository-name TEXT Name for Workload repository -wlgrn, --workload-gitops-repository-name TEXT Name for Workload GitOps repository --verbosity [DEBUG, INFO, WARNING, ERROR, CRITICAL] Set the logging verbosity level, default CRITICAL Note! : For all names use kebab-case. Command snippet Using command arguments cgdevxcli workload create --workload-name your-workload-name \\ --workload-repository-name your-workload-repository-name --workload-gitops-repository-name your-workload-gitops-repository-name","title":"Create"},{"location":"operators_guide/workload_management/cli_commands/#bootstrap","text":"This command creates the folder structure and injects all the necessary configurations for your Workload into repositories created by the create command . By default, the bootstrap command uses: The CG DevX Workload template repository The CG DevX Workload GitOps template repository For other template options, please see bootstrap templates You can also fork and customize existing template repositories and use them by providing custom template repository URLs and branches. Note! : The boostrap command must be executed using the same workload and workload repository names. For example, if your --workload-name is my-workload your --workload-repository-name should be my-workload-repo . The workload bootstrap command can be executed using arguments or environment variables. Arguments : Name (short, full) Type Description -wl, --workload-name TEXT Workload name -wlrn, --workload-repository-name TEXT Name for Workload repository -wlgrn, --workload-gitops-repository-name TEXT Name for Workload GitOps repository -wltu, --workload-template-url TEXT Workload repository template -wltb, --workload-template-branch TEXT Workload repository template branch -wlgu, --workload-gitops-template-url TEXT Workload GitOps repository template -wlgb, --workload-gitops-template-branch TEXT Workload GitOps repository template branch -wls, --workload-service-name TEXT Workload service name -wlsp, --workload-service-port NUMBER Workload service port, default 3000 --verbosity [DEBUG, INFO, WARNING, ERROR, CRITICAL] Set the logging verbosity level, default CRITICAL Note! : For all names, use kebab-case. Command snippet Using command arguments cgdevxcli workload bootstrap --workload-name your-workload-name \\ --workload-repository-name your-workload-repository-name --workload-gitops-repository-name your-workload-gitops-repository-name --workload-service-name your-first-service-name --workload-service-port your-first-service-name-port","title":"Bootstrap"},{"location":"operators_guide/workload_management/cli_commands/#delete","text":"This command removes the declarative configuration of resources required for the workload to function. The CLI automatically pushes changes to a feature branch and creates a Pull Request (PR). Changes introduced with PR should be reviewed and adjusted when necessary. All the changes to platform GitOps repository should be applied via Atlantis by typing atlantis apply in the PR comments section. The workload delete command deletes all the configurations generated by workload create command : When executed with the --destroy-resources flag it will also destroy all the resources created for a specific workload. Please note that the workload GitOps repository name should match the one for the workload. When executing with the --destroy-resources flag enabled it must be executed by the cluster owner. Under the hood, it will execute tf destroy locally, and tf state storage is protected and accessible only by the owner. NOTE! : This process is irreversible! The workload delete command can be executed using arguments or environment variables. Arguments : Name (short, full) Type Description -wl, --workload-names TEXT Workload name(s), could be multiple args --all Flag Destroy all existing workloads -wldr, --destroy-resources Flag Destroy workload resources -wlgrn, --workload-gitops-repository-name TEXT Name for Workload GitOps repository --verbosity [DEBUG, INFO, WARNING, ERROR, CRITICAL] Set the logging verbosity level, default CRITICAL Note! : For all names, use kebab-case. Command snippet Using command arguments cgdevxcli workload delete --workload-name your-workload-name","title":"Delete"},{"location":"operators_guide/workload_management/environments/","text":"Workload Environments A workload may have multiple stages/environments or instances of a workload with its associated configuration and resources. Delivery pipelines automatically roll out changes to all stages except live environments, which are usually user-facing environments such as production, and by default, require manual promotion. Each new workload has three environments (dev, sta, prod) with three different environment variants (dev, uat, prod) associated with it, where one (prod) is the \"live\" environment. Optionally, it\u2019s possible to create a temporary environment (also known as an ephemeral environment) associated with a specific feature branch, with the environment lifecycle tied to the branch lifecycle. This can be used for feature testing in isolation or User Acceptance Testing (UAT). An environment can be associated with a specific CG DevX Runtime cluster (K8s cluster).","title":"Workload Environments"},{"location":"operators_guide/workload_management/environments/#workload-environments","text":"A workload may have multiple stages/environments or instances of a workload with its associated configuration and resources. Delivery pipelines automatically roll out changes to all stages except live environments, which are usually user-facing environments such as production, and by default, require manual promotion. Each new workload has three environments (dev, sta, prod) with three different environment variants (dev, uat, prod) associated with it, where one (prod) is the \"live\" environment. Optionally, it\u2019s possible to create a temporary environment (also known as an ephemeral environment) associated with a specific feature branch, with the environment lifecycle tied to the branch lifecycle. This can be used for feature testing in isolation or User Acceptance Testing (UAT). An environment can be associated with a specific CG DevX Runtime cluster (K8s cluster).","title":"Workload Environments"},{"location":"operators_guide/workload_management/workloads/","text":"Workloads Quickstart Guide A Workload is a high-level abstraction describing application(s) and/or service(s) that provide business value. A Workload is self-contained. Workloads are configured under the CG DevX installation GitOps repository and then managed by the team owning the Workload. CG DevX's goal is to provide isolation at the Workload level through all the core services provided by the CG DevX platform. A Workload is divided into the following logical blocks: Source code : Code of the service(s) the workload consists of K8s manifests : All the manifests required to run a workload on K8s IaC : Additional out-of-cluster resources required by the Workload. When using Crossplane, CRDs should be merged with K8s manifests. These blocks can all be part of one repository or belong to different repositories, depending on user requirements. To provide better isolation of concerns, two repositories are provided by default; one contains the source code for the Workload, and the other contains manifests and IaC. When a user wants to use a different schema, workloads should be created manually, or the repository structure should be adjusted after the workload bootstrap. Bootstrap templates are designed to enable users to blend them or split the GitOps repository into two. Prerequisites An up and running CG DevX cluster. Workloads are defined in the CG DevX platform GitOps repository. All operations with workloads should be done from the same machine used for CG DevX cluster provisioning, as there is a hard dependency on the output of the CG DevX setup flow. There should be no other ongoing changes (active PRs) in the CG DevX platform GitOps repository. This is required to avoid inconsistency in the infrastructure state. Workloads Management Create Workload configuration is generated by the workload create command . This command creates a new feature branch in the CG DevX platform GitOps repository, pushes all the required configurations, and opens a pull request (PR). As a user, you should review the PR and apply the changes via the PR automation solution ( Atlantis ). You can get more details on Atlantis commands here . This creates new repositories for your Workload under your Git organizations. By default, CG DevX will create two repositories, one for Workload application(s)/service(s) source code, and another for manifests, environment definitions, and Infrastructure as Code (IaC). This is done to provide better isolation and access control, and can be changed by updating the Workload configuration for the VCS module. For example, you may want to use one repository per application/service instead of using a monorepo, which is CG DevX's default behavior. You can delete a Workload by running the workload delete command . This will reverse the changes done by the workload create command and open a PR to apply them. Note : You must create and delete workloads one by one to avoid conflicts. Bootstrap When a PR opened by the workload create command is merged and closed, you can bootstrap Workload repositories with the workload bootstrap command . This command provides the following features based on templates created by the CG DevX team: A pre-defined folder structure Environment definitions (dev, sta, prod environments) IaC templates CI/CD process for Workload application(s)/service(s) IaC PR automation configuration Note : Reference implementations of delivery pipelines, repository structure, manifest, and environment definitions are given as examples of platform capabilities. They should be adjusted for your specific use case before production use. The following templates are used by default: Workload template repository Workload GitOps template repository After uploading parameterized repo templates, the workload bootstrap process will create a PR under your Workload GitOps repository to initialize secrets and create the IAM role for the Workload service. Your new workload will have a pre-built CI process triggered by a tag applied to the workload source code repository. Semantic versioning is used. CI builds an image (or images when you have more than one service in your monorepo) and uploads them to the CG DevX image registry ( Harbor ). After that, it updates the image version of the Workload service in K8s deployment definitions in the Workload GitOps repository to trigger the CD process. At this point, you can promote changes from the dev environment to other environments by running the promotion action under the Workload GitOps repository. Manually Customizing and Managing Workloads On a platform level, workloads are defined as a set of Workload objects that are passed to IaC modules via the terraform.tfvars.json file ( more here ). To create additional workloads or workload repositories, or to manage repository settings, you need to update terraform.tfvars.json . The workloads variable schema consists of: variable \"workloads\" { description = \"workloads configuration\" type = map(object({ description = optional(string, \"\") repos = map(object({ description = optional(string, \"\") visibility = optional(string, \"private\") auto_init = optional(bool, false) archive_on_destroy = optional(bool, false) has_issues = optional(bool, false) default_branch_name = optional(string, \"main\") delete_branch_on_merge = optional(bool, true) branch_protection = optional(bool, true) atlantis_enabled = optional(bool, false) })) })) default = {} } Workloads are managed and discovered by ArgoCD automatically. For more details please see under the hood section","title":"Workloads Quickstart Guide"},{"location":"operators_guide/workload_management/workloads/#workloads-quickstart-guide","text":"A Workload is a high-level abstraction describing application(s) and/or service(s) that provide business value. A Workload is self-contained. Workloads are configured under the CG DevX installation GitOps repository and then managed by the team owning the Workload. CG DevX's goal is to provide isolation at the Workload level through all the core services provided by the CG DevX platform. A Workload is divided into the following logical blocks: Source code : Code of the service(s) the workload consists of K8s manifests : All the manifests required to run a workload on K8s IaC : Additional out-of-cluster resources required by the Workload. When using Crossplane, CRDs should be merged with K8s manifests. These blocks can all be part of one repository or belong to different repositories, depending on user requirements. To provide better isolation of concerns, two repositories are provided by default; one contains the source code for the Workload, and the other contains manifests and IaC. When a user wants to use a different schema, workloads should be created manually, or the repository structure should be adjusted after the workload bootstrap. Bootstrap templates are designed to enable users to blend them or split the GitOps repository into two.","title":"Workloads Quickstart Guide"},{"location":"operators_guide/workload_management/workloads/#prerequisites","text":"An up and running CG DevX cluster. Workloads are defined in the CG DevX platform GitOps repository. All operations with workloads should be done from the same machine used for CG DevX cluster provisioning, as there is a hard dependency on the output of the CG DevX setup flow. There should be no other ongoing changes (active PRs) in the CG DevX platform GitOps repository. This is required to avoid inconsistency in the infrastructure state.","title":"Prerequisites"},{"location":"operators_guide/workload_management/workloads/#workloads-management","text":"","title":"Workloads Management"},{"location":"operators_guide/workload_management/workloads/#create","text":"Workload configuration is generated by the workload create command . This command creates a new feature branch in the CG DevX platform GitOps repository, pushes all the required configurations, and opens a pull request (PR). As a user, you should review the PR and apply the changes via the PR automation solution ( Atlantis ). You can get more details on Atlantis commands here . This creates new repositories for your Workload under your Git organizations. By default, CG DevX will create two repositories, one for Workload application(s)/service(s) source code, and another for manifests, environment definitions, and Infrastructure as Code (IaC). This is done to provide better isolation and access control, and can be changed by updating the Workload configuration for the VCS module. For example, you may want to use one repository per application/service instead of using a monorepo, which is CG DevX's default behavior. You can delete a Workload by running the workload delete command . This will reverse the changes done by the workload create command and open a PR to apply them. Note : You must create and delete workloads one by one to avoid conflicts.","title":"Create"},{"location":"operators_guide/workload_management/workloads/#bootstrap","text":"When a PR opened by the workload create command is merged and closed, you can bootstrap Workload repositories with the workload bootstrap command . This command provides the following features based on templates created by the CG DevX team: A pre-defined folder structure Environment definitions (dev, sta, prod environments) IaC templates CI/CD process for Workload application(s)/service(s) IaC PR automation configuration Note : Reference implementations of delivery pipelines, repository structure, manifest, and environment definitions are given as examples of platform capabilities. They should be adjusted for your specific use case before production use. The following templates are used by default: Workload template repository Workload GitOps template repository After uploading parameterized repo templates, the workload bootstrap process will create a PR under your Workload GitOps repository to initialize secrets and create the IAM role for the Workload service. Your new workload will have a pre-built CI process triggered by a tag applied to the workload source code repository. Semantic versioning is used. CI builds an image (or images when you have more than one service in your monorepo) and uploads them to the CG DevX image registry ( Harbor ). After that, it updates the image version of the Workload service in K8s deployment definitions in the Workload GitOps repository to trigger the CD process. At this point, you can promote changes from the dev environment to other environments by running the promotion action under the Workload GitOps repository.","title":"Bootstrap"},{"location":"operators_guide/workload_management/workloads/#manually-customizing-and-managing-workloads","text":"On a platform level, workloads are defined as a set of Workload objects that are passed to IaC modules via the terraform.tfvars.json file ( more here ). To create additional workloads or workload repositories, or to manage repository settings, you need to update terraform.tfvars.json . The workloads variable schema consists of: variable \"workloads\" { description = \"workloads configuration\" type = map(object({ description = optional(string, \"\") repos = map(object({ description = optional(string, \"\") visibility = optional(string, \"private\") auto_init = optional(bool, false) archive_on_destroy = optional(bool, false) has_issues = optional(bool, false) default_branch_name = optional(string, \"main\") delete_branch_on_merge = optional(bool, true) branch_protection = optional(bool, true) atlantis_enabled = optional(bool, false) })) })) default = {} } Workloads are managed and discovered by ArgoCD automatically. For more details please see under the hood section","title":"Manually Customizing and Managing Workloads"},{"location":"under_the_hood/architecture/","text":"Under the hood Networking Networking Runtime Runtime Cost Management Cost Management Delivery Pipelines Delivery Pipelines DevSecOps DevSecOps Observability Observability Workload Management Workload Management IAM IAM","title":"Under the hood"},{"location":"under_the_hood/architecture/#under-the-hood","text":"","title":"Under the hood"},{"location":"under_the_hood/architecture/#networking","text":"Networking","title":"Networking"},{"location":"under_the_hood/architecture/#runtime","text":"Runtime","title":"Runtime"},{"location":"under_the_hood/architecture/#cost-management","text":"Cost Management","title":"Cost Management"},{"location":"under_the_hood/architecture/#delivery-pipelines","text":"Delivery Pipelines","title":"Delivery Pipelines"},{"location":"under_the_hood/architecture/#devsecops","text":"DevSecOps","title":"DevSecOps"},{"location":"under_the_hood/architecture/#observability","text":"Observability","title":"Observability"},{"location":"under_the_hood/architecture/#workload-management","text":"Workload Management","title":"Workload Management"},{"location":"under_the_hood/architecture/#iam","text":"IAM","title":"IAM"},{"location":"under_the_hood/cost_management/cost_management/","text":"Cost Management Our goal is to improve cost visibility and provide cost savings recommendations for the platform and its core services. To achieve this, CG DevX reference architecture incorporates cloud and in-cluster resource labeling/tagging, and integrates with real-time cost visibility and insights tools. By default, the free version of KubeCost is installed with the platform and can be replaced with OpenCost or other tools from our integration catalog. To properly manage costs, users of CG DevX should have a full-scale cost management (visibility, analysis, optimization, governance) FinOps solution. CG DevX reference architecture cannot and should not replace your primary cost management solution. Cost Visibility & Attribution As with many practices in operations, implementing a labeling/tagging strategy is a process of iteration and improvement. CG DevX comes with a minimalistic set of labels/tags as part of an integrated labeling strategy. The main goal is to address immediate needs like lineage tracking and cost attribution, and allow users to grow and evolve the strategy. Cost Attribution Map spending data to the business to enable setback/chargeback by: Creating a meaningful taxonomy of costs Providing an organizational breakdown (by division, business unit, management hierarchy) Offering a functional breakdown (by workload) Classifying by cost center, service, or other unit (map to available taxonomy) Secure Operations By using shared ownership and consumer information associated with resources, you can quickly track all affected parties in case of a security incident. It helps to isolate, identify, and notify the right person quicker, reducing potential impact. Such data could include: Workload owner name Operations team / Team accountable for day-to-day operations Business criticality / Business impact of the resource or supported workload Disaster recovery / Business criticality of the workload or service End date of the project / Date when the workload is scheduled for retirement Rightsizing and Cost Recommendations Containerization helps solve the rightsizing problem by making workloads smaller and easier to properly distribute across multiple nodes. However, it is still challenging to rightsize and attribute workloads correctly. In the K8s world, rightsizing should be done on multiple levels. Pod/Container Rightsizing Ensure that your containers request an appropriate amount of resources. Asking for too little means your application cannot perform, while too much means inefficiency. Often, there is a buffer between the requests or limits configured for a container and what it really needs. When this buffer is larger than necessary, there is an opportunity for cost savings. KubeCost identifies and highlights such opportunities. It works with Vertical Pod Autoscaler (VPA) and Horizontal Pod Autoscaler (HPA), where VPA helps adjust the requests and limits configuration based on actual usage, and HPA scales out and in. Node Rightsizing Node rightsizing involves choosing the right worker node type for the cluster and bin packing. KubeCost provides instance type recommendations, but in practice, this can be challenging. CG DevX allows you to use multiple node types per node group and multiple node groups to improve resource utilization. To enable additional node groups, update your hosting module IaC configuration. Please check this section for more details. CG DevX comes with limits set based on a \"demo scenario,\" prioritizing cost minimization and running core services with a low number of active workloads. On the K8s cluster level, Cluster Autoscaler is turned on by default. These limits should be adjusted for heavy use. For user workloads, we recommend starting with reasonable limits and adjusting them after some time (usually \u2264 1 week) to understand load patterns. Labels/Tags Recommendations Considerations Labels/tags should not be used as a configuration management database. Values that are not easily human-readable can have a human-readable name/description added to another label/tag. Labels/tags should be (semi-) static. Namespaces are used to help organize data specific to a domain or organization. Namespace should be either dash - or dot-separated . . Azure doesn't support colon-separator : . Kebab-case or camelCase is used for keys. Note: some AWS resources may have issues with kebab-case. The format for multi-value is the following: Single-Value: name=value Multi-Value: name=value1:value2 Multi-Attribute: name=attribute1=value1/attribute2=value2 Combination: name=attribute1=value1:value2/attribute2=value1:value3 When date-time data is stored in labels/tags, the UTC ISO 8061 standard is used. Cloud Resources Labels / Tags Below are suggested labels with examples: Use Case Tag Rationale Allowed Values (Listed or value prefix/su\ufb03x) Cost Allocation cg-devx.cost-allocation.workload-id Track cost vs value generated by each line of business analytics/landing-page Cost Allocation cg-devx.cost-allocation.business-unit-id Monitor costs by business unit architecture/devops/finance Cost Allocation cg-devx.cost-allocation.cost-center Monitor costs by cost center 123-* Cost Allocation cg-devx.cost-allocation.owner Which budget holder is responsible for this workload marketing/retail-support Data Classification cg-devx.data.classification Classify data for compliance and governance public/private/confidential, restricted Security cg-devx.security.application-owner IT Application Owner NameA/NameB Security cg-devx.security.operations-team Team that is accountable for day-to-day operations TeamA/TeamB Security cg-devx.security.business-criticality Business criticality of the application, workload, or service High/Medium/Low Business criticality values Business criticality Description high Exploitation causes serious public image damage and financial loss with long-term business impact medium Applications connected to the internet that process financial or other private information low Typically internal applications with non-critical business impact K8s Cluster Resources Labels Use Case Label Rationale Examples Cost Allocation cg-devx.cost-allocation.environment Defines SDLC stage dev/test/stg/prod Cost Allocation cg-devx.cost-allocation.wl-name The name of the workload workload1/workload2/workload3 Cost Allocation cg-devx.cost-allocation.service The name of the service in the workload service1/service2/service3 Cost Allocation cg-devx.cost-allocation.version The current version of the application (e.g., a SemVer 1.0, revision hash, etc.) 5.7.21 Cost Allocation cg-devx.cost-allocation.component The component within the architecture frontend, backend, database OperationalExcellence cg-devx.operations.deployed-by The controller/user who created this resource hpotter, ggranger, rweasley Security cg-devx.security.business-criticality Business criticality of the application, workload, or service High, Medium, Low CG DevX default labels CG DevX applies the following labels by default Cloud Resources Labels / Tags Tag / Label Value Rationale cg-devx.cost-allocation.cost-center platform Indicates cost center cg-devx.metadata.owner platform-admin Indicates team owning the resource cg-devx.metadata.cluster-name Cluster name provided as input parameter K8s Cluster Resources Labels Label Value Rationale cg-devx.cost-allocation.workload Workload name. Applied to workload resources cg-devx.cost-allocation.environment dev,sta,prod Workload environment. Applied to workload resources only cg-devx.cost-allocation.cost-center platform,development Indicates cost center cg-devx.metadata.owner platform-admin/workload-admin Indicates team owning the resource cg-devx.metadata.service Indicates service, workload only cg-devx.metadata.chart-version Indicates version of Helm chart used to deploy service, core-services only cg-devx.metadata.version Indicates version of application deployed, core-services only","title":"Cost Management"},{"location":"under_the_hood/cost_management/cost_management/#cost-management","text":"Our goal is to improve cost visibility and provide cost savings recommendations for the platform and its core services. To achieve this, CG DevX reference architecture incorporates cloud and in-cluster resource labeling/tagging, and integrates with real-time cost visibility and insights tools. By default, the free version of KubeCost is installed with the platform and can be replaced with OpenCost or other tools from our integration catalog. To properly manage costs, users of CG DevX should have a full-scale cost management (visibility, analysis, optimization, governance) FinOps solution. CG DevX reference architecture cannot and should not replace your primary cost management solution.","title":"Cost Management"},{"location":"under_the_hood/cost_management/cost_management/#cost-visibility-attribution","text":"As with many practices in operations, implementing a labeling/tagging strategy is a process of iteration and improvement. CG DevX comes with a minimalistic set of labels/tags as part of an integrated labeling strategy. The main goal is to address immediate needs like lineage tracking and cost attribution, and allow users to grow and evolve the strategy.","title":"Cost Visibility &amp; Attribution"},{"location":"under_the_hood/cost_management/cost_management/#cost-attribution","text":"Map spending data to the business to enable setback/chargeback by: Creating a meaningful taxonomy of costs Providing an organizational breakdown (by division, business unit, management hierarchy) Offering a functional breakdown (by workload) Classifying by cost center, service, or other unit (map to available taxonomy)","title":"Cost Attribution"},{"location":"under_the_hood/cost_management/cost_management/#secure-operations","text":"By using shared ownership and consumer information associated with resources, you can quickly track all affected parties in case of a security incident. It helps to isolate, identify, and notify the right person quicker, reducing potential impact. Such data could include: Workload owner name Operations team / Team accountable for day-to-day operations Business criticality / Business impact of the resource or supported workload Disaster recovery / Business criticality of the workload or service End date of the project / Date when the workload is scheduled for retirement","title":"Secure Operations"},{"location":"under_the_hood/cost_management/cost_management/#rightsizing-and-cost-recommendations","text":"Containerization helps solve the rightsizing problem by making workloads smaller and easier to properly distribute across multiple nodes. However, it is still challenging to rightsize and attribute workloads correctly. In the K8s world, rightsizing should be done on multiple levels.","title":"Rightsizing and Cost Recommendations"},{"location":"under_the_hood/cost_management/cost_management/#podcontainer-rightsizing","text":"Ensure that your containers request an appropriate amount of resources. Asking for too little means your application cannot perform, while too much means inefficiency. Often, there is a buffer between the requests or limits configured for a container and what it really needs. When this buffer is larger than necessary, there is an opportunity for cost savings. KubeCost identifies and highlights such opportunities. It works with Vertical Pod Autoscaler (VPA) and Horizontal Pod Autoscaler (HPA), where VPA helps adjust the requests and limits configuration based on actual usage, and HPA scales out and in.","title":"Pod/Container Rightsizing"},{"location":"under_the_hood/cost_management/cost_management/#node-rightsizing","text":"Node rightsizing involves choosing the right worker node type for the cluster and bin packing. KubeCost provides instance type recommendations, but in practice, this can be challenging. CG DevX allows you to use multiple node types per node group and multiple node groups to improve resource utilization. To enable additional node groups, update your hosting module IaC configuration. Please check this section for more details. CG DevX comes with limits set based on a \"demo scenario,\" prioritizing cost minimization and running core services with a low number of active workloads. On the K8s cluster level, Cluster Autoscaler is turned on by default. These limits should be adjusted for heavy use. For user workloads, we recommend starting with reasonable limits and adjusting them after some time (usually \u2264 1 week) to understand load patterns.","title":"Node Rightsizing"},{"location":"under_the_hood/cost_management/cost_management/#labelstags-recommendations","text":"","title":"Labels/Tags Recommendations"},{"location":"under_the_hood/cost_management/cost_management/#considerations","text":"Labels/tags should not be used as a configuration management database. Values that are not easily human-readable can have a human-readable name/description added to another label/tag. Labels/tags should be (semi-) static. Namespaces are used to help organize data specific to a domain or organization. Namespace should be either dash - or dot-separated . . Azure doesn't support colon-separator : . Kebab-case or camelCase is used for keys. Note: some AWS resources may have issues with kebab-case. The format for multi-value is the following: Single-Value: name=value Multi-Value: name=value1:value2 Multi-Attribute: name=attribute1=value1/attribute2=value2 Combination: name=attribute1=value1:value2/attribute2=value1:value3 When date-time data is stored in labels/tags, the UTC ISO 8061 standard is used.","title":"Considerations"},{"location":"under_the_hood/cost_management/cost_management/#cloud-resources-labels-tags","text":"Below are suggested labels with examples: Use Case Tag Rationale Allowed Values (Listed or value prefix/su\ufb03x) Cost Allocation cg-devx.cost-allocation.workload-id Track cost vs value generated by each line of business analytics/landing-page Cost Allocation cg-devx.cost-allocation.business-unit-id Monitor costs by business unit architecture/devops/finance Cost Allocation cg-devx.cost-allocation.cost-center Monitor costs by cost center 123-* Cost Allocation cg-devx.cost-allocation.owner Which budget holder is responsible for this workload marketing/retail-support Data Classification cg-devx.data.classification Classify data for compliance and governance public/private/confidential, restricted Security cg-devx.security.application-owner IT Application Owner NameA/NameB Security cg-devx.security.operations-team Team that is accountable for day-to-day operations TeamA/TeamB Security cg-devx.security.business-criticality Business criticality of the application, workload, or service High/Medium/Low Business criticality values Business criticality Description high Exploitation causes serious public image damage and financial loss with long-term business impact medium Applications connected to the internet that process financial or other private information low Typically internal applications with non-critical business impact","title":"Cloud Resources Labels / Tags"},{"location":"under_the_hood/cost_management/cost_management/#k8s-cluster-resources-labels","text":"Use Case Label Rationale Examples Cost Allocation cg-devx.cost-allocation.environment Defines SDLC stage dev/test/stg/prod Cost Allocation cg-devx.cost-allocation.wl-name The name of the workload workload1/workload2/workload3 Cost Allocation cg-devx.cost-allocation.service The name of the service in the workload service1/service2/service3 Cost Allocation cg-devx.cost-allocation.version The current version of the application (e.g., a SemVer 1.0, revision hash, etc.) 5.7.21 Cost Allocation cg-devx.cost-allocation.component The component within the architecture frontend, backend, database OperationalExcellence cg-devx.operations.deployed-by The controller/user who created this resource hpotter, ggranger, rweasley Security cg-devx.security.business-criticality Business criticality of the application, workload, or service High, Medium, Low","title":"K8s Cluster Resources Labels"},{"location":"under_the_hood/cost_management/cost_management/#cg-devx-default-labels","text":"CG DevX applies the following labels by default","title":"CG DevX default labels"},{"location":"under_the_hood/cost_management/cost_management/#cloud-resources-labels-tags_1","text":"Tag / Label Value Rationale cg-devx.cost-allocation.cost-center platform Indicates cost center cg-devx.metadata.owner platform-admin Indicates team owning the resource cg-devx.metadata.cluster-name Cluster name provided as input parameter","title":"Cloud Resources Labels / Tags"},{"location":"under_the_hood/cost_management/cost_management/#k8s-cluster-resources-labels_1","text":"Label Value Rationale cg-devx.cost-allocation.workload Workload name. Applied to workload resources cg-devx.cost-allocation.environment dev,sta,prod Workload environment. Applied to workload resources only cg-devx.cost-allocation.cost-center platform,development Indicates cost center cg-devx.metadata.owner platform-admin/workload-admin Indicates team owning the resource cg-devx.metadata.service Indicates service, workload only cg-devx.metadata.chart-version Indicates version of Helm chart used to deploy service, core-services only cg-devx.metadata.version Indicates version of application deployed, core-services only","title":"K8s Cluster Resources Labels"},{"location":"under_the_hood/delivery_pipelines/argo_cleanup/","text":"Argo workflows cleanup options To avoid cluster overload with completed pods Argo Workflow Controller has a cleanup feature. Cleanup parameters could be defined in a workflow and also in workflow defaults subsection of helm values in application.yaml : workflowDefaults: spec: podGC: strategy: OnWorkflowSuccess ttlStrategy: secondsAfterCompletion: 43200 secondsAfterSuccess: 1800 secondsAfterFailure: 21600 As shown above the declaration is self-explaining: podGC strategy is a pod garbage collector parameter, and ttl values define the time to keep workflows after execution. After the complete cleanup performed, the only option to observe the results of workflows is to look at collected artifacts as well as archived logs which in their tur are also kind of an artifact.","title":"Argo workflows cleanup options"},{"location":"under_the_hood/delivery_pipelines/argo_cleanup/#argo-workflows-cleanup-options","text":"To avoid cluster overload with completed pods Argo Workflow Controller has a cleanup feature. Cleanup parameters could be defined in a workflow and also in workflow defaults subsection of helm values in application.yaml : workflowDefaults: spec: podGC: strategy: OnWorkflowSuccess ttlStrategy: secondsAfterCompletion: 43200 secondsAfterSuccess: 1800 secondsAfterFailure: 21600 As shown above the declaration is self-explaining: podGC strategy is a pod garbage collector parameter, and ttl values define the time to keep workflows after execution. After the complete cleanup performed, the only option to observe the results of workflows is to look at collected artifacts as well as archived logs which in their tur are also kind of an artifact.","title":"Argo workflows cleanup options"},{"location":"under_the_hood/delivery_pipelines/argo_config/","text":"Argo workflows configuration Argo workflows configuration is divided by 2 parts: one is a helm chart values in application.yaml and basic entities declaration in cluster repo common for all the cluster installation, another one is a workload-specific kubernetes entities declaration in the wl-<workload name>-dev namespace, executing through kustomize scenario. Helm chart values defined in application.yaml consists of several sections, one for each component of Argo Workflows: controller for Workflow Controller executor for Executor server for Argo Workflow Server artifactRepository for default artifact repository SSO declaration is a part of server section; also every section has it's own rbac parameters. All the Argo Workflow components images are pulling through Harbor registry proxy. To dive deep into the helm chart values see https://github.com/argoproj/argo-helm/blob/main/charts/argo-workflows/values.yaml . Namespaces Namespaces names based on the name of workloads are not known before the workload created, so only argo namesapace noticed for provisioning by helm; all the others are provisioned by kustomize scenario in -gitops repo of workload.","title":"Argo workflows configuration"},{"location":"under_the_hood/delivery_pipelines/argo_config/#argo-workflows-configuration","text":"Argo workflows configuration is divided by 2 parts: one is a helm chart values in application.yaml and basic entities declaration in cluster repo common for all the cluster installation, another one is a workload-specific kubernetes entities declaration in the wl-<workload name>-dev namespace, executing through kustomize scenario. Helm chart values defined in application.yaml consists of several sections, one for each component of Argo Workflows: controller for Workflow Controller executor for Executor server for Argo Workflow Server artifactRepository for default artifact repository SSO declaration is a part of server section; also every section has it's own rbac parameters. All the Argo Workflow components images are pulling through Harbor registry proxy. To dive deep into the helm chart values see https://github.com/argoproj/argo-helm/blob/main/charts/argo-workflows/values.yaml .","title":"Argo workflows configuration"},{"location":"under_the_hood/delivery_pipelines/argo_config/#namespaces","text":"Namespaces names based on the name of workloads are not known before the workload created, so only argo namesapace noticed for provisioning by helm; all the others are provisioned by kustomize scenario in -gitops repo of workload.","title":"Namespaces"},{"location":"under_the_hood/delivery_pipelines/argo_permissions/","text":"Argo workflows permission scopes As noticed in Argo workflows configuration chapter, two Argo workflows permission scopes: argo namespace and wl-<workload name>-dev namespace. Also there are some clusterroles defined by helm chart. argo namespace system accounts and roles are used to execute Argo Workflows Server and Workflow controller. System accounts and roles Each workload provides following system accounts and roles in wl-<workload name>-dev : argo-admin \u2014 RBAC map target to view, change and delete workflows argo-developer \u2014 RBAC map target to view workflows argo-workflow \u2014 used to execute all the CI chain workflows argo namespace are provided with several special systemaccounts: argo-server for Argo Workflows Server argo-workflow-controller for Argo Workflow controller argo-default-sa \u2014 default account with the lowest permissions to login into Argo Workflows UI and is used before the RBAC rules are applied. To dive deep inside Argo Workflows RBAC, see: - [https://argo-workflows.readthedocs.io/en/stable/workflow-rbac/](https://argo-workflows.readthedocs.io/en/stable/workflow-rbac/) - [https://argo-workflows.readthedocs.io/en/stable/argo-server-sso/#sso-rbac](https://argo-workflows.readthedocs.io/en/stable/argo-server-sso/#sso-rbac)","title":"Argo workflows permission scopes"},{"location":"under_the_hood/delivery_pipelines/argo_permissions/#argo-workflows-permission-scopes","text":"As noticed in Argo workflows configuration chapter, two Argo workflows permission scopes: argo namespace and wl-<workload name>-dev namespace. Also there are some clusterroles defined by helm chart. argo namespace system accounts and roles are used to execute Argo Workflows Server and Workflow controller.","title":"Argo workflows permission scopes"},{"location":"under_the_hood/delivery_pipelines/argo_permissions/#system-accounts-and-roles","text":"Each workload provides following system accounts and roles in wl-<workload name>-dev : argo-admin \u2014 RBAC map target to view, change and delete workflows argo-developer \u2014 RBAC map target to view workflows argo-workflow \u2014 used to execute all the CI chain workflows argo namespace are provided with several special systemaccounts: argo-server for Argo Workflows Server argo-workflow-controller for Argo Workflow controller argo-default-sa \u2014 default account with the lowest permissions to login into Argo Workflows UI and is used before the RBAC rules are applied. To dive deep inside Argo Workflows RBAC, see: - [https://argo-workflows.readthedocs.io/en/stable/workflow-rbac/](https://argo-workflows.readthedocs.io/en/stable/workflow-rbac/) - [https://argo-workflows.readthedocs.io/en/stable/argo-server-sso/#sso-rbac](https://argo-workflows.readthedocs.io/en/stable/argo-server-sso/#sso-rbac)","title":"System accounts and roles"},{"location":"under_the_hood/delivery_pipelines/atlantis/","text":"Atlantis Atlantis is used as PR automation tool for IaC part of GitOps. The goal is to: Centralize IaC execution Allow revocation of write access to infrastructure from the team Provide visibility on IaC changes and serve as audit log As Git (users, teams, repositories) and cloud infrastructure are managed using IaC, Atlantis must have sufficient permission to manage them. Git Access To manage Git Atlantis is using a user associated with Git access token provided during setup process. This token is stored as a secret in Vault during platform provisioning proces. Atlantis reads secrets including ATLANTIS_GH_TOKEN from environment variables, which are loaded from Vault by external secret operator from atlantis-envs-secrets . Full list of variables used by Atlantis is defined in your platform GitOps repository platform/terraform/modules/secrets_vault/secrets.tf The concept is described in the Atlantis documentation and GitHub guidance: Creating an Atlantis User (Optional): It's recommended to create a new user named @atlantis or use a dedicated CI user for clarity, although this is not mandatory. You can also use an existing user or GitHub app credentials. The important aspect is that Atlantis will perform actions and write comments under this user's name, so it should be recognizable and not confuse the workflow. In case you want to use a different user, you need to create new user (or use existing user) and access token. This token will replace exiting user token, and should be manually updates in Vault. For mor details on secret management please see . Suggested token scope is \" Administration: Read-only \" and includes following permissions: Checks: Read and write. Commit statuses: Read and write. Contents: Read and write. Issues: Read and write. Metadata: Read-only (default). Pull requests: Read and write. Webhooks: Read and write. Members: Read-only. To get a personal access token, also known as a \"developer token\", please follow the steps as described in GitHub guide . For the most accurate and up-to-date information, you should consult the Atlantis documentation and the GitHub documentation for creating personal access tokens as these resources will have the latest guidelines and best practices for setting up Atlantis with GitHub. For more details on permission please check setup guide Cloud Access Atlantis role for cloud provider access is managed via service account mechanism applied with annotation(s). For more details on role mappings please see IAM Default role permissions should be adjusted based on your specific needs. AWS IAM Access Analyzer or similar tools could be used to evaluate required permissions scope.","title":"Atlantis"},{"location":"under_the_hood/delivery_pipelines/atlantis/#atlantis","text":"Atlantis is used as PR automation tool for IaC part of GitOps. The goal is to: Centralize IaC execution Allow revocation of write access to infrastructure from the team Provide visibility on IaC changes and serve as audit log As Git (users, teams, repositories) and cloud infrastructure are managed using IaC, Atlantis must have sufficient permission to manage them.","title":"Atlantis"},{"location":"under_the_hood/delivery_pipelines/atlantis/#git-access","text":"To manage Git Atlantis is using a user associated with Git access token provided during setup process. This token is stored as a secret in Vault during platform provisioning proces. Atlantis reads secrets including ATLANTIS_GH_TOKEN from environment variables, which are loaded from Vault by external secret operator from atlantis-envs-secrets . Full list of variables used by Atlantis is defined in your platform GitOps repository platform/terraform/modules/secrets_vault/secrets.tf The concept is described in the Atlantis documentation and GitHub guidance: Creating an Atlantis User (Optional): It's recommended to create a new user named @atlantis or use a dedicated CI user for clarity, although this is not mandatory. You can also use an existing user or GitHub app credentials. The important aspect is that Atlantis will perform actions and write comments under this user's name, so it should be recognizable and not confuse the workflow. In case you want to use a different user, you need to create new user (or use existing user) and access token. This token will replace exiting user token, and should be manually updates in Vault. For mor details on secret management please see . Suggested token scope is \" Administration: Read-only \" and includes following permissions: Checks: Read and write. Commit statuses: Read and write. Contents: Read and write. Issues: Read and write. Metadata: Read-only (default). Pull requests: Read and write. Webhooks: Read and write. Members: Read-only. To get a personal access token, also known as a \"developer token\", please follow the steps as described in GitHub guide . For the most accurate and up-to-date information, you should consult the Atlantis documentation and the GitHub documentation for creating personal access tokens as these resources will have the latest guidelines and best practices for setting up Atlantis with GitHub. For more details on permission please check setup guide","title":"Git Access"},{"location":"under_the_hood/delivery_pipelines/atlantis/#cloud-access","text":"Atlantis role for cloud provider access is managed via service account mechanism applied with annotation(s). For more details on role mappings please see IAM Default role permissions should be adjusted based on your specific needs. AWS IAM Access Analyzer or similar tools could be used to evaluate required permissions scope.","title":"Cloud Access"},{"location":"under_the_hood/delivery_pipelines/delivery_pipelines/","text":"Delivery Pipelines Workflows VCS -> ArgoWorkflow integration configuration ArgoWorkflow template Argo Workflows configuration Cleanup of Argo Workflows entities Argo Workflows permission scopes K8s delivery ArgoCD template ArgoCD platform project Artifact store Project management and default limits Vulnerability scanner integration Image and charts replication","title":"Delivery Pipelines"},{"location":"under_the_hood/delivery_pipelines/delivery_pipelines/#delivery-pipelines","text":"","title":"Delivery Pipelines"},{"location":"under_the_hood/delivery_pipelines/delivery_pipelines/#workflows","text":"","title":"Workflows"},{"location":"under_the_hood/delivery_pipelines/delivery_pipelines/#vcs-argoworkflow-integration-configuration","text":"","title":"VCS -&gt; ArgoWorkflow integration configuration"},{"location":"under_the_hood/delivery_pipelines/delivery_pipelines/#argoworkflow-template","text":"Argo Workflows configuration Cleanup of Argo Workflows entities Argo Workflows permission scopes","title":"ArgoWorkflow template"},{"location":"under_the_hood/delivery_pipelines/delivery_pipelines/#k8s-delivery","text":"","title":"K8s delivery"},{"location":"under_the_hood/delivery_pipelines/delivery_pipelines/#argocd-template","text":"","title":"ArgoCD template"},{"location":"under_the_hood/delivery_pipelines/delivery_pipelines/#argocd-platform-project","text":"","title":"ArgoCD platform project"},{"location":"under_the_hood/delivery_pipelines/delivery_pipelines/#artifact-store","text":"","title":"Artifact store"},{"location":"under_the_hood/delivery_pipelines/delivery_pipelines/#project-management-and-default-limits","text":"","title":"Project management and default limits"},{"location":"under_the_hood/delivery_pipelines/delivery_pipelines/#vulnerability-scanner-integration","text":"","title":"Vulnerability scanner integration"},{"location":"under_the_hood/delivery_pipelines/delivery_pipelines/#image-and-charts-replication","text":"","title":"Image and charts replication"},{"location":"under_the_hood/devsecops/devsecops/","text":"DevSecOps The goal is to brings together all the key elements of a security and compliance framework by embedding best practices, policies, and tools at every stage of the delivery lifecycle, so that: Developers, Operators, and Security teams work together Security controls and visualisation is a part of delivery process Audit and compliance checks are in place Delivery pipelines CG DevX reference implementation of delivery pipelines integrate vulnerability & misconfiguration analysis as part of delivery pipelines. Checks that can take place without code being executed also known as but not limited to static security. Provide feedback on early stages of delivery pipeline Help prioritise vulnerabilities Works with source code, dependencies, manifests, and IaC Vulnerability & misconfiguration checks during build time Implemented as an additional step in CI pipelines using Trivy scanner. For more details please see CI Image analysis CG DevX utilizes Harbor ability to provide static analysis of vulnerabilities in images via integration with Trivy and Clair scanners. Trivy integration is turned on by default. To enable clair or turn of trivy integration you should update harbor configuration in your platform GitOps repository gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/harbor/harbor.yaml trivy: # enabled the flag to enable Trivy scanner enabled: false clair: # enabled the flag to enable Clair scanner enabled: true Findings should appear in harbor web portal under project => registry => image as shown in an example below. Other scanners could be enabled through Harbor\u2019s embedded interrogation service. For more details please see official documentation Runtime CG DevX platform reference architecture provides runtime checks - real-time analysis of workloads and core services runtime for vulnerabilities also known as dynamic security Continuous scanning of application, runtime, configuration (secrets, passwords, etc.) Focuses on live (production) workloads, runtime, and infrastructure Vulnerability & misconfiguration checks in runtime Implemented using Aqua Security Trivy operator Trivy operator reports are available as CRDs. For more details on CRDs please check Trivy documentation . Trivy findings are available via Trivy Operator - Vulnerabilities Grafana dashboard For more details on configuration options please check official documentation System runtime security Implemented using Aqua Security Tracee Tracee findings are available via tracee-dashboard Grafana dashboard or using kubectl logs command: kubectl logs -f daemonset/tracee -n tracee You could create policies by adding them directly to your platform GitOps repository under /gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/devsecops/tracee/ path. For more details please check official documentation Policy check and enforcement Implementation is done using Kyverno policy engine. Kyverno has the following policies installed by default using kyverno-policies helm chart as configured in the platform GitOps repository /gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/devsecops/kyverno/kyverno-policies.yaml Baseline disallow-capabilities disallow-host-namespaces disallow-host-path disallow-host-ports disallow-host-process disallow-privileged-containers disallow-proc-mount disallow-selinux restrict-apparmor-profiles restrict-seccomp restrict-sysctls Restricted disallow-capabilities-strict disallow-privilege-escalation require-run-as-non-root-user require-run-as-nonroot restrict-seccomp-strict restrict-volume-types You could add more policies by: Setting a link to existing policy from Kyverno policies repository Adding them directly to your platform GitOps repository under /gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/devsecops/kyverno/ path. Kyverno provides UI which is available via port forwarding, e.g. kubectl port-forward service/policy-reporter-ui 8080:8080 -n kyverno Policy Reporter plugin is installed by default and provides additional information like what policies exist in your Cluster, how they are configured and if there are any violations associated with them.","title":"DevSecOps"},{"location":"under_the_hood/devsecops/devsecops/#devsecops","text":"The goal is to brings together all the key elements of a security and compliance framework by embedding best practices, policies, and tools at every stage of the delivery lifecycle, so that: Developers, Operators, and Security teams work together Security controls and visualisation is a part of delivery process Audit and compliance checks are in place","title":"DevSecOps"},{"location":"under_the_hood/devsecops/devsecops/#delivery-pipelines","text":"CG DevX reference implementation of delivery pipelines integrate vulnerability & misconfiguration analysis as part of delivery pipelines. Checks that can take place without code being executed also known as but not limited to static security. Provide feedback on early stages of delivery pipeline Help prioritise vulnerabilities Works with source code, dependencies, manifests, and IaC","title":"Delivery pipelines"},{"location":"under_the_hood/devsecops/devsecops/#vulnerability-misconfiguration-checks-during-build-time","text":"Implemented as an additional step in CI pipelines using Trivy scanner. For more details please see CI","title":"Vulnerability &amp; misconfiguration checks during build time"},{"location":"under_the_hood/devsecops/devsecops/#image-analysis","text":"CG DevX utilizes Harbor ability to provide static analysis of vulnerabilities in images via integration with Trivy and Clair scanners. Trivy integration is turned on by default. To enable clair or turn of trivy integration you should update harbor configuration in your platform GitOps repository gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/harbor/harbor.yaml trivy: # enabled the flag to enable Trivy scanner enabled: false clair: # enabled the flag to enable Clair scanner enabled: true Findings should appear in harbor web portal under project => registry => image as shown in an example below. Other scanners could be enabled through Harbor\u2019s embedded interrogation service. For more details please see official documentation","title":"Image analysis"},{"location":"under_the_hood/devsecops/devsecops/#runtime","text":"CG DevX platform reference architecture provides runtime checks - real-time analysis of workloads and core services runtime for vulnerabilities also known as dynamic security Continuous scanning of application, runtime, configuration (secrets, passwords, etc.) Focuses on live (production) workloads, runtime, and infrastructure","title":"Runtime"},{"location":"under_the_hood/devsecops/devsecops/#vulnerability-misconfiguration-checks-in-runtime","text":"Implemented using Aqua Security Trivy operator Trivy operator reports are available as CRDs. For more details on CRDs please check Trivy documentation . Trivy findings are available via Trivy Operator - Vulnerabilities Grafana dashboard For more details on configuration options please check official documentation","title":"Vulnerability &amp; misconfiguration checks in runtime"},{"location":"under_the_hood/devsecops/devsecops/#system-runtime-security","text":"Implemented using Aqua Security Tracee Tracee findings are available via tracee-dashboard Grafana dashboard or using kubectl logs command: kubectl logs -f daemonset/tracee -n tracee You could create policies by adding them directly to your platform GitOps repository under /gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/devsecops/tracee/ path. For more details please check official documentation","title":"System runtime security"},{"location":"under_the_hood/devsecops/devsecops/#policy-check-and-enforcement","text":"Implementation is done using Kyverno policy engine. Kyverno has the following policies installed by default using kyverno-policies helm chart as configured in the platform GitOps repository /gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/devsecops/kyverno/kyverno-policies.yaml Baseline disallow-capabilities disallow-host-namespaces disallow-host-path disallow-host-ports disallow-host-process disallow-privileged-containers disallow-proc-mount disallow-selinux restrict-apparmor-profiles restrict-seccomp restrict-sysctls Restricted disallow-capabilities-strict disallow-privilege-escalation require-run-as-non-root-user require-run-as-nonroot restrict-seccomp-strict restrict-volume-types You could add more policies by: Setting a link to existing policy from Kyverno policies repository Adding them directly to your platform GitOps repository under /gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/devsecops/kyverno/ path. Kyverno provides UI which is available via port forwarding, e.g. kubectl port-forward service/policy-reporter-ui 8080:8080 -n kyverno Policy Reporter plugin is installed by default and provides additional information like what policies exist in your Cluster, how they are configured and if there are any violations associated with them.","title":"Policy check and enforcement"},{"location":"under_the_hood/iam/iam/","text":"IAM CG DevX reference implementation attempts to standardize and simplify user identity and access management. Cloud services access Roles and policies used on Cloud Provider IAM level are defined as part of cloud provider module. All the modules implement roles for the following platform services: PR automation for IaC CI Cert manager External DNS Secret Manager Cluster Autoscaler In addition, there could be cloud provider specific roles required by K8s plugins, or other actors. Roles are mapped to K8s service accounts using cloud provider specific annotations using IAM roles for service accounts (IRSA), Workload Identity, or similar mechanism, eg eks.amazonaws.com/role-arn azure.workload.identity/client-id Permission scope set by default could be too wide, or narrow for your specific case. In order to adjust them you should edit cloud provider specific implementation located in your platform GitOps repository: AWS terraform/modules/cloud_aws/iam.tf Azure terraform/modules/cloud_azure/service_accounts.tf User management User management is controlled by CG DevX. This includes Git access, Vault access, and access to all core services via OIDC, and access to workloads. By default, during platform setup process CG DevX CLI creates one user with admin permissions. Additional users could be defined via IaC. Git user management is done using Git provider specific implementation: GitHub : terraform/modules/users_github Users should be defined as dictionary in platform GitOps repository terraform/users/main.tf as shown below. users = { ### Primary bot user \"cgdevx-bot\" = { vcs_username = local.vcs_bot_username email = local.bot_email first_name = \"CG DevX\" last_name = \"Bot\" vcs_team_slugs = [\"${local.gitops_repo_name}-admins\"] acl_policies = [\"admin\", \"default\"] oidc_groups_for_user = [\"admins\"] }, ### Additional users defined bellow. Use this as an example \"my-awesome-user\" = { vcs_username = \"git user slug\" email = \"user email\" first_name = \"First\" last_name = \"Last\" vcs_team_slugs = [\"git team slugs\", \"demo-workload-developers\"] acl_policies = [\"developers\", \"default\", \"workload-name-role\", \"demo-workload-admins\"] oidc_groups_for_user = [\"developers\", \"demo-workload-admins\"] }, } When PR introducing new users is applied via Atlantis, user password for accessing Vault and other platform core services through Vault, will be stored to Vault. You should use root token to access password, and distribute them to the end users. OIDC and SSO Vault is used as OIDC provider enabling SSO experience for platform users. Configuration is done using IaC terraform/modules/secrets_vault/oidc-clients.tf and done during platform provisioning. Argo Workflow Argo CD Grafana Harbor SonarQube Backstage Following scopes are provided via OIDC and could be changed via IaC terraform/modules/secrets_vault/oidc-scopes.tf email groups profile user Permissions across all core services are mapped using groups scope. Vault Access control is done using groups. CG DevX reference implementation has two default groups: admins developers Those two groups have platform wide access. For each workload CG DevX creates the same two groups scoped to a workload using <workload-name>-<group-name> pattern. Admins group policy: # Create and manage entities and groups path \"identity/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } # Configure the OIDC auth method path \"auth/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } # Write ACL policies path \"sys/policies/acl/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } # Allow default access to secret path \"secret/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } # Allow full access to workloads secrets path \"workloads/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } # allow admins to manage mounts path \"sys/mounts/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } # List enabled secrets engine path \"sys/mounts\" { capabilities = [\"read\", \"list\"] } # allow admins to manage auth methods path \"/sys/auth*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"] } # allow admins to manage auth methods path \"sys/auth/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } Developers group policy: # Allow full write access to platform developers, without delete path \"secret/*\" { capabilities = [\"create\", \"read\", \"update\", \"list\"] } # Allow write access to workloads secrets path \"workloads/*\" { capabilities = [\"create\", \"read\", \"update\", \"list\"] } # List available secrets engines to retrieve accessor ID path \"sys/mounts\" { capabilities = [\"read\"] } Policies could be edited via terraform/modules/secrets_vault/policies.tf Argo Workflows CG DevX reference implementation has the following Argo Workflow group mapping: CG DevX role Argo Workflows role Platform admins All project admins Platform developers All project developers Workload admins Workload Namespace admins Workload developers Workload Namespace developers Mapping is configured via Argo Workflow ServiceAccounts gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/argo-workflows/sa-argo.yaml Argo CD CG DevX reference implementation has the following Argo CD group mapping: CG DevX role Argo CD role Platform admins All project admins Platform developers All project read-only Workload admins Project full-access Workload developers Project read-only Mapping is configured via Argo CD ConfigMap gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/argocd/argocd-rbac-cm.yaml Harbor CG DevX reference implementation has the following Harbor group mapping: CG DevX role Harbor role Platform admins Harbor admins Platform developers Harbor guest Workload admins Project maintainer Workload developers Project developer Mapping is configured via IaC terraform/modules/registry_harbor/project/main.tf resource \"harbor_project_member_group\" \"platform_developers\" { project_id = harbor_project.this.id group_name = \"developers\" role = \"guest\" type = \"oidc\" } resource \"harbor_project_member_group\" \"workload_developers\" { project_id = harbor_project.this.id group_name = \"${var.project_name}-developers\" ##choose correct role for workload developers from projectadmin, maintainer(master), developer, guest, limited guest ## https://goharbor.io/docs/2.0.0/administration/managing-users/user-permissions-by-role/ role = \"developer\" type = \"oidc\" } resource \"harbor_project_member_group\" \"workload_admins\" { project_id = harbor_project.this.id group_name = \"${var.project_name}-admins\" role = \"maintainer\" type = \"oidc\" } SonarQube CG DevX reference implementation has the following SonarQube group mapping: CG DevX role Harbor role Platform admins SonarQube admins Platform developers GateAdmin, Scan Workload admins Project admins Workload developers Project: CodeViewer, IssueAdmin, Scan, SecurityHotSpotAdmin OIDC configuration terraform/modules/code_quality_sonarqube/oidc.tf Group mapping terraform/modules/code_quality_sonarqube/groups.tf resource \"sonarqube_group\" \"admins\" { name = \"admins\" description = \"This is a vault admin group\" } resource \"sonarqube_group\" \"developers\" { name = \"developers\" description = \"This is a vault developers group\" } resource \"sonarqube_permissions\" \"admins\" { depends_on = [sonarqube_group.admins] group_name = \"admins\" permissions = [\"admin\"] } # Valid values are [admin, gateadmin, profileadmin, provisioning, scan] # incorrect documentation here https://registry.terraform.io/providers/jdamata/sonarqube/latest/docs/resources/sonarqube_permissions resource \"sonarqube_permissions\" \"developers\" { depends_on = [sonarqube_group.developers] group_name = \"developers\" permissions = [\"gateadmin\", \"scan\"] } Workload group mapping terraform/modules/code_quality_sonarqube/project/main.tf resource \"sonarqube_group\" \"workload_admins\" { name = \"${var.project_name}-admins\" description = \"This is a ${var.project_name} admin group\" } resource \"sonarqube_group\" \"workload_developers\" { name = \"${var.project_name}-developers\" description = \"This is a ${var.project_name} developers group\" } resource \"sonarqube_permissions\" \"workload-admins\" { group_name = \"${var.project_name}-admins\" project_key = sonarqube_project.this.project permissions = [\"admin\"] depends_on = [sonarqube_group.workload_admins] } resource \"sonarqube_permissions\" \"workload-developers\" { group_name = \"${var.project_name}-developers\" project_key = sonarqube_project.this.project permissions = [\"codeviewer\", \"issueadmin\", \"scan\", \"securityhotspotadmin\"] depends_on = [sonarqube_group.workload_developers] }","title":"IAM"},{"location":"under_the_hood/iam/iam/#iam","text":"CG DevX reference implementation attempts to standardize and simplify user identity and access management.","title":"IAM"},{"location":"under_the_hood/iam/iam/#cloud-services-access","text":"Roles and policies used on Cloud Provider IAM level are defined as part of cloud provider module. All the modules implement roles for the following platform services: PR automation for IaC CI Cert manager External DNS Secret Manager Cluster Autoscaler In addition, there could be cloud provider specific roles required by K8s plugins, or other actors. Roles are mapped to K8s service accounts using cloud provider specific annotations using IAM roles for service accounts (IRSA), Workload Identity, or similar mechanism, eg eks.amazonaws.com/role-arn azure.workload.identity/client-id Permission scope set by default could be too wide, or narrow for your specific case. In order to adjust them you should edit cloud provider specific implementation located in your platform GitOps repository: AWS terraform/modules/cloud_aws/iam.tf Azure terraform/modules/cloud_azure/service_accounts.tf","title":"Cloud services access"},{"location":"under_the_hood/iam/iam/#user-management","text":"User management is controlled by CG DevX. This includes Git access, Vault access, and access to all core services via OIDC, and access to workloads. By default, during platform setup process CG DevX CLI creates one user with admin permissions. Additional users could be defined via IaC. Git user management is done using Git provider specific implementation: GitHub : terraform/modules/users_github Users should be defined as dictionary in platform GitOps repository terraform/users/main.tf as shown below. users = { ### Primary bot user \"cgdevx-bot\" = { vcs_username = local.vcs_bot_username email = local.bot_email first_name = \"CG DevX\" last_name = \"Bot\" vcs_team_slugs = [\"${local.gitops_repo_name}-admins\"] acl_policies = [\"admin\", \"default\"] oidc_groups_for_user = [\"admins\"] }, ### Additional users defined bellow. Use this as an example \"my-awesome-user\" = { vcs_username = \"git user slug\" email = \"user email\" first_name = \"First\" last_name = \"Last\" vcs_team_slugs = [\"git team slugs\", \"demo-workload-developers\"] acl_policies = [\"developers\", \"default\", \"workload-name-role\", \"demo-workload-admins\"] oidc_groups_for_user = [\"developers\", \"demo-workload-admins\"] }, } When PR introducing new users is applied via Atlantis, user password for accessing Vault and other platform core services through Vault, will be stored to Vault. You should use root token to access password, and distribute them to the end users.","title":"User management"},{"location":"under_the_hood/iam/iam/#oidc-and-sso","text":"Vault is used as OIDC provider enabling SSO experience for platform users. Configuration is done using IaC terraform/modules/secrets_vault/oidc-clients.tf and done during platform provisioning. Argo Workflow Argo CD Grafana Harbor SonarQube Backstage Following scopes are provided via OIDC and could be changed via IaC terraform/modules/secrets_vault/oidc-scopes.tf email groups profile user Permissions across all core services are mapped using groups scope.","title":"OIDC and SSO"},{"location":"under_the_hood/iam/iam/#vault","text":"Access control is done using groups. CG DevX reference implementation has two default groups: admins developers Those two groups have platform wide access. For each workload CG DevX creates the same two groups scoped to a workload using <workload-name>-<group-name> pattern. Admins group policy: # Create and manage entities and groups path \"identity/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } # Configure the OIDC auth method path \"auth/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } # Write ACL policies path \"sys/policies/acl/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } # Allow default access to secret path \"secret/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } # Allow full access to workloads secrets path \"workloads/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } # allow admins to manage mounts path \"sys/mounts/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } # List enabled secrets engine path \"sys/mounts\" { capabilities = [\"read\", \"list\"] } # allow admins to manage auth methods path \"/sys/auth*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"] } # allow admins to manage auth methods path \"sys/auth/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } Developers group policy: # Allow full write access to platform developers, without delete path \"secret/*\" { capabilities = [\"create\", \"read\", \"update\", \"list\"] } # Allow write access to workloads secrets path \"workloads/*\" { capabilities = [\"create\", \"read\", \"update\", \"list\"] } # List available secrets engines to retrieve accessor ID path \"sys/mounts\" { capabilities = [\"read\"] } Policies could be edited via terraform/modules/secrets_vault/policies.tf","title":"Vault"},{"location":"under_the_hood/iam/iam/#argo-workflows","text":"CG DevX reference implementation has the following Argo Workflow group mapping: CG DevX role Argo Workflows role Platform admins All project admins Platform developers All project developers Workload admins Workload Namespace admins Workload developers Workload Namespace developers Mapping is configured via Argo Workflow ServiceAccounts gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/argo-workflows/sa-argo.yaml","title":"Argo Workflows"},{"location":"under_the_hood/iam/iam/#argo-cd","text":"CG DevX reference implementation has the following Argo CD group mapping: CG DevX role Argo CD role Platform admins All project admins Platform developers All project read-only Workload admins Project full-access Workload developers Project read-only Mapping is configured via Argo CD ConfigMap gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/argocd/argocd-rbac-cm.yaml","title":"Argo CD"},{"location":"under_the_hood/iam/iam/#harbor","text":"CG DevX reference implementation has the following Harbor group mapping: CG DevX role Harbor role Platform admins Harbor admins Platform developers Harbor guest Workload admins Project maintainer Workload developers Project developer Mapping is configured via IaC terraform/modules/registry_harbor/project/main.tf resource \"harbor_project_member_group\" \"platform_developers\" { project_id = harbor_project.this.id group_name = \"developers\" role = \"guest\" type = \"oidc\" } resource \"harbor_project_member_group\" \"workload_developers\" { project_id = harbor_project.this.id group_name = \"${var.project_name}-developers\" ##choose correct role for workload developers from projectadmin, maintainer(master), developer, guest, limited guest ## https://goharbor.io/docs/2.0.0/administration/managing-users/user-permissions-by-role/ role = \"developer\" type = \"oidc\" } resource \"harbor_project_member_group\" \"workload_admins\" { project_id = harbor_project.this.id group_name = \"${var.project_name}-admins\" role = \"maintainer\" type = \"oidc\" }","title":"Harbor"},{"location":"under_the_hood/iam/iam/#sonarqube","text":"CG DevX reference implementation has the following SonarQube group mapping: CG DevX role Harbor role Platform admins SonarQube admins Platform developers GateAdmin, Scan Workload admins Project admins Workload developers Project: CodeViewer, IssueAdmin, Scan, SecurityHotSpotAdmin OIDC configuration terraform/modules/code_quality_sonarqube/oidc.tf Group mapping terraform/modules/code_quality_sonarqube/groups.tf resource \"sonarqube_group\" \"admins\" { name = \"admins\" description = \"This is a vault admin group\" } resource \"sonarqube_group\" \"developers\" { name = \"developers\" description = \"This is a vault developers group\" } resource \"sonarqube_permissions\" \"admins\" { depends_on = [sonarqube_group.admins] group_name = \"admins\" permissions = [\"admin\"] } # Valid values are [admin, gateadmin, profileadmin, provisioning, scan] # incorrect documentation here https://registry.terraform.io/providers/jdamata/sonarqube/latest/docs/resources/sonarqube_permissions resource \"sonarqube_permissions\" \"developers\" { depends_on = [sonarqube_group.developers] group_name = \"developers\" permissions = [\"gateadmin\", \"scan\"] } Workload group mapping terraform/modules/code_quality_sonarqube/project/main.tf resource \"sonarqube_group\" \"workload_admins\" { name = \"${var.project_name}-admins\" description = \"This is a ${var.project_name} admin group\" } resource \"sonarqube_group\" \"workload_developers\" { name = \"${var.project_name}-developers\" description = \"This is a ${var.project_name} developers group\" } resource \"sonarqube_permissions\" \"workload-admins\" { group_name = \"${var.project_name}-admins\" project_key = sonarqube_project.this.project permissions = [\"admin\"] depends_on = [sonarqube_group.workload_admins] } resource \"sonarqube_permissions\" \"workload-developers\" { group_name = \"${var.project_name}-developers\" project_key = sonarqube_project.this.project permissions = [\"codeviewer\", \"issueadmin\", \"scan\", \"securityhotspotadmin\"] depends_on = [sonarqube_group.workload_developers] }","title":"SonarQube"},{"location":"under_the_hood/network/networking/","text":"Networking Network topology CG DevX reference implementation uses tree subnets: public : contains load balancers internal : private : designated to contain K8s cluster(s), out of cluster workload resources, etc When possible there is a direct connection / link between private subnet containing K8s cluster and Blob storage used for terraform state, and cloud native key management solution used for K8s worker node volumes and Vault encryption. 10.0.0.0/16 IPv4 CIDR block address range is used by default. Range is split equally between subnets. Network implementation is cloud provider specific and described in: AWS : terraform/modules/cloud_aws/vpc.tf Azure : terraform/modules/cloud_azure/network.tf DNS management DNS management is done using external DNS controller. It synchronizes exposed Kubernetes Services and Ingresses with DNS providers. Configuration is done using manifest located in platform GitOps repository gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/kube-system/external-dns/application.yaml Currently, only cloud provider native implementation is supported AWS : Route53 Azure : Azure DNS GCP : Cloud DNS Ingress CG DevX uses Ingress NGINX Controller . SSL passthrough is enabled to allow SSL termination in a specific service. Configuration is done using manifest located in platform GitOps repository gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/kube-system/ingress-nginx/application.yaml graph LR; client([client])-. Ingress-managed <br> load balancer .->ingress[Ingress]; ingress-->|routing rule|service[Service]; subgraph cluster ingress; service-->pod1[Pod]; service-->pod2[Pod]; end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class ingress,service,pod1,pod2 k8s; class client plain; class cluster cluster;","title":"Networking"},{"location":"under_the_hood/network/networking/#networking","text":"","title":"Networking"},{"location":"under_the_hood/network/networking/#network-topology","text":"CG DevX reference implementation uses tree subnets: public : contains load balancers internal : private : designated to contain K8s cluster(s), out of cluster workload resources, etc When possible there is a direct connection / link between private subnet containing K8s cluster and Blob storage used for terraform state, and cloud native key management solution used for K8s worker node volumes and Vault encryption. 10.0.0.0/16 IPv4 CIDR block address range is used by default. Range is split equally between subnets. Network implementation is cloud provider specific and described in: AWS : terraform/modules/cloud_aws/vpc.tf Azure : terraform/modules/cloud_azure/network.tf","title":"Network topology"},{"location":"under_the_hood/network/networking/#dns-management","text":"DNS management is done using external DNS controller. It synchronizes exposed Kubernetes Services and Ingresses with DNS providers. Configuration is done using manifest located in platform GitOps repository gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/kube-system/external-dns/application.yaml Currently, only cloud provider native implementation is supported AWS : Route53 Azure : Azure DNS GCP : Cloud DNS","title":"DNS management"},{"location":"under_the_hood/network/networking/#ingress","text":"CG DevX uses Ingress NGINX Controller . SSL passthrough is enabled to allow SSL termination in a specific service. Configuration is done using manifest located in platform GitOps repository gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/kube-system/ingress-nginx/application.yaml graph LR; client([client])-. Ingress-managed <br> load balancer .->ingress[Ingress]; ingress-->|routing rule|service[Service]; subgraph cluster ingress; service-->pod1[Pod]; service-->pod2[Pod]; end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class ingress,service,pod1,pod2 k8s; class client plain; class cluster cluster;","title":"Ingress"},{"location":"under_the_hood/observability/observability/","text":"Observability The CG DevX reference implementation provides a unified experience while working with the observability stack. Monitoring The default option for monitoring is based on the kube-prometheus-stack . This chart also installs the following charts: prometheus-community/kube-state-metrics prometheus-community/prometheus-node-exporter grafana/grafana For more details, please see the official documentation . Metrics Collection K8s state and K8s node metrics are exported and collected by default. All services that expose a standard metrics endpoint ( /metrics ) and are marked with the label expose-metrics: \"true\" are discovered automatically and metrics are collected. To change this behavior or rename labels, update the ServiceMonitor configuration for workloads in gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/monitoring/service-monitor-workloads.yaml . Dashboards You can install additional dashboards or remove pre-installed ones by updating the dashboards configuration section of the kube-prometheus-stack manifest in your platform GitOps repository gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/monitoring/kube-prometheus-stack.yaml . dashboardProviders: dashboardproviders.yaml: apiVersion: 1 providers: - name: 'awesome-grafana-dashboards-provider' orgId: 1 folder: 'awesome-folder' type: file disableDeletion: true editable: true options: path: /var/lib/grafana/dashboards/awesome-grafana-dashboards-provider dashboards: awesome-grafana-dashboards-provider: grafana-dashboards-name: url: 'path to dashboard json file' token: '' ``` ### Alerts To enable Slack alerts, uncomment and update the following configuration in your platform GitOps repository `gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/monitoring/kube-prometheus-stack.yaml`. Both snippets are included with the reference implementation. To enable or add new configurations, edit the promtail manifest gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/promtail/promtail.yaml. Log-Based Alerts You can use log data provided by Loki to create log-based alerts in Grafana. For detailed instructions, please see the official guide. ```yaml alertmanager: config: global: resolve_timeout: 10m slack_api_url: \"https://hooks.slack.com/services/PLACE/HERE/YOURTOKEN\" route: group_by: [ 'namespace' ] group_wait: 30s group_interval: 5m repeat_interval: 12h receiver: 'slack-notifications' routes: - receiver: 'null' matchers: - alertname =~ \"InfoInhibitor|Watchdog\" - receiver: 'slack-notifications' continue: true receivers: - name: 'null' - name: 'slack-notifications' slack_configs: - channel: 'alerts-channel' username: 'AlertBot' send_resolved: true title_link: ' ' title: '[{{ .Status | toUpper }}{{ if eq .Status \"firing\" }}:{{ .Alerts.Firing | len }}{{ end }}] Monitoring Event Notification' text: >- {{ range .Alerts }} *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}` *Description:* {{ .Annotations.description }} *Details:* {{ range .Labels.SortedPairs }} \u2022 *{{ .Name }}:* `{{ .Value }}` {{ end }} {{ end }} ##Log Management The default log management implementation is based on Grafana Loki. Loki is optimized to work with K8s pod logs by design. It allows you to seamlessly switch between metrics and logs using the same labels, greatly improving the user experience. Loki is integrated with Grafana for monitoring, and Grafana is used as the default user interface to query logs. The default configuration of Loki is based on PersistentVolumes and uses in-memory stores. It's not suitable for storing a large amount of data with a short retention period. ##Collection Log collection is done automatically for all workloads using the promtail agent. Logs containing sensitive data can be filtered and data obfuscated using promtail's built-in functionality. The snippet below enables IPv4 address and email obfuscation in log streams: ```yaml pipelineStages: - cri: { } - match: # use sensitive data obfuscation only for a specific app selector: '{app=\"specific-app\"}' stages: - replace: # IP4 expression: '(\\d{1,3}[.]\\d{1,3}[.]\\d{1,3}[.]\\d{1,3})' replace: >- {{ printf \"*IP4*{{ .Value | Hash \\\"salt\\\" }}*\" }} - replace: # email expression: '([\\w\\.=-]+@[\\w\\.-]+\\.[\\w]{2,64})' replace: >- {{ printf \"*email*{{ .Value | Hash \\\"salt\\\" }}*\" }} This snippet allows completely dropping logs by specific source label: ```yaml extraRelabelConfigs: # drop logs for sources with the label drop-logs - source_labels: [ __meta_kubernetes_pod_label_drop_logs ] action: drop regex: true Both snippets are included with the reference implementation. To enable or add new configurations, edit the promtail manifest gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/promtail/promtail.yaml. Log-Based Alerts You can use log data provided by Loki to create log-based alerts in Grafana. For detailed instructions, please see the official guide.","title":"Observability"},{"location":"under_the_hood/observability/observability/#observability","text":"The CG DevX reference implementation provides a unified experience while working with the observability stack.","title":"Observability"},{"location":"under_the_hood/observability/observability/#monitoring","text":"The default option for monitoring is based on the kube-prometheus-stack . This chart also installs the following charts: prometheus-community/kube-state-metrics prometheus-community/prometheus-node-exporter grafana/grafana For more details, please see the official documentation .","title":"Monitoring"},{"location":"under_the_hood/observability/observability/#metrics-collection","text":"K8s state and K8s node metrics are exported and collected by default. All services that expose a standard metrics endpoint ( /metrics ) and are marked with the label expose-metrics: \"true\" are discovered automatically and metrics are collected. To change this behavior or rename labels, update the ServiceMonitor configuration for workloads in gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/monitoring/service-monitor-workloads.yaml .","title":"Metrics Collection"},{"location":"under_the_hood/observability/observability/#dashboards","text":"You can install additional dashboards or remove pre-installed ones by updating the dashboards configuration section of the kube-prometheus-stack manifest in your platform GitOps repository gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/monitoring/kube-prometheus-stack.yaml . dashboardProviders: dashboardproviders.yaml: apiVersion: 1 providers: - name: 'awesome-grafana-dashboards-provider' orgId: 1 folder: 'awesome-folder' type: file disableDeletion: true editable: true options: path: /var/lib/grafana/dashboards/awesome-grafana-dashboards-provider dashboards: awesome-grafana-dashboards-provider: grafana-dashboards-name: url: 'path to dashboard json file' token: '' ``` ### Alerts To enable Slack alerts, uncomment and update the following configuration in your platform GitOps repository `gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/monitoring/kube-prometheus-stack.yaml`. Both snippets are included with the reference implementation. To enable or add new configurations, edit the promtail manifest gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/promtail/promtail.yaml. Log-Based Alerts You can use log data provided by Loki to create log-based alerts in Grafana. For detailed instructions, please see the official guide. ```yaml alertmanager: config: global: resolve_timeout: 10m slack_api_url: \"https://hooks.slack.com/services/PLACE/HERE/YOURTOKEN\" route: group_by: [ 'namespace' ] group_wait: 30s group_interval: 5m repeat_interval: 12h receiver: 'slack-notifications' routes: - receiver: 'null' matchers: - alertname =~ \"InfoInhibitor|Watchdog\" - receiver: 'slack-notifications' continue: true receivers: - name: 'null' - name: 'slack-notifications' slack_configs: - channel: 'alerts-channel' username: 'AlertBot' send_resolved: true title_link: ' ' title: '[{{ .Status | toUpper }}{{ if eq .Status \"firing\" }}:{{ .Alerts.Firing | len }}{{ end }}] Monitoring Event Notification' text: >- {{ range .Alerts }} *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}` *Description:* {{ .Annotations.description }} *Details:* {{ range .Labels.SortedPairs }} \u2022 *{{ .Name }}:* `{{ .Value }}` {{ end }} {{ end }} ##Log Management The default log management implementation is based on Grafana Loki. Loki is optimized to work with K8s pod logs by design. It allows you to seamlessly switch between metrics and logs using the same labels, greatly improving the user experience. Loki is integrated with Grafana for monitoring, and Grafana is used as the default user interface to query logs. The default configuration of Loki is based on PersistentVolumes and uses in-memory stores. It's not suitable for storing a large amount of data with a short retention period. ##Collection Log collection is done automatically for all workloads using the promtail agent. Logs containing sensitive data can be filtered and data obfuscated using promtail's built-in functionality. The snippet below enables IPv4 address and email obfuscation in log streams: ```yaml pipelineStages: - cri: { } - match: # use sensitive data obfuscation only for a specific app selector: '{app=\"specific-app\"}' stages: - replace: # IP4 expression: '(\\d{1,3}[.]\\d{1,3}[.]\\d{1,3}[.]\\d{1,3})' replace: >- {{ printf \"*IP4*{{ .Value | Hash \\\"salt\\\" }}*\" }} - replace: # email expression: '([\\w\\.=-]+@[\\w\\.-]+\\.[\\w]{2,64})' replace: >- {{ printf \"*email*{{ .Value | Hash \\\"salt\\\" }}*\" }} This snippet allows completely dropping logs by specific source label: ```yaml extraRelabelConfigs: # drop logs for sources with the label drop-logs - source_labels: [ __meta_kubernetes_pod_label_drop_logs ] action: drop regex: true Both snippets are included with the reference implementation. To enable or add new configurations, edit the promtail manifest gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/promtail/promtail.yaml.","title":"Dashboards"},{"location":"under_the_hood/observability/observability/#log-based-alerts","text":"You can use log data provided by Loki to create log-based alerts in Grafana. For detailed instructions, please see the official guide.","title":"Log-Based Alerts"},{"location":"under_the_hood/runtime/runtime/","text":"Runtime CG DevX extends K8s runtime by providing default monitoring, log management, and secrets management to all services. Isolation CG DevX provides namespace based isolation for workloads. It is also possible to use cluster level isolation using virtual clusters or physical clusters. Workload namespaces are prefixed with wl- and following pattern wl-<WL_NAME>-<ENV_NAME> Autoscaling Cluster autoscaler is enabled by default with CG DevX reference architecture. Cluster autoscaler configuration is cloud provider specific and may rely on additional labels. Initial cluster node pool capacity is based on values specified in hosting provider terraform module. If you change those values later, autoscaler configuration should be also updates gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/kube-system/cluster-autoscaler/application.yaml","title":"Runtime"},{"location":"under_the_hood/runtime/runtime/#runtime","text":"CG DevX extends K8s runtime by providing default monitoring, log management, and secrets management to all services.","title":"Runtime"},{"location":"under_the_hood/runtime/runtime/#isolation","text":"CG DevX provides namespace based isolation for workloads. It is also possible to use cluster level isolation using virtual clusters or physical clusters. Workload namespaces are prefixed with wl- and following pattern wl-<WL_NAME>-<ENV_NAME>","title":"Isolation"},{"location":"under_the_hood/runtime/runtime/#autoscaling","text":"Cluster autoscaler is enabled by default with CG DevX reference architecture. Cluster autoscaler configuration is cloud provider specific and may rely on additional labels. Initial cluster node pool capacity is based on values specified in hosting provider terraform module. If you change those values later, autoscaler configuration should be also updates gitops-pipelines/delivery/clusters/cc-cluster/core-services/components/kube-system/cluster-autoscaler/application.yaml","title":"Autoscaling"},{"location":"under_the_hood/workloads/workload_management/","text":"Workload Management Workloads are managed by ArgoCD via the ApplicationSet located at /gitops-pipelines/delivery/clusters/cc-cluster/core-services/200-wl-argocd.yaml ArgoCD monitors the /gitops-pipelines/delivery/clusters/cc-cluster/workloads path for the CC cluster. Per-workload definitions are based on a template file, workload-template.yaml and added to that path by the CLI. When manually adding workloads, you should either define your own workload ApplicationSet based on the template and put it into the workloads folder, or create a new ArgoCD application.","title":"Workload Management"},{"location":"under_the_hood/workloads/workload_management/#workload-management","text":"Workloads are managed by ArgoCD via the ApplicationSet located at /gitops-pipelines/delivery/clusters/cc-cluster/core-services/200-wl-argocd.yaml ArgoCD monitors the /gitops-pipelines/delivery/clusters/cc-cluster/workloads path for the CC cluster. Per-workload definitions are based on a template file, workload-template.yaml and added to that path by the CLI. When manually adding workloads, you should either define your own workload ApplicationSet based on the template and put it into the workloads folder, or create a new ArgoCD application.","title":"Workload Management"}]}